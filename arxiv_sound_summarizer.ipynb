{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<feed xmlns=\"http://www.w3.org/2005/Atom\">\n",
      "  <link href=\"http://arxiv.org/api/query?search_query%3Dcat%3Acs.SD%26id_list%3D%26start%3D0%26max_results%3D50\" rel=\"self\" type=\"application/atom+xml\"/>\n",
      "  <title type=\"html\">ArXiv Query: search_query=cat:cs.SD&amp;id_list=&amp;start=0&amp;max_results=50</title>\n",
      "  <id>http://arxiv.org/api/Uk4hOarnIHkcx3INbQB+qNBCFEo</id>\n",
      "  <updated>2024-02-21T00:00:00-05:00</updated>\n",
      "  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">13172</opensearch:totalResults>\n",
      "  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n",
      "  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">50</opensearch:itemsPerPage>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.13236v1</id>\n",
      "    <updated>2024-02-20T18:50:25Z</updated>\n",
      "    <published>2024-02-20T18:50:25Z</published>\n",
      "    <title>Towards audio language modeling - an overview</title>\n",
      "    <summary>  Neural audio codecs are initially introduced to compress audio data into\n",
      "compact codes to reduce transmission latency. Researchers recently discovered\n",
      "the potential of codecs as suitable tokenizers for converting continuous audio\n",
      "into discrete codes, which can be employed to develop audio language models\n",
      "(LMs). Numerous high-performance neural audio codecs and codec-based LMs have\n",
      "been developed. The paper aims to provide a thorough and systematic overview of\n",
      "the neural audio codec models and codec-based LMs.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Haibin Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xuanjun Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yi-Cheng Lin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kai-wei Chang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ho-Lam Chung</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alexander H. Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hung-yi Lee</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2402.13236v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.13236v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.13110v1</id>\n",
      "    <updated>2024-02-20T16:03:02Z</updated>\n",
      "    <published>2024-02-20T16:03:02Z</published>\n",
      "    <title>HiRIS: an Airborne Sonar Sensor with a 1024 Channel Microphone Array for\n",
      "  In-Air Acoustic Imaging</title>\n",
      "    <summary>  Airborne 3D imaging using ultrasound is a promising sensing modality for\n",
      "robotic applications in harsh environments. Over the last decade, several\n",
      "high-performance systems have been proposed in the literature. Most of these\n",
      "sensors use a reduced aperture microphone array, leading to artifacts in the\n",
      "resulting acoustic images. This paper presents a novel in-air ultrasound sensor\n",
      "that incorporates 1024 microphones, in a 32-by- 32 uniform rectangular array,\n",
      "in combination with a distributed embedded hardware design to perform the data\n",
      "acquisition. Using a broadband Minimum Variance Distortionless Response (MVDR)\n",
      "beamformer with Forward-Backward Spatial Smoothing (FB-SS), the sensor is able\n",
      "to create both 2D and 3D ultrasound images of the full-frontal hemisphere with\n",
      "high angular accuracy with up to 70dB main lobe to side lobe ratio. This paper\n",
      "describes both the hardware infrastructure needed to obtain such highly\n",
      "detailed acoustical images, as well as the signal processing chain needed to\n",
      "convert the raw acoustic data into said images. Utilizing this novel\n",
      "high-resolution ultrasound imaging sensor, we wish to investigate the limits of\n",
      "both passive and active airborne ultrasound sensing by utilizing this virtually\n",
      "artifact-free imaging modality.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Dennis Laurijssen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Walter Daems</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jan Steckel</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2402.13110v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.13110v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.13076v1</id>\n",
      "    <updated>2024-02-20T15:22:25Z</updated>\n",
      "    <published>2024-02-20T15:22:25Z</published>\n",
      "    <title>Not All Weights Are Created Equal: Enhancing Energy Efficiency in\n",
      "  On-Device Streaming Speech Recognition</title>\n",
      "    <summary>  Power consumption plays an important role in on-device streaming speech\n",
      "recognition, as it has a direct impact on the user experience. This study\n",
      "delves into how weight parameters in speech recognition models influence the\n",
      "overall power consumption of these models. We discovered that the impact of\n",
      "weight parameters on power consumption varies, influenced by factors including\n",
      "how often they are invoked and their placement in memory. Armed with this\n",
      "insight, we developed design guidelines aimed at optimizing on-device speech\n",
      "recognition models. These guidelines focus on minimizing power use without\n",
      "substantially affecting accuracy. Our method, which employs targeted\n",
      "compression based on the varying sensitivities of weight parameters,\n",
      "demonstrates superior performance compared to state-of-the-art compression\n",
      "methods. It achieves a reduction in energy usage of up to 47% while maintaining\n",
      "similar model accuracy and improving the real-time factor.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Yang Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuan Shangguan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuhao Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Liangzhen Lai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ernie Chang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Changsheng Zhao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yangyang Shi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vikas Chandra</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2402.13076v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.13076v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.13071v1</id>\n",
      "    <updated>2024-02-20T15:13:38Z</updated>\n",
      "    <published>2024-02-20T15:13:38Z</published>\n",
      "    <title>Codec-SUPERB: An In-Depth Analysis of Sound Codec Models</title>\n",
      "    <summary>  The sound codec's dual roles in minimizing data transmission latency and\n",
      "serving as tokenizers underscore its critical importance. Recent years have\n",
      "witnessed significant developments in codec models. The ideal sound codec\n",
      "should preserve content, paralinguistics, speakers, and audio information.\n",
      "However, the question of which codec achieves optimal sound information\n",
      "preservation remains unanswered, as in different papers, models are evaluated\n",
      "on their selected experimental settings. This study introduces Codec-SUPERB, an\n",
      "acronym for Codec sound processing Universal PERformance Benchmark. It is an\n",
      "ecosystem designed to assess codec models across representative sound\n",
      "applications and signal-level metrics rooted in sound domain\n",
      "knowledge.Codec-SUPERB simplifies result sharing through an online leaderboard,\n",
      "promoting collaboration within a community-driven benchmark database, thereby\n",
      "stimulating new development cycles for codecs. Furthermore, we undertake an\n",
      "in-depth analysis to offer insights into codec models from both application and\n",
      "signal perspectives, diverging from previous codec papers mainly concentrating\n",
      "on signal-level comparisons. Finally, we will release codes, the leaderboard,\n",
      "and data to accelerate progress within the community.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Haibin Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ho-Lam Chung</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yi-Cheng Lin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuan-Kuei Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xuanjun Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yu-Chi Pai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hsiu-Hsuan Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kai-Wei Chang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Alexander H. Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hung-yi Lee</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Github: https://github.com/voidful/Codec-SUPERB</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2402.13071v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.13071v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.13018v1</id>\n",
      "    <updated>2024-02-20T14:00:53Z</updated>\n",
      "    <published>2024-02-20T14:00:53Z</published>\n",
      "    <title>EMO-SUPERB: An In-depth Look at Speech Emotion Recognition</title>\n",
      "    <summary>  Speech emotion recognition (SER) is a pivotal technology for human-computer\n",
      "interaction systems. However, 80.77% of SER papers yield results that cannot be\n",
      "reproduced. We develop EMO-SUPERB, short for EMOtion Speech Universal\n",
      "PERformance Benchmark, which aims to enhance open-source initiatives for SER.\n",
      "EMO-SUPERB includes a user-friendly codebase to leverage 15 state-of-the-art\n",
      "speech self-supervised learning models (SSLMs) for exhaustive evaluation across\n",
      "six open-source SER datasets. EMO-SUPERB streamlines result sharing via an\n",
      "online leaderboard, fostering collaboration within a community-driven benchmark\n",
      "and thereby enhancing the development of SER. On average, 2.58% of annotations\n",
      "are annotated using natural language. SER relies on classification models and\n",
      "is unable to process natural languages, leading to the discarding of these\n",
      "valuable annotations. We prompt ChatGPT to mimic annotators, comprehend natural\n",
      "language annotations, and subsequently re-label the data. By utilizing labels\n",
      "generated by ChatGPT, we consistently achieve an average relative gain of 3.08%\n",
      "across all settings.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Haibin Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Huang-Cheng Chou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kai-Wei Chang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lucas Goncalves</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiawei Du</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jyh-Shing Roger Jang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chi-Chun Lee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hung-Yi Lee</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">webpage: https://emosuperb.github.io/</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2402.13018v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.13018v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.12746v1</id>\n",
      "    <updated>2024-02-20T06:24:38Z</updated>\n",
      "    <published>2024-02-20T06:24:38Z</published>\n",
      "    <title>Plugin Speech Enhancement: A Universal Speech Enhancement Framework\n",
      "  Inspired by Dynamic Neural Network</title>\n",
      "    <summary>  The expectation to deploy a universal neural network for speech enhancement,\n",
      "with the aim of improving noise robustness across diverse speech processing\n",
      "tasks, faces challenges due to the existing lack of awareness within static\n",
      "speech enhancement frameworks regarding the expected speech in downstream\n",
      "modules. These limitations impede the effectiveness of static speech\n",
      "enhancement approaches in achieving optimal performance for a range of speech\n",
      "processing tasks, thereby challenging the notion of universal applicability.\n",
      "The fundamental issue in achieving universal speech enhancement lies in\n",
      "effectively informing the speech enhancement module about the features of\n",
      "downstream modules. In this study, we present a novel weighting prediction\n",
      "approach, which explicitly learns the task relationships from downstream\n",
      "training information to address the core challenge of universal speech\n",
      "enhancement. We found the role of deciding whether to employ data augmentation\n",
      "techniques as crucial downstream training information. This decision\n",
      "significantly impacts the expected speech and the performance of the speech\n",
      "enhancement module. Moreover, we introduce a novel speech enhancement network,\n",
      "the Plugin Speech Enhancement (Plugin-SE). The Plugin-SE is a dynamic neural\n",
      "network that includes the speech enhancement module, gate module, and weight\n",
      "prediction module. Experimental results demonstrate that the proposed Plugin-SE\n",
      "approach is competitive or superior to other joint training methods across\n",
      "various downstream tasks.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Yanan Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zihao Cui</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yingying Gao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Junlan Feng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chao Deng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shilei Zhang</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2402.12746v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.12746v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.12660v1</id>\n",
      "    <updated>2024-02-20T02:16:24Z</updated>\n",
      "    <published>2024-02-20T02:16:24Z</published>\n",
      "    <title>SingVisio: Visual Analytics of Diffusion Model for Singing Voice\n",
      "  Conversion</title>\n",
      "    <summary>  In this study, we present SingVisio, an interactive visual analysis system\n",
      "that aims to explain the diffusion model used in singing voice conversion.\n",
      "SingVisio provides a visual display of the generation process in diffusion\n",
      "models, showcasing the step-by-step denoising of the noisy spectrum and its\n",
      "transformation into a clean spectrum that captures the desired singer's timbre.\n",
      "The system also facilitates side-by-side comparisons of different conditions,\n",
      "such as source content, melody, and target timbre, highlighting the impact of\n",
      "these conditions on the diffusion generation process and resulting conversions.\n",
      "Through comprehensive evaluations, SingVisio demonstrates its effectiveness in\n",
      "terms of system design, functionality, explainability, and user-friendliness.\n",
      "It offers users of various backgrounds valuable learning experiences and\n",
      "insights into the diffusion model for singing voice conversion.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Liumeng Xue</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chaoren Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mingxuan Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xueyao Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jun Han</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhizheng Wu</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2402.12660v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.12660v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.12658v1</id>\n",
      "    <updated>2024-02-20T02:14:45Z</updated>\n",
      "    <published>2024-02-20T02:14:45Z</published>\n",
      "    <title>Guiding the underwater acoustic target recognition with interpretable\n",
      "  contrastive learning</title>\n",
      "    <summary>  Recognizing underwater targets from acoustic signals is a challenging task\n",
      "owing to the intricate ocean environments and variable underwater channels.\n",
      "While deep learning-based systems have become the mainstream approach for\n",
      "underwater acoustic target recognition, they have faced criticism for their\n",
      "lack of interpretability and weak generalization performance in practical\n",
      "applications. In this work, we apply the class activation mapping (CAM) to\n",
      "generate visual explanations for the predictions of a spectrogram-based\n",
      "recognition system. CAM can help to understand the behavior of recognition\n",
      "models by highlighting the regions of the input features that contribute the\n",
      "most to the prediction. Our explorations reveal that recognition models tend to\n",
      "focus on the low-frequency line spectrum and high-frequency periodic modulation\n",
      "information of underwater signals. Based on the observation, we propose an\n",
      "interpretable contrastive learning (ICL) strategy that employs two encoders to\n",
      "learn from acoustic features with different emphases (line spectrum and\n",
      "modulation information). By imposing constraints between encoders, the proposed\n",
      "strategy can enhance the generalization performance of the recognition system.\n",
      "Our experiments demonstrate that the proposed contrastive learning approach can\n",
      "improve the recognition accuracy and bring significant improvements across\n",
      "various underwater databases.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Yuan Xie</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiawei Ren</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ji Xu</name>\n",
      "    </author>\n",
      "    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/OCEANSLimerick52467.2023.10244447</arxiv:doi>\n",
      "    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/OCEANSLimerick52467.2023.10244447\" rel=\"related\"/>\n",
      "    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">OCEANS 2023-Limerick. IEEE, 2023: 1-6</arxiv:journal_ref>\n",
      "    <link href=\"http://arxiv.org/abs/2402.12658v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.12658v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.12654v1</id>\n",
      "    <updated>2024-02-20T02:04:38Z</updated>\n",
      "    <published>2024-02-20T02:04:38Z</published>\n",
      "    <title>OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech\n",
      "  Recognition, Translation, and Language Identification</title>\n",
      "    <summary>  There has been an increasing interest in large speech models that can perform\n",
      "multiple speech processing tasks in a single model. Such models usually adopt\n",
      "the encoder-decoder or decoder-only architecture due to their popularity and\n",
      "good performance in many domains. However, autoregressive models can be slower\n",
      "during inference compared to non-autoregressive models and also have potential\n",
      "risks of hallucination. Though prior studies observed promising results of\n",
      "non-autoregressive models for certain tasks at small scales, it remains unclear\n",
      "if they can be scaled to speech-to-text generation in diverse languages and\n",
      "tasks. Inspired by the Open Whisper-style Speech Model (OWSM) project, we\n",
      "propose OWSM-CTC, a novel encoder-only speech foundation model based on\n",
      "Connectionist Temporal Classification (CTC). It is trained on 180k hours of\n",
      "public audio data for multilingual automatic speech recognition (ASR), speech\n",
      "translation (ST), and language identification (LID). Compared to\n",
      "encoder-decoder OWSM, our OWSM-CTC achieves competitive results on ASR and up\n",
      "to 25% relative improvement on ST, while it is more robust and 3 to 4 times\n",
      "faster for inference. OWSM-CTC also improves the long-form ASR result with 20x\n",
      "speed-up. We will publicly release our codebase, pre-trained model, and\n",
      "training logs to promote open science in speech foundation models.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Yifan Peng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yui Sudo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Muhammad Shakeel</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shinji Watanabe</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">18 pages, 2 figures</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2402.12654v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.12654v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.12482v1</id>\n",
      "    <updated>2024-02-19T19:38:37Z</updated>\n",
      "    <published>2024-02-19T19:38:37Z</published>\n",
      "    <title>SECP: A Speech Enhancement-Based Curation Pipeline For Scalable\n",
      "  Acquisition Of Clean Speech</title>\n",
      "    <summary>  As more speech technologies rely on a supervised deep learning approach with\n",
      "clean speech as the ground truth, a methodology to onboard said speech at scale\n",
      "is needed. However, this approach needs to minimize the dependency on human\n",
      "listening and annotation, only requiring a human-in-the-loop when needed. In\n",
      "this paper, we address this issue by outlining Speech Enhancement-based\n",
      "Curation Pipeline (SECP) which serves as a framework to onboard clean speech.\n",
      "This clean speech can then train a speech enhancement model, which can further\n",
      "refine the original dataset and thus close the iterative loop. By running two\n",
      "iterative rounds, we observe that enhanced output used as ground truth does not\n",
      "degrade model performance according to $\\Delta_{PESQ}$, a metric used in this\n",
      "paper. We also show through comparative mean opinion score (CMOS) based\n",
      "subjective tests that the highest and lowest bound of refined data is\n",
      "perceptually better than the original data.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Adam Sabra</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Cyprian Wronka</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Michelle Mao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Samer Hijazi</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted to the International Conference on Acoustics, Speech and\n",
      "  Signal Processing (ICASSP) 2024</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2402.12482v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.12482v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.12423v1</id>\n",
      "    <updated>2024-02-19T16:22:21Z</updated>\n",
      "    <published>2024-02-19T16:22:21Z</published>\n",
      "    <title>On the Semantic Latent Space of Diffusion-Based Text-to-Speech Models</title>\n",
      "    <summary>  The incorporation of Denoising Diffusion Models (DDMs) in the Text-to-Speech\n",
      "(TTS) domain is rising, providing great value in synthesizing high quality\n",
      "speech. Although they exhibit impressive audio quality, the extent of their\n",
      "semantic capabilities is unknown, and controlling their synthesized speech's\n",
      "vocal properties remains a challenge. Inspired by recent advances in image\n",
      "synthesis, we explore the latent space of frozen TTS models, which is composed\n",
      "of the latent bottleneck activations of the DDM's denoiser. We identify that\n",
      "this space contains rich semantic information, and outline several novel\n",
      "methods for finding semantic directions within it, both supervised and\n",
      "unsupervised. We then demonstrate how these enable off-the-shelf audio editing,\n",
      "without any further training, architectural changes or data requirements. We\n",
      "present evidence of the semantic and acoustic qualities of the edited audio,\n",
      "and provide supplemental samples:\n",
      "https://latent-analysis-grad-tts.github.io/speech-samples/.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Miri Varshavsky Hassid</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Roy Hirsch</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Regev Cohen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tomer Golany</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Daniel Freedman</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ehud Rivlin</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2402.12423v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.12423v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.12239v1</id>\n",
      "    <updated>2024-02-19T15:50:32Z</updated>\n",
      "    <published>2024-02-19T15:50:32Z</published>\n",
      "    <title>Significance of Chirp MFCC as a Feature in Speech and Audio Applications</title>\n",
      "    <summary>  A novel feature, based on the chirp z-transform, that offers an improved\n",
      "representation of the underlying true spectrum is proposed. This feature, the\n",
      "chirp MFCC, is derived by computing the Mel frequency cepstral coefficients\n",
      "from the chirp magnitude spectrum, instead of the Fourier transform magnitude\n",
      "spectrum. The theoretical foundations for the proposal, and the experimental\n",
      "validation using product of likelihood Gaussians, to show the improved class\n",
      "separation offered by the proposed chirp MFCC, when compared with vanilla MFCC\n",
      "are discussed. Further, real world evaluation of the feature is performed using\n",
      "three diverse tasks, namely, speech-music classification, speaker\n",
      "identification, and speech commands recognition. It is shown in all three tasks\n",
      "that the proposed chirp MFCC offers considerable improvements.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>S. Johanan Joysingh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>P. Vijayalakshmi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>T. Nagarajan</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2402.12239v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.12239v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.12208v2</id>\n",
      "    <updated>2024-02-20T15:24:30Z</updated>\n",
      "    <published>2024-02-19T15:12:12Z</published>\n",
      "    <title>Language-Codec: Reducing the Gaps Between Discrete Codec Representation\n",
      "  and Speech Language Models</title>\n",
      "    <summary>  In recent years, large language models have achieved significant success in\n",
      "generative tasks (e.g., speech cloning and audio generation) related to speech,\n",
      "audio, music, and other signal domains. A crucial element of these models is\n",
      "the discrete acoustic codecs, which serves as an intermediate representation\n",
      "replacing the mel-spectrogram. However, there exist several gaps between\n",
      "discrete codecs and downstream speech language models. Specifically, 1) most\n",
      "codec models are trained on only 1,000 hours of data, whereas most speech\n",
      "language models are trained on 60,000 hours; 2) Achieving good reconstruction\n",
      "performance requires the utilization of numerous codebooks, which increases the\n",
      "burden on downstream speech language models; 3) The initial channel of the\n",
      "codebooks contains excessive information, making it challenging to directly\n",
      "generate acoustic tokens from weakly supervised signals such as text in\n",
      "downstream tasks. Consequently, leveraging the characteristics of speech\n",
      "language models, we propose Language-Codec. In the Language-Codec, we introduce\n",
      "a Mask Channel Residual Vector Quantization (MCRVQ) mechanism along with\n",
      "improved Fourier transform structures and larger training datasets to address\n",
      "the aforementioned gaps. We compare our method with competing audio compression\n",
      "algorithms and observe significant outperformance across extensive evaluations.\n",
      "Furthermore, we also validate the efficiency of the Language-Codec on\n",
      "downstream speech language models. The source code and pre-trained models can\n",
      "be accessed at https://github.com/jishengpeng/languagecodec .\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Shengpeng Ji</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Minghui Fang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ziyue Jiang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rongjie Huang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jialung Zuo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shulei Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhou Zhao</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2402.12208v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.12208v2\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.12094v1</id>\n",
      "    <updated>2024-02-19T12:20:56Z</updated>\n",
      "    <published>2024-02-19T12:20:56Z</published>\n",
      "    <title>On the relationship between speech and hearing</title>\n",
      "    <summary>  We present a framework for experimentally linking speech production and\n",
      "hearing. Using this approach, we describe experimental results, that lead to\n",
      "the concept that sounds made by different individuals and perceived to be the\n",
      "same can be transformed into each other by a \"speech scale\". The speech scale\n",
      "is empirically determined using only speech data. We show the similarity of the\n",
      "speech scale to the MEL scale of Stevens and Volkmann, which was derived only\n",
      "from hearing experiments. We thus experimentally link speech production and\n",
      "hearing.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Srinivasan Umesh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Leon Cohen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Douglas Nelson</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2402.12094v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.12094v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.11954v1</id>\n",
      "    <updated>2024-02-19T08:49:09Z</updated>\n",
      "    <published>2024-02-19T08:49:09Z</published>\n",
      "    <title>Multimodal Emotion Recognition from Raw Audio with Sinc-convolution</title>\n",
      "    <summary>  Speech Emotion Recognition (SER) is still a complex task for computers with\n",
      "average recall rates usually about 70% on the most realistic datasets. Most SER\n",
      "systems use hand-crafted features extracted from audio signal such as energy,\n",
      "zero crossing rate, spectral information, prosodic, mel frequency cepstral\n",
      "coefficient (MFCC), and so on. More recently, using raw waveform for training\n",
      "neural network is becoming an emerging trend. This approach is advantageous as\n",
      "it eliminates the feature extraction pipeline. Learning from time-domain signal\n",
      "has shown good results for tasks such as speech recognition, speaker\n",
      "verification etc. In this paper, we utilize Sinc-convolution layer, which is an\n",
      "efficient architecture for preprocessing raw speech waveform for emotion\n",
      "recognition, to extract acoustic features from raw audio signals followed by a\n",
      "long short-term memory (LSTM). We also incorporate linguistic features and\n",
      "append a dialogical emotion decoding (DED) strategy. Our approach achieves a\n",
      "weighted accuracy of 85.1\\% in four class emotion on the Interactive Emotional\n",
      "Dyadic Motion Capture (IEMOCAP) dataset.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Xiaohui Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wenjie Fu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mangui Liang</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2402.11954v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.11954v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.11931v1</id>\n",
      "    <updated>2024-02-19T08:18:52Z</updated>\n",
      "    <published>2024-02-19T08:18:52Z</published>\n",
      "    <title>Soft-Weighted CrossEntropy Loss for Continous Alzheimer's Disease\n",
      "  Detection</title>\n",
      "    <summary>  Alzheimer's disease is a common cognitive disorder in the elderly. Early and\n",
      "accurate diagnosis of Alzheimer's disease (AD) has a major impact on the\n",
      "progress of research on dementia. At present, researchers have used machine\n",
      "learning methods to detect Alzheimer's disease from the speech of participants.\n",
      "However, the recognition accuracy of current methods is unsatisfactory, and\n",
      "most of them focus on using low-dimensional handcrafted features to extract\n",
      "relevant information from audios. This paper proposes an Alzheimer's disease\n",
      "detection system based on the pre-trained framework Wav2vec 2.0 (Wav2vec2). In\n",
      "addition, by replacing the loss function with the Soft-Weighted CrossEntropy\n",
      "loss function, we achieved 85.45\\% recognition accuracy on the same test\n",
      "dataset.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Xiaohui Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wenjie Fu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Mangui Liang</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2402.11931v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.11931v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"q-bio.NC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.11919v1</id>\n",
      "    <updated>2024-02-19T08:07:01Z</updated>\n",
      "    <published>2024-02-19T08:07:01Z</published>\n",
      "    <title>Unraveling Complex Data Diversity in Underwater Acoustic Target\n",
      "  Recognition through Convolution-based Mixture of Experts</title>\n",
      "    <summary>  Underwater acoustic target recognition is a difficult task owing to the\n",
      "intricate nature of underwater acoustic signals. The complex underwater\n",
      "environments, unpredictable transmission channels, and dynamic motion states\n",
      "greatly impact the real-world underwater acoustic signals, and may even obscure\n",
      "the intrinsic characteristics related to targets. Consequently, the data\n",
      "distribution of underwater acoustic signals exhibits high intra-class\n",
      "diversity, thereby compromising the accuracy and robustness of recognition\n",
      "systems.To address these issues, this work proposes a convolution-based mixture\n",
      "of experts (CMoE) that recognizes underwater targets in a fine-grained manner.\n",
      "The proposed technique introduces multiple expert layers as independent\n",
      "learners, along with a routing layer that determines the assignment of experts\n",
      "according to the characteristics of inputs. This design allows the model to\n",
      "utilize independent parameter spaces, facilitating the learning of complex\n",
      "underwater signals with high intra-class diversity. Furthermore, this work\n",
      "optimizes the CMoE structure by balancing regularization and an optional\n",
      "residual module. To validate the efficacy of our proposed techniques, we\n",
      "conducted detailed experiments and visualization analyses on three underwater\n",
      "acoustic databases across several acoustic features. The experimental results\n",
      "demonstrate that our CMoE consistently achieves significant performance\n",
      "improvements, delivering superior recognition accuracy when compared to\n",
      "existing advanced methods.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Yuan Xie</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiawei Ren</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ji Xu</name>\n",
      "    </author>\n",
      "    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1016/j.eswa.2024.123431</arxiv:doi>\n",
      "    <link title=\"doi\" href=\"http://dx.doi.org/10.1016/j.eswa.2024.123431\" rel=\"related\"/>\n",
      "    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Expert Systems with Applications (2024): 123431</arxiv:journal_ref>\n",
      "    <link href=\"http://arxiv.org/abs/2402.11919v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.11919v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.11748v1</id>\n",
      "    <updated>2024-02-19T00:21:13Z</updated>\n",
      "    <published>2024-02-19T00:21:13Z</published>\n",
      "    <title>Low-power SNN-based audio source localisation using a Hilbert Transform\n",
      "  spike encoding scheme</title>\n",
      "    <summary>  Sound source localisation is used in many consumer electronics devices, to\n",
      "help isolate audio from individual speakers and to reject noise. Localization\n",
      "is frequently accomplished by \"beamforming\" algorithms, which combine\n",
      "microphone audio streams to improve received signal power from particular\n",
      "incident source directions. Beamforming algorithms generally use knowledge of\n",
      "the frequency components of the audio source, along with the known microphone\n",
      "array geometry, to analytically phase-shift microphone streams before combining\n",
      "them. A dense set of band-pass filters is often used to obtain known-frequency\n",
      "\"narrowband\" components from wide-band audio streams. These approaches achieve\n",
      "high accuracy, but state of the art narrowband beamforming algorithms are\n",
      "computationally demanding, and are therefore difficult to integrate into\n",
      "low-power IoT devices. We demonstrate a novel method for sound source\n",
      "localisation in arbitrary microphone arrays, designed for efficient\n",
      "implementation in ultra-low-power spiking neural networks (SNNs). We use a\n",
      "novel short-time Hilbert transform (STHT) to remove the need for demanding\n",
      "band-pass filtering of audio, and introduce a new accompanying method for audio\n",
      "encoding with spiking events. Our beamforming and localisation approach\n",
      "achieves state-of-the-art accuracy for SNN methods, and comparable with\n",
      "traditional non-SNN super-resolution approaches. We deploy our method to\n",
      "low-power SNN audio inference hardware, and achieve much lower power\n",
      "consumption compared with super-resolution methods. We demonstrate that signal\n",
      "processing approaches can be co-designed with spiking neural network\n",
      "implementations to achieve high levels of power efficiency. Our new\n",
      "Hilbert-transform-based method for beamforming promises to also improve the\n",
      "efficiency of traditional DSP-based signal processing.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Saeid Haghighatshoar</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dylan R Muir</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2402.11748v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.11748v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.11747v1</id>\n",
      "    <updated>2024-02-19T00:21:07Z</updated>\n",
      "    <published>2024-02-19T00:21:07Z</published>\n",
      "    <title>Parameter Efficient Finetuning for Speech Emotion Recognition and Domain\n",
      "  Adaptation</title>\n",
      "    <summary>  Foundation models have shown superior performance for speech emotion\n",
      "recognition (SER). However, given the limited data in emotion corpora,\n",
      "finetuning all parameters of large pre-trained models for SER can be both\n",
      "resource-intensive and susceptible to overfitting. This paper investigates\n",
      "parameter-efficient finetuning (PEFT) for SER. Various PEFT adaptors are\n",
      "systematically studied for both classification of discrete emotion categories\n",
      "and prediction of dimensional emotional attributes. The results demonstrate\n",
      "that the combination of PEFT methods surpasses full finetuning with a\n",
      "significant reduction in the number of trainable parameters. Furthermore, a\n",
      "two-stage adaptation strategy is proposed to adapt models trained on acted\n",
      "emotion data, which is more readily available, to make the model more adept at\n",
      "capturing natural emotional expressions. Both intra- and cross-corpus\n",
      "experiments validate the efficacy of the proposed approach in enhancing the\n",
      "performance on both the source and target domains.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Nineli Lashkarashvili</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wen Wu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Guangzhi Sun</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Philip C. Woodland</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2402.11747v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.11747v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.11330v1</id>\n",
      "    <updated>2024-02-17T16:59:08Z</updated>\n",
      "    <published>2024-02-17T16:59:08Z</published>\n",
      "    <title>Diffuse Sound Field Synthesis</title>\n",
      "    <summary>  Can uncorrelated surrounding sound sources be used to generate extended\n",
      "diffuse sound fields? By definition, targets are a constant sound pressure\n",
      "level, a vanishing average sound intensity, uncorrelated sound waves arriving\n",
      "isotropically from all directions. Does this require specific sources and\n",
      "geometries for surrounding 2D and 3D source layouts?\n",
      "  As methods, we employ numeric simulations and undertake a series of\n",
      "calculations with uncorrelated circular/spherical source layouts, or such with\n",
      "infinite excess dimensions, and we point out relations to potential theory.\n",
      "Using a radial decay 1/r^b modified by the exponent b, the representation of\n",
      "the resulting fields with hypergeometric functions, Gegenbauer polynomials, and\n",
      "circular as well as spherical harmonics yields fruitful insights.\n",
      "  In circular layouts, waves decaying by the exponent b=1/2 synthesize ideally\n",
      "extended, diffuse sound fields; spherical layouts do so with b=1. None of the\n",
      "layouts synthesizes a perfectly constant expected sound pressure level but its\n",
      "flatness is acceptable.\n",
      "  Spherical t-designs describe optimal source layouts with well-described area\n",
      "of high diffuseness, and non-spherical, convex layouts can be improved by\n",
      "restoring isotropy or by mode matching for a maximally diffuse synthesis.\n",
      "  Theory and simulation offer a basis for loudspeaker-based synthesis of\n",
      "diffuse sound fields and contribute physical reasons to recent psychoacoustic\n",
      "findings in spatial audio.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Franz Zotter</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Stefan Riedel</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Lukas Glles</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Matthias Frank</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">27 pages, 17 figures, submitted to acta acustica nov 20th 2023,\n",
      "  including jan/feb 2024 upgrades while awaiting the reviews</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2402.11330v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.11330v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.13199v1</id>\n",
      "    <updated>2024-02-17T13:45:00Z</updated>\n",
      "    <published>2024-02-17T13:45:00Z</published>\n",
      "    <title>Target Speech Extraction with Pre-trained Self-supervised Learning\n",
      "  Models</title>\n",
      "    <summary>  Pre-trained self-supervised learning (SSL) models have achieved remarkable\n",
      "success in various speech tasks. However, their potential in target speech\n",
      "extraction (TSE) has not been fully exploited. TSE aims to extract the speech\n",
      "of a target speaker in a mixture guided by enrollment utterances. We exploit\n",
      "pre-trained SSL models for two purposes within a TSE framework, i.e., to\n",
      "process the input mixture and to derive speaker embeddings from the enrollment.\n",
      "In this paper, we focus on how to effectively use SSL models for TSE. We first\n",
      "introduce a novel TSE downstream task following the SUPERB principles. This\n",
      "simple experiment shows the potential of SSL models for TSE, but extraction\n",
      "performance remains far behind the state-of-the-art. We then extend a powerful\n",
      "TSE architecture by incorporating two SSL-based modules: an Adaptive Input\n",
      "Enhancer (AIE) and a speaker encoder. Specifically, the proposed AIE utilizes\n",
      "intermediate representations from the CNN encoder by adjusting the time\n",
      "resolution of CNN encoder and transformer blocks through progressive\n",
      "upsampling, capturing both fine-grained and hierarchical features. Our method\n",
      "outperforms current TSE systems achieving a SI-SDR improvement of 14.0 dB on\n",
      "LibriMix. Moreover, we can further improve performance by 0.7 dB by fine-tuning\n",
      "the whole model including the SSL model parameters.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Junyi Peng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Marc Delcroix</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tsubasa Ochiai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Oldrich Plchot</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shoko Araki</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jan Cernocky</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted to ICASSP 2024</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2402.13199v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.13199v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.13200v1</id>\n",
      "    <updated>2024-02-17T13:37:22Z</updated>\n",
      "    <published>2024-02-17T13:37:22Z</published>\n",
      "    <title>Probing Self-supervised Learning Models with Target Speech Extraction</title>\n",
      "    <summary>  Large-scale pre-trained self-supervised learning (SSL) models have shown\n",
      "remarkable advancements in speech-related tasks. However, the utilization of\n",
      "these models in complex multi-talker scenarios, such as extracting a target\n",
      "speaker in a mixture, is yet to be fully evaluated. In this paper, we introduce\n",
      "target speech extraction (TSE) as a novel downstream task to evaluate the\n",
      "feature extraction capabilities of pre-trained SSL models. TSE uniquely\n",
      "requires both speaker identification and speech separation, distinguishing it\n",
      "from other tasks in the Speech processing Universal PERformance Benchmark\n",
      "(SUPERB) evaluation. Specifically, we propose a TSE downstream model composed\n",
      "of two lightweight task-oriented modules based on the same frozen SSL model.\n",
      "One module functions as a speaker encoder to obtain target speaker information\n",
      "from an enrollment speech, while the other estimates the target speaker's mask\n",
      "to extract its speech from the mixture. Experimental results on the Libri2mix\n",
      "datasets reveal the relevance of the TSE downstream task to probe SSL models,\n",
      "as its performance cannot be simply deduced from other related tasks such as\n",
      "speaker verification and separation.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Junyi Peng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Marc Delcroix</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tsubasa Ochiai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Oldrich Plchot</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Takanori Ashihara</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shoko Araki</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jan Cernocky</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted to ICASSP 2024, Self-supervision in Audio, Speech, and\n",
      "  Beyond (SASB) workshop</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2402.13200v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.13200v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.11216v1</id>\n",
      "    <updated>2024-02-17T07:52:21Z</updated>\n",
      "    <published>2024-02-17T07:52:21Z</published>\n",
      "    <title>Feedback Delay Network Optimization</title>\n",
      "    <summary>  A common bane of artificial reverberation algorithms is spectral coloration,\n",
      "typically manifesting as metallic ringing, leading to a degradation in the\n",
      "perceived sound quality. This paper presents an optimization framework where a\n",
      "differentiable feedback delay network is used to learn a set of parameters to\n",
      "reduce coloration iteratively. The parameters under optimization include the\n",
      "feedback matrix, as well as the input and output gains. The optimization\n",
      "objective is twofold: to maximize spectral flatness through a spectral loss\n",
      "while maintaining temporal density by penalizing sparseness in the parameter\n",
      "values. A favorable narrower distribution of modal excitation is achieved while\n",
      "maintaining the desired impulse response density. In a subjective assessment,\n",
      "the new method proves effective in reducing perceptual coloration of late\n",
      "reverberation. The proposed method achieves computational savings compared to\n",
      "the baseline while preserving its performance. The effectiveness of this work\n",
      "is demonstrated through two application scenarios where natural-sounding\n",
      "synthetic impulse responses are obtained via the introduction of attenuation\n",
      "filters and an optimizable scattering feedback matrix.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Gloria Dal Santo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Karolina Prawda</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sebastian J. Schlecht</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vesa Vlimki</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2402.11216v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.11216v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.10547v1</id>\n",
      "    <updated>2024-02-16T10:20:42Z</updated>\n",
      "    <published>2024-02-16T10:20:42Z</published>\n",
      "    <title>Learning Disentangled Audio Representations through Controlled Synthesis</title>\n",
      "    <summary>  This paper tackles the scarcity of benchmarking data in disentangled auditory\n",
      "representation learning. We introduce SynTone, a synthetic dataset with\n",
      "explicit ground truth explanatory factors for evaluating disentanglement\n",
      "techniques. Benchmarking state-of-the-art methods on SynTone highlights its\n",
      "utility for method evaluation. Our results underscore strengths and limitations\n",
      "in audio disentanglement, motivating future research.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Yusuf Brima</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ulf Krumnack</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Simone Pika</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Gunther Heidemann</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">12 pages, 12 figures, accepted as a Tiny paper at ICLR 2024</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2402.10547v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.10547v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.10533v1</id>\n",
      "    <updated>2024-02-16T09:38:16Z</updated>\n",
      "    <published>2024-02-16T09:38:16Z</published>\n",
      "    <title>APCodec: A Neural Audio Codec with Parallel Amplitude and Phase Spectrum\n",
      "  Encoding and Decoding</title>\n",
      "    <summary>  This paper introduces a novel neural audio codec targeting high waveform\n",
      "sampling rates and low bitrates named APCodec, which seamlessly integrates the\n",
      "strengths of parametric codecs and waveform codecs. The APCodec revolutionizes\n",
      "the process of audio encoding and decoding by concurrently handling the\n",
      "amplitude and phase spectra as audio parametric characteristics like parametric\n",
      "codecs. It is composed of an encoder and a decoder with the modified ConvNeXt\n",
      "v2 network as the backbone, connected by a quantizer based on the residual\n",
      "vector quantization (RVQ) mechanism. The encoder compresses the audio amplitude\n",
      "and phase spectra in parallel, amalgamating them into a continuous latent code\n",
      "at a reduced temporal resolution. This code is subsequently quantized by the\n",
      "quantizer. Ultimately, the decoder reconstructs the audio amplitude and phase\n",
      "spectra in parallel, and the decoded waveform is obtained by inverse short-time\n",
      "Fourier transform. To ensure the fidelity of decoded audio like waveform\n",
      "codecs, spectral-level loss, quantization loss, and generative adversarial\n",
      "network (GAN) based loss are collectively employed for training the APCodec. To\n",
      "support low-latency streamable inference, we employ feed-forward layers and\n",
      "causal convolutional layers in APCodec, incorporating a knowledge distillation\n",
      "training strategy to enhance the quality of decoded audio. Experimental results\n",
      "confirm that our proposed APCodec can encode 48 kHz audio at bitrate of just 6\n",
      "kbps, with no significant degradation in the quality of the decoded audio. At\n",
      "the same bitrate, our proposed APCodec also demonstrates superior decoded audio\n",
      "quality and faster generation speed compared to well-known codecs, such as\n",
      "SoundStream, Encodec, HiFi-Codec and AudioDec.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Yang Ai</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiao-Hang Jiang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ye-Xin Lu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hui-Peng Du</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhen-Hua Ling</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n",
      "  Processing</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2402.10533v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.10533v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.10427v1</id>\n",
      "    <updated>2024-02-16T03:30:27Z</updated>\n",
      "    <published>2024-02-16T03:30:27Z</published>\n",
      "    <title>Evaluating and Improving Continual Learning in Spoken Language\n",
      "  Understanding</title>\n",
      "    <summary>  Continual learning has emerged as an increasingly important challenge across\n",
      "various tasks, including Spoken Language Understanding (SLU). In SLU, its\n",
      "objective is to effectively handle the emergence of new concepts and evolving\n",
      "environments. The evaluation of continual learning algorithms typically\n",
      "involves assessing the model's stability, plasticity, and generalizability as\n",
      "fundamental aspects of standards. However, existing continual learning metrics\n",
      "primarily focus on only one or two of the properties. They neglect the overall\n",
      "performance across all tasks, and do not adequately disentangle the plasticity\n",
      "versus stability/generalizability trade-offs within the model. In this work, we\n",
      "propose an evaluation methodology that provides a unified evaluation on\n",
      "stability, plasticity, and generalizability in continual learning. By employing\n",
      "the proposed metric, we demonstrate how introducing various knowledge\n",
      "distillations can improve different aspects of these three properties of the\n",
      "SLU model. We further show that our proposed metric is more sensitive in\n",
      "capturing the impact of task ordering in continual learning, making it better\n",
      "suited for practical use-case scenarios.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Muqiao Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiang Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Umberto Cappellazzo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shinji Watanabe</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bhiksha Raj</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2402.10427v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.10427v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.10168v1</id>\n",
      "    <updated>2024-02-15T18:11:02Z</updated>\n",
      "    <published>2024-02-15T18:11:02Z</published>\n",
      "    <title>DeepSRGM -- Sequence Classification and Ranking in Indian Classical\n",
      "  Music with Deep Learning</title>\n",
      "    <summary>  A vital aspect of Indian Classical Music (ICM) is Raga, which serves as a\n",
      "melodic framework for compositions and improvisations alike. Raga Recognition\n",
      "is an important music information retrieval task in ICM as it can aid numerous\n",
      "downstream applications ranging from music recommendations to organizing huge\n",
      "music collections. In this work, we propose a deep learning based approach to\n",
      "Raga recognition. Our approach employs efficient pre possessing and learns\n",
      "temporal sequences in music data using Long Short Term Memory based Recurrent\n",
      "Neural Networks (LSTM-RNN). We train and test the network on smaller sequences\n",
      "sampled from the original audio while the final inference is performed on the\n",
      "audio as a whole. Our method achieves an accuracy of 88.1% and 97 % during\n",
      "inference on the Comp Music Carnatic dataset and its 10 Raga subset\n",
      "respectively making it the state-of-the-art for the Raga recognition task. Our\n",
      "approach also enables sequence ranking which aids us in retrieving melodic\n",
      "patterns from a given music data base that are closely related to the presented\n",
      "query sequence.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Sathwik Tejaswi Madhusudhan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Girish Chowdhary</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2402.10168v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.10168v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.10009v2</id>\n",
      "    <updated>2024-02-16T09:49:10Z</updated>\n",
      "    <published>2024-02-15T15:17:26Z</published>\n",
      "    <title>Zero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion</title>\n",
      "    <summary>  Editing signals using large pre-trained models, in a zero-shot manner, has\n",
      "recently seen rapid advancements in the image domain. However, this wave has\n",
      "yet to reach the audio domain. In this paper, we explore two zero-shot editing\n",
      "techniques for audio signals, which use DDPM inversion on pre-trained diffusion\n",
      "models. The first, adopted from the image domain, allows text-based editing.\n",
      "The second, is a novel approach for discovering semantically meaningful editing\n",
      "directions without supervision. When applied to music signals, this method\n",
      "exposes a range of musically interesting modifications, from controlling the\n",
      "participation of specific instruments to improvisations on the melody. Samples\n",
      "and code can be found on our examples page in\n",
      "https://hilamanor.github.io/AudioEditing/ .\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Hila Manor</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tomer Michaeli</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Examples and code available in\n",
      "  https://hilamanor.github.io/AudioEditing/</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2402.10009v2\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.10009v2\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.09871v1</id>\n",
      "    <updated>2024-02-15T10:55:01Z</updated>\n",
      "    <published>2024-02-15T10:55:01Z</published>\n",
      "    <title>MuChin: A Chinese Colloquial Description Benchmark for Evaluating\n",
      "  Language Models in the Field of Music</title>\n",
      "    <summary>  The rapidly evolving multimodal Large Language Models (LLMs) urgently require\n",
      "new benchmarks to uniformly evaluate their performance on understanding and\n",
      "textually describing music. However, due to semantic gaps between Music\n",
      "Information Retrieval (MIR) algorithms and human understanding, discrepancies\n",
      "between professionals and the public, and low precision of annotations,\n",
      "existing music description datasets cannot serve as benchmarks. To this end, we\n",
      "present MuChin, the first open-source music description benchmark in Chinese\n",
      "colloquial language, designed to evaluate the performance of multimodal LLMs in\n",
      "understanding and describing music. We established the Caichong Music\n",
      "Annotation Platform (CaiMAP) that employs an innovative multi-person,\n",
      "multi-stage assurance method, and recruited both amateurs and professionals to\n",
      "ensure the precision of annotations and alignment with popular semantics.\n",
      "Utilizing this method, we built a dataset with multi-dimensional,\n",
      "high-precision music annotations, the Caichong Music Dataset (CaiMD), and\n",
      "carefully selected 1,000 high-quality entries to serve as the test set for\n",
      "MuChin. Based on MuChin, we analyzed the discrepancies between professionals\n",
      "and amateurs in terms of music description, and empirically demonstrated the\n",
      "effectiveness of annotated data for fine-tuning LLMs. Ultimately, we employed\n",
      "MuChin to evaluate existing music understanding models on their ability to\n",
      "provide colloquial descriptions of music. All data related to the benchmark and\n",
      "the code for scoring have been open-sourced.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Zihao Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shuyu Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Tao Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Qi Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pengfei Yu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jinyang Luo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yan Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ming Xi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Kejun Zhang</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2402.09871v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.09871v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"68Txx(Primary)14F05, 91Fxx(Secondary)\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"I.2.7; J.5\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.10247v1</id>\n",
      "    <updated>2024-02-15T10:28:59Z</updated>\n",
      "    <published>2024-02-15T10:28:59Z</published>\n",
      "    <title>Engraving Oriented Joint Estimation of Pitch Spelling and Local and\n",
      "  Global Keys</title>\n",
      "    <summary>  We revisit the problems of pitch spelling and tonality guessing with a new\n",
      "algorithm for their joint estimation from a MIDI file including information\n",
      "about the measure boundaries. Our algorithm does not only identify a global key\n",
      "but also local ones all along the analyzed piece. It uses Dynamic Programming\n",
      "techniques to search for an optimal spelling in term, roughly, of the number of\n",
      "accidental symbols that would be displayed in the engraved score. The\n",
      "evaluation of this number is coupled with an estimation of the global key and\n",
      "some local keys, one for each measure. Each of the three informations is used\n",
      "for the estimation of the other, in a multi-steps procedure. An evaluation\n",
      "conducted on a monophonic and a piano dataset, comprising 216 464 notes in\n",
      "total, shows a high degree of accuracy, both for pitch spelling (99.5% on\n",
      "average on the Bach corpus and 98.2% on the whole dataset) and global key\n",
      "signature estimation (93.0% on average, 95.58% on the piano dataset). Designed\n",
      "originally as a backend tool in a music transcription framework, this method\n",
      "should also be useful in other tasks related to music notation processing.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Augustin Bouquillard</name>\n",
      "      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">CEDRIC - VERTIGO</arxiv:affiliation>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Florent Jacquemard</name>\n",
      "      <arxiv:affiliation xmlns:arxiv=\"http://arxiv.org/schemas/atom\">CEDRIC - VERTIGO</arxiv:affiliation>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">International Conference on Technologies for Music Notation and\n",
      "  Representation (TENOR), Apr 2024, Zurich (CH), Switzerland</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2402.10247v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.10247v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.09821v1</id>\n",
      "    <updated>2024-02-15T09:36:36Z</updated>\n",
      "    <published>2024-02-15T09:36:36Z</published>\n",
      "    <title>Diffusion Models for Audio Restoration</title>\n",
      "    <summary>  With the development of audio playback devices and fast data transmission,\n",
      "the demand for high sound quality is rising, for both entertainment and\n",
      "communications. In this quest for better sound quality, challenges emerge from\n",
      "distortions and interferences originating at the recording side or caused by an\n",
      "imperfect transmission pipeline. To address this problem, audio restoration\n",
      "methods aim to recover clean sound signals from the corrupted input data. We\n",
      "present here audio restoration algorithms based on diffusion models, with a\n",
      "focus on speech enhancement and music restoration tasks. Traditional\n",
      "approaches, often grounded in handcrafted rules and statistical heuristics,\n",
      "have shaped our understanding of audio signals. In the past decades, there has\n",
      "been a notable shift towards data-driven methods that exploit the modeling\n",
      "capabilities of deep neural networks (DNNs). Deep generative models, and among\n",
      "them diffusion models, have emerged as powerful techniques for learning complex\n",
      "data distributions. However, relying solely on DNN-based learning approaches\n",
      "carries the risk of reducing interpretability, particularly when employing\n",
      "end-to-end models. Nonetheless, data-driven approaches allow more flexibility\n",
      "in comparison to statistical model-based frameworks whose performance depends\n",
      "on distributional and statistical assumptions that can be difficult to\n",
      "guarantee. Here, we aim to show that diffusion models can combine the best of\n",
      "both worlds and offer the opportunity to design audio restoration algorithms\n",
      "with a good degree of interpretability and a remarkable performance in terms of\n",
      "sound quality.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Jean-Marie Lemercier</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Julius Richter</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Simon Welker</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Eloi Moliner</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vesa Vlimki</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Timo Gerkmann</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Full paper invited to the IEEE Signal Processing Magazine Special\n",
      "  Issue \"Model-based and Data-Driven Audio Signal Processing\"</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2402.09821v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.09821v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.09797v1</id>\n",
      "    <updated>2024-02-15T08:52:31Z</updated>\n",
      "    <published>2024-02-15T08:52:31Z</published>\n",
      "    <title>A cross-talk robust multichannel VAD model for multiparty agent\n",
      "  interactions trained using synthetic re-recordings</title>\n",
      "    <summary>  In this work, we propose a novel cross-talk rejection framework for a\n",
      "multi-channel multi-talker setup for a live multiparty interactive show. Our\n",
      "far-field audio setup is required to be hands-free during live interaction and\n",
      "comprises four adjacent talkers with directional microphones in the same space.\n",
      "Such setups often introduce heavy cross-talk between channels, resulting in\n",
      "reduced automatic speech recognition (ASR) and natural language understanding\n",
      "(NLU) performance. To address this problem, we propose voice activity detection\n",
      "(VAD) model for all talkers using multichannel information, which is then used\n",
      "to filter audio for downstream tasks. We adopt a synthetic training data\n",
      "generation approach through playback and re-recording for such scenarios,\n",
      "simulating challenging speech overlap conditions. We train our models on this\n",
      "synthetic data and demonstrate that our approach outperforms single-channel VAD\n",
      "models and energy-based multi-channel VAD algorithm in various acoustic\n",
      "environments. In addition to VAD results, we also present multiparty ASR\n",
      "evaluation results to highlight the impact of using our VAD model for filtering\n",
      "audio in downstream tasks by significantly reducing the insertion error.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Hyewon Han</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Naveen Kumar</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted for presentation at the Hands-free Speech Communication and\n",
      "  Microphone Arrays (HSCMA 2024)</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2402.09797v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.09797v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.09585v1</id>\n",
      "    <updated>2024-02-14T21:25:06Z</updated>\n",
      "    <published>2024-02-14T21:25:06Z</published>\n",
      "    <title>Domain Adaptation for Contrastive Audio-Language Models</title>\n",
      "    <summary>  Audio-Language Models (ALM) aim to be general-purpose audio models by\n",
      "providing zero-shot capabilities at test time. The zero-shot performance of ALM\n",
      "improves by using suitable text prompts for each domain. The text prompts are\n",
      "usually hand-crafted through an ad-hoc process and lead to a drop in ALM\n",
      "generalization and out-of-distribution performance. Existing approaches to\n",
      "improve domain performance, like few-shot learning or fine-tuning, require\n",
      "access to annotated data and iterations of training. Therefore, we propose a\n",
      "test-time domain adaptation method for ALMs that does not require access to\n",
      "annotations. Our method learns a domain vector by enforcing consistency across\n",
      "augmented views of the testing audio. We extensively evaluate our approach on\n",
      "12 downstream tasks across domains. With just one example, our domain\n",
      "adaptation method leads to 3.2% (max 8.4%) average zero-shot performance\n",
      "improvement. After adaptation, the model still retains the generalization\n",
      "property of ALMs.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Soham Deshmukh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rita Singh</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Bhiksha Raj</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2402.09585v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.09585v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.09508v1</id>\n",
      "    <updated>2024-02-14T19:00:01Z</updated>\n",
      "    <published>2024-02-14T19:00:01Z</published>\n",
      "    <title>Arrange, Inpaint, and Refine: Steerable Long-term Music Audio Generation\n",
      "  and Editing via Content-based Controls</title>\n",
      "    <summary>  Controllable music generation plays a vital role in human-AI music\n",
      "co-creation. While Large Language Models (LLMs) have shown promise in\n",
      "generating high-quality music, their focus on autoregressive generation limits\n",
      "their utility in music editing tasks. To bridge this gap, we introduce a novel\n",
      "Parameter-Efficient Fine-Tuning (PEFT) method. This approach enables\n",
      "autoregressive language models to seamlessly address music inpainting tasks.\n",
      "Additionally, our PEFT method integrates frame-level content-based controls,\n",
      "facilitating track-conditioned music refinement and score-conditioned music\n",
      "arrangement. We apply this method to fine-tune MusicGen, a leading\n",
      "autoregressive music generation model. Our experiments demonstrate promising\n",
      "results across multiple music editing tasks, offering more flexible controls\n",
      "for future AI-driven music editing tools. A demo\n",
      "page\\footnote{\\url{https://kikyo-16.github.io/AIR/}.} showcasing our work and\n",
      "source codes\\footnote{\\url{https://github.com/Kikyo-16/airgen}.} are available\n",
      "online.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Liwei Lin</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Gus Xia</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yixiao Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Junyan Jiang</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2402.09508v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.09508v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.09378v1</id>\n",
      "    <updated>2024-02-14T18:24:41Z</updated>\n",
      "    <published>2024-02-14T18:24:41Z</published>\n",
      "    <title>MobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot\n",
      "  Text-to-Speech</title>\n",
      "    <summary>  Zero-shot text-to-speech (TTS) has gained significant attention due to its\n",
      "powerful voice cloning capabilities, requiring only a few seconds of unseen\n",
      "speaker voice prompts. However, all previous work has been developed for\n",
      "cloud-based systems. Taking autoregressive models as an example, although these\n",
      "approaches achieve high-fidelity voice cloning, they fall short in terms of\n",
      "inference speed, model size, and robustness. Therefore, we propose\n",
      "MobileSpeech, which is a fast, lightweight, and robust zero-shot text-to-speech\n",
      "system based on mobile devices for the first time. Specifically: 1) leveraging\n",
      "discrete codec, we design a parallel speech mask decoder module called SMD,\n",
      "which incorporates hierarchical information from the speech codec and weight\n",
      "mechanisms across different codec layers during the generation process.\n",
      "Moreover, to bridge the gap between text and speech, we introduce a high-level\n",
      "probabilistic mask that simulates the progression of information flow from less\n",
      "to more during speech generation. 2) For speaker prompts, we extract\n",
      "fine-grained prompt duration from the prompt speech and incorporate text,\n",
      "prompt speech by cross attention in SMD. We demonstrate the effectiveness of\n",
      "MobileSpeech on multilingual datasets at different levels, achieving\n",
      "state-of-the-art results in terms of generating speed and speech quality.\n",
      "MobileSpeech achieves RTF of 0.09 on a single A100 GPU and we have successfully\n",
      "deployed MobileSpeech on mobile devices. Audio samples are available at\n",
      "\\url{https://mobilespeech.github.io/} .\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Shengpeng Ji</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ziyue Jiang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Hanting Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jialong Zuo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhou Zhao</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2402.09378v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.09378v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.09318v1</id>\n",
      "    <updated>2024-02-14T17:13:36Z</updated>\n",
      "    <published>2024-02-14T17:13:36Z</published>\n",
      "    <title>Leveraging Pre-Trained Autoencoders for Interpretable Prototype Learning\n",
      "  of Music Audio</title>\n",
      "    <summary>  We present PECMAE, an interpretable model for music audio classification\n",
      "based on prototype learning. Our model is based on a previous method, APNet,\n",
      "which jointly learns an autoencoder and a prototypical network. Instead, we\n",
      "propose to decouple both training processes. This enables us to leverage\n",
      "existing self-supervised autoencoders pre-trained on much larger data\n",
      "(EnCodecMAE), providing representations with better generalization. APNet\n",
      "allows prototypes' reconstruction to waveforms for interpretability relying on\n",
      "the nearest training data samples. In contrast, we explore using a diffusion\n",
      "decoder that allows reconstruction without such dependency. We evaluate our\n",
      "method on datasets for music instrument classification (Medley-Solos-DB) and\n",
      "genre recognition (GTZAN and a larger in-house dataset), the latter being a\n",
      "more challenging task not addressed with prototypical networks before. We find\n",
      "that the prototype-based models preserve most of the performance achieved with\n",
      "the autoencoder embeddings, while the sonification of prototypes benefits\n",
      "understanding the behavior of the classifier.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Pablo Alonso-Jimnez</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Leonardo Pepino</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Roser Batlle-Roca</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pablo Zinemanas</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dmitry Bogdanov</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xavier Serra</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Martn Rocamora</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2402.09318v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.09318v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.08932v1</id>\n",
      "    <updated>2024-02-14T04:21:11Z</updated>\n",
      "    <published>2024-02-14T04:21:11Z</published>\n",
      "    <title>Listening to Multi-talker Conversations: Modular and End-to-end\n",
      "  Perspectives</title>\n",
      "    <summary>  Since the first speech recognition systems were built more than 30 years ago,\n",
      "improvement in voice technology has enabled applications such as smart\n",
      "assistants and automated customer support. However, conversation intelligence\n",
      "of the future requires recognizing free-flowing multi-party conversations,\n",
      "which is a crucial and challenging component that still remains unsolved. In\n",
      "this dissertation, we focus on this problem of speaker-attributed multi-talker\n",
      "speech recognition, and propose two perspectives which result from its\n",
      "probabilistic formulation.\n",
      "  In the modular perspective, we build a pipeline of sub-tasks involving\n",
      "speaker diarization, target speaker extraction, and speech recognition. Our\n",
      "first contribution is a method to perform overlap-aware diarization by\n",
      "reformulating spectral clustering as a constrained optimization problem. We\n",
      "also describe an algorithm to ensemble diarization outputs, either to combine\n",
      "overlap-aware systems or to perform multi-channel diarization by late fusion.\n",
      "Once speaker segments are identified, we robustly extract single-speaker\n",
      "utterances from the mixture using a GPU-accelerated implementation of guided\n",
      "source separation, which allows us to use an off-the-shelf ASR system to obtain\n",
      "speaker-attributed transcripts.\n",
      "  Since the modular approach suffers from error propagation, we propose an\n",
      "alternate \"end-to-end\" perspective on the problem. For this, we describe the\n",
      "Streaming Unmixing and Recognition Transducer (SURT). We show how to train SURT\n",
      "models efficiently by carefully designing the network architecture, objective\n",
      "functions, and mixture simulation techniques. Finally, we add an auxiliary\n",
      "speaker branch to enable joint prediction of speaker labels synchronized with\n",
      "the speech tokens. We demonstrate that training on synthetic mixtures and\n",
      "adapting with real data helps these models transfer well for streaming\n",
      "transcription of real meeting sessions.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Desh Raj</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Ph.D. dissertation</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2402.08932v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.08932v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.08904v1</id>\n",
      "    <updated>2024-02-14T02:32:18Z</updated>\n",
      "    <published>2024-02-14T02:32:18Z</published>\n",
      "    <title>Sound Field Reconstruction Using a Compact Acoustics-informed Neural\n",
      "  Network</title>\n",
      "    <summary>  Sound field reconstruction (SFR) augments the information of a sound field\n",
      "captured by a microphone array. Conventional SFR methods using basis function\n",
      "decomposition are straightforward and computationally efficient, but may\n",
      "require more microphones than needed to measure the sound field. Recent studies\n",
      "show that pure data-driven and learning-based methods are promising in some SFR\n",
      "tasks, but they are usually computationally heavy and may fail to reconstruct a\n",
      "physically valid sound field. This paper proposes a compact acoustics-informed\n",
      "neural network (AINN) method for SFR, whereby the Helmholtz equation is\n",
      "exploited to regularize the neural network. As opposed to pure data-driven\n",
      "approaches that solely rely on measured sound pressures, the integration of the\n",
      "Helmholtz equation improves robustness of the neural network against variations\n",
      "during the measurement processes and prompts the generation of physically valid\n",
      "reconstructions. The AINN is designed to be compact, and is able to predict not\n",
      "only the sound pressures but also sound pressure gradients within a spatial\n",
      "region of interest based on measured sound pressures along the boundary.\n",
      "Numerical experiments with acoustic transfer functions measured in different\n",
      "environments demonstrate the superiority of the AINN method over the\n",
      "traditional cylinder harmonic decomposition and the singular value\n",
      "decomposition methods.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Fei Ma</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sipei Zhao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ian S. Burnett</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2402.08904v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.08904v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.08898v1</id>\n",
      "    <updated>2024-02-14T02:11:04Z</updated>\n",
      "    <published>2024-02-14T02:11:04Z</published>\n",
      "    <title>UniEnc-CASSNAT: An Encoder-only Non-autoregressive ASR for Speech SSL\n",
      "  Models</title>\n",
      "    <summary>  Non-autoregressive automatic speech recognition (NASR) models have gained\n",
      "attention due to their parallelism and fast inference. The encoder-based NASR,\n",
      "e.g. connectionist temporal classification (CTC), can be initialized from the\n",
      "speech foundation models (SFM) but does not account for any dependencies among\n",
      "intermediate tokens. The encoder-decoder-based NASR, like CTC alignment-based\n",
      "single-step non-autoregressive transformer (CASS-NAT), can mitigate the\n",
      "dependency problem but is not able to efficiently integrate SFM. Inspired by\n",
      "the success of recent work of speech-text joint pre-training with a shared\n",
      "transformer encoder, we propose a new encoder-based NASR, UniEnc-CASSNAT, to\n",
      "combine the advantages of CTC and CASS-NAT. UniEnc-CASSNAT consists of only an\n",
      "encoder as the major module, which can be the SFM. The encoder plays the role\n",
      "of both the CASS-NAT encoder and decoder by two forward passes. The first pass\n",
      "of the encoder accepts the speech signal as input, while the concatenation of\n",
      "the speech signal and the token-level acoustic embedding is used as the input\n",
      "for the second pass. Examined on the Librispeech 100h, MyST, and Aishell1\n",
      "datasets, the proposed UniEnc-CASSNAT achieves state-of-the-art NASR results\n",
      "and is better or comparable to CASS-NAT with only an encoder and hence, fewer\n",
      "model parameters. Our codes are publicly available.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Ruchao Fan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Natarajan Balaji Shanka</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Abeer Alwan</name>\n",
      "    </author>\n",
      "    <arxiv:doi xmlns:arxiv=\"http://arxiv.org/schemas/atom\">10.1109/LSP.2024.3365036</arxiv:doi>\n",
      "    <link title=\"doi\" href=\"http://dx.doi.org/10.1109/LSP.2024.3365036\" rel=\"related\"/>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Published in IEEE Signal Processing Letters</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2402.08898v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.08898v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.08846v1</id>\n",
      "    <updated>2024-02-13T23:25:04Z</updated>\n",
      "    <published>2024-02-13T23:25:04Z</published>\n",
      "    <title>An Embarrassingly Simple Approach for LLM with Strong ASR Capacity</title>\n",
      "    <summary>  In this paper, we focus on solving one of the most important tasks in the\n",
      "field of speech processing, i.e., automatic speech recognition (ASR), with\n",
      "speech foundation encoders and large language models (LLM). Recent works have\n",
      "complex designs such as compressing the output temporally for the speech\n",
      "encoder, tackling modal alignment for the projector, and utilizing\n",
      "parameter-efficient fine-tuning for the LLM. We found that delicate designs are\n",
      "not necessary, while an embarrassingly simple composition of off-the-shelf\n",
      "speech encoder, LLM, and the only trainable linear projector is competent for\n",
      "the ASR task. To be more specific, we benchmark and explore various\n",
      "combinations of LLMs and speech encoders, leading to the optimal LLM-based ASR\n",
      "system, which we call SLAM-ASR. The proposed SLAM-ASR provides a clean setup\n",
      "and little task-specific design, where only the linear projector is trained. To\n",
      "the best of our knowledge, SLAM-ASR achieves the best performance on the\n",
      "Librispeech benchmark among LLM-based ASR models and even outperforms the\n",
      "latest LLM-based audio-universal model trained on massive pair data. Finally,\n",
      "we explore the capability emergence of LLM-based ASR in the process of modal\n",
      "alignment. We hope that our study can facilitate the research on extending LLM\n",
      "with cross-modality capacity and shed light on the LLM-based ASR community.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Ziyang Ma</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Guanrou Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yifan Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhifu Gao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jiaming Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhihao Du</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Fan Yu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Qian Chen</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Siqi Zheng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shiliang Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xie Chen</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Working in progress and will open-source soon</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2402.08846v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.08846v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.MM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.08788v1</id>\n",
      "    <updated>2024-02-13T20:54:24Z</updated>\n",
      "    <published>2024-02-13T20:54:24Z</published>\n",
      "    <title>Syllable based DNN-HMM Cantonese Speech to Text System</title>\n",
      "    <summary>  This paper reports our work on building up a Cantonese Speech-to-Text (STT)\n",
      "system with a syllable based acoustic model. This is a part of an effort in\n",
      "building a STT system to aid dyslexic students who have cognitive deficiency in\n",
      "writing skills but have no problem expressing their ideas through speech. For\n",
      "Cantonese speech recognition, the basic unit of acoustic models can either be\n",
      "the conventional Initial-Final (IF) syllables, or the Onset-Nucleus-Coda (ONC)\n",
      "syllables where finals are further split into nucleus and coda to reflect the\n",
      "intra-syllable variations in Cantonese. By using the Kaldi toolkit, our system\n",
      "is trained using the stochastic gradient descent optimization model with the\n",
      "aid of GPUs for the hybrid Deep Neural Network and Hidden Markov Model\n",
      "(DNN-HMM) with and without I-vector based speaker adaptive training technique.\n",
      "The input features of the same Gaussian Mixture Model with speaker adaptive\n",
      "training (GMM-SAT) to DNN are used in all cases. Experiments show that the\n",
      "ONC-based syllable acoustic modeling with I-vector based DNN-HMM achieves the\n",
      "best performance with the word error rate (WER) of 9.66% and the real time\n",
      "factor (RTF) of 1.38812.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Timothy Wong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Claire Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sam Lam</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Billy Chiu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Qin Lu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Minglei Li</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Dan Xiong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Roy Shing Yu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vincent T. Y. Ng</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">7 pages, 3 figures, LREC 2016</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2402.08788v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.08788v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"94-06\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"I.2.7\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.08521v1</id>\n",
      "    <updated>2024-02-13T15:24:19Z</updated>\n",
      "    <published>2024-02-13T15:24:19Z</published>\n",
      "    <title>Benchmarking multi-component signal processing methods in the\n",
      "  time-frequency plane</title>\n",
      "    <summary>  Signal processing in the time-frequency plane has a long history and remains\n",
      "a field of methodological innovation. For instance, detection and denoising\n",
      "based on the zeros of the spectrogram have been proposed since 2015,\n",
      "contrasting with a long history of focusing on larger values of the\n",
      "spectrogram. Yet, unlike neighboring fields like optimization and machine\n",
      "learning, time-frequency signal processing lacks widely-adopted benchmarking\n",
      "tools. In this work, we contribute an open-source, Python-based toolbox termed\n",
      "MCSM-Benchs for benchmarking multi-component signal analysis methods, and we\n",
      "demonstrate our toolbox on three time-frequency benchmarks. First, we compare\n",
      "different methods for signal detection based on the zeros of the spectrogram,\n",
      "including unexplored variations of previously proposed detection tests. Second,\n",
      "we compare zero-based denoising methods to both classical and novel methods\n",
      "based on large values and ridges of the spectrogram. Finally, we compare the\n",
      "denoising performance of these methods against typical spectrogram thresholding\n",
      "strategies, in terms of post-processing artifacts commonly referred to as\n",
      "musical noise. At a low level, the obtained results provide new insight on the\n",
      "assessed approaches, and in particular research directions to further develop\n",
      "zero-based methods. At a higher level, our benchmarks exemplify the benefits of\n",
      "using a public, collaborative, common framework for benchmarking.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Juan M. Miramont</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Rmi Bardenet</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pierre Chainais</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Francois Auger</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2402.08521v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.08521v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.08312v1</id>\n",
      "    <updated>2024-02-13T09:16:40Z</updated>\n",
      "    <published>2024-02-13T09:16:40Z</published>\n",
      "    <title>Channel-Combination Algorithms for Robust Distant Voice Activity and\n",
      "  Overlapped Speech Detection</title>\n",
      "    <summary>  Voice Activity Detection (VAD) and Overlapped Speech Detection (OSD) are key\n",
      "pre-processing tasks for speaker diarization. In the meeting context, it is\n",
      "often easier to capture speech with a distant device. This consideration\n",
      "however leads to severe performance degradation. We study a unified supervised\n",
      "learning framework to solve distant multi-microphone joint VAD and OSD\n",
      "(VAD+OSD). This paper investigates various multi-channel VAD+OSD front-ends\n",
      "that weight and combine incoming channels. We propose three algorithms based on\n",
      "the Self-Attention Channel Combinator (SACC), previously proposed in the\n",
      "literature. Experiments conducted on the AMI meeting corpus exhibit that\n",
      "channel combination approaches bring significant VAD+OSD improvements in the\n",
      "distant speech scenario. Specifically, we explore the use of learned complex\n",
      "combination weights and demonstrate the benefits of such an approach in terms\n",
      "of explainability. Channel combination-based VAD+OSD systems are evaluated on\n",
      "the final back-end task, i.e. speaker diarization, and show significant\n",
      "improvements. Finally, since multi-channel systems are trained given a fixed\n",
      "array configuration, they may fail in generalizing to other array set-ups, e.g.\n",
      "mismatched number of microphones. A channel-number invariant loss is proposed\n",
      "to learn a unique feature representation regardless of the number of available\n",
      "microphones. The evaluation conducted on mismatched array configurations\n",
      "highlights the robustness of this training strategy.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Tho Mariotte</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Anthony Larcher</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Silvio Montrsor</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jean-Hugh Thomas</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">14 pages, 5 figures, accepted at IEEE/ACM Transactions on Audio,\n",
      "  Speech and Language Processing (TASLP)</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2402.08312v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.08312v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.08252v1</id>\n",
      "    <updated>2024-02-13T06:47:26Z</updated>\n",
      "    <published>2024-02-13T06:47:26Z</published>\n",
      "    <title>Unrestricted Global Phase Bias-Aware Single-channel Speech Enhancement\n",
      "  with Conformer-based Metric GAN</title>\n",
      "    <summary>  With the rapid development of neural networks in recent years, the ability of\n",
      "various networks to enhance the magnitude spectrum of noisy speech in the\n",
      "single-channel speech enhancement domain has become exceptionally outstanding.\n",
      "However, enhancing the phase spectrum using neural networks is often\n",
      "ineffective, which remains a challenging problem. In this paper, we found that\n",
      "the human ear cannot sensitively perceive the difference between a precise\n",
      "phase spectrum and a biased phase (BP) spectrum. Therefore, we propose an\n",
      "optimization method of phase reconstruction, allowing freedom on the\n",
      "global-phase bias instead of reconstructing the precise phase spectrum. We\n",
      "applied it to a Conformer-based Metric Generative Adversarial Networks (CMGAN)\n",
      "baseline model, which relaxes the existing constraints of precise phase and\n",
      "gives the neural network a broader learning space. Results show that this\n",
      "method achieves a new state-of-the-art performance without incurring additional\n",
      "computational overhead.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Shiqi Zhang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zheng Qiu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Daiki Takeuchi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Noboru Harada</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Shoji Makino</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Accepted by ICASSP 2024</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2402.08252v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.08252v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.08217v1</id>\n",
      "    <updated>2024-02-13T05:10:04Z</updated>\n",
      "    <published>2024-02-13T05:10:04Z</published>\n",
      "    <title>Springboard, Roadblock or \"Crutch\"?: How Transgender Users Leverage\n",
      "  Voice Changers for Gender Presentation in Social Virtual Reality</title>\n",
      "    <summary>  Social virtual reality (VR) serves as a vital platform for transgender\n",
      "individuals to explore their identities through avatars and foster personal\n",
      "connections within online communities. However, it presents a challenge: the\n",
      "disconnect between avatar embodiment and voice representation, often leading to\n",
      "misgendering and harassment. Prior research acknowledges this issue but\n",
      "overlooks the potential solution of voice changers. We interviewed 13\n",
      "transgender and gender-nonconforming users of social VR platforms, focusing on\n",
      "their experiences with and without voice changers. We found that using a voice\n",
      "changer not only reduces voice-related harassment, but also allows them to\n",
      "experience gender euphoria through both hearing their modified voice and the\n",
      "reactions of others to their modified voice, motivating them to pursue voice\n",
      "training and medication to achieve desired voices. Furthermore, we identified\n",
      "the technical barriers to current voice changer technology and potential\n",
      "improvements to alleviate the problems that transgender and\n",
      "gender-nonconforming users face.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Kassie Povinelli</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuhang Zhao</name>\n",
      "    </author>\n",
      "    <arxiv:journal_ref xmlns:arxiv=\"http://arxiv.org/schemas/atom\">IEEE VR 2024</arxiv:journal_ref>\n",
      "    <link href=\"http://arxiv.org/abs/2402.08217v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.08217v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.07729v1</id>\n",
      "    <updated>2024-02-12T15:41:22Z</updated>\n",
      "    <published>2024-02-12T15:41:22Z</published>\n",
      "    <title>AIR-Bench: Benchmarking Large Audio-Language Models via Generative\n",
      "  Comprehension</title>\n",
      "    <summary>  Recently, instruction-following audio-language models have received broad\n",
      "attention for human-audio interaction. However, the absence of benchmarks\n",
      "capable of evaluating audio-centric interaction capabilities has impeded\n",
      "advancements in this field. Previous models primarily focus on assessing\n",
      "different fundamental tasks, such as Automatic Speech Recognition (ASR), and\n",
      "lack an assessment of the open-ended generative capabilities centered around\n",
      "audio. Thus, it is challenging to track the progression in the Large\n",
      "Audio-Language Models (LALMs) domain and to provide guidance for future\n",
      "improvement. In this paper, we introduce AIR-Bench (\\textbf{A}udio\n",
      "\\textbf{I}nst\\textbf{R}uction \\textbf{Bench}mark), the first benchmark designed\n",
      "to evaluate the ability of LALMs to understand various types of audio signals\n",
      "(including human speech, natural sounds, and music), and furthermore, to\n",
      "interact with humans in the textual format. AIR-Bench encompasses two\n",
      "dimensions: \\textit{foundation} and \\textit{chat} benchmarks. The former\n",
      "consists of 19 tasks with approximately 19k single-choice questions, intending\n",
      "to inspect the basic single-task ability of LALMs. The latter one contains 2k\n",
      "instances of open-ended question-and-answer data, directly assessing the\n",
      "comprehension of the model on complex audio and its capacity to follow\n",
      "instructions. Both benchmarks require the model to generate hypotheses\n",
      "directly. We design a unified framework that leverages advanced language\n",
      "models, such as GPT-4, to evaluate the scores of generated hypotheses given the\n",
      "meta-information of the audio. Experimental results demonstrate a high level of\n",
      "consistency between GPT-4-based evaluation and human evaluation. By revealing\n",
      "the limitations of existing LALMs through evaluation results, AIR-Bench can\n",
      "provide insights into the direction of future research.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Qian Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jin Xu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wenrui Liu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yunfei Chu</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Ziyue Jiang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xiaohuan Zhou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yichong Leng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuanjun Lv</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Zhou Zhao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Chang Zhou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jingren Zhou</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2402.07729v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.07729v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.07658v1</id>\n",
      "    <updated>2024-02-12T14:01:12Z</updated>\n",
      "    <published>2024-02-12T14:01:12Z</published>\n",
      "    <title>The Sound of Healthcare: Improving Medical Transcription ASR Accuracy\n",
      "  with Large Language Models</title>\n",
      "    <summary>  In the rapidly evolving landscape of medical documentation, transcribing\n",
      "clinical dialogues accurately is increasingly paramount. This study explores\n",
      "the potential of Large Language Models (LLMs) to enhance the accuracy of\n",
      "Automatic Speech Recognition (ASR) systems in medical transcription. Utilizing\n",
      "the PriMock57 dataset, which encompasses a diverse range of primary care\n",
      "consultations, we apply advanced LLMs to refine ASR-generated transcripts. Our\n",
      "research is multifaceted, focusing on improvements in general Word Error Rate\n",
      "(WER), Medical Concept WER (MC-WER) for the accurate transcription of essential\n",
      "medical terms, and speaker diarization accuracy. Additionally, we assess the\n",
      "role of LLM post-processing in improving semantic textual similarity, thereby\n",
      "preserving the contextual integrity of clinical dialogues. Through a series of\n",
      "experiments, we compare the efficacy of zero-shot and Chain-of-Thought (CoT)\n",
      "prompting techniques in enhancing diarization and correction accuracy. Our\n",
      "findings demonstrate that LLMs, particularly through CoT prompting, not only\n",
      "improve the diarization accuracy of existing ASR systems but also achieve\n",
      "state-of-the-art performance in this domain. This improvement extends to more\n",
      "accurately capturing medical concepts and enhancing the overall semantic\n",
      "coherence of the transcribed dialogues. These findings illustrate the dual role\n",
      "of LLMs in augmenting ASR outputs and independently excelling in transcription\n",
      "tasks, holding significant promise for transforming medical ASR systems and\n",
      "leading to more accurate and reliable patient records in healthcare settings.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Ayo Adedeji</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sarita Joshi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Brendan Doohan</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">31 pages, 17 figures</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2402.07658v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.07658v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.07619v1</id>\n",
      "    <updated>2024-02-12T12:52:47Z</updated>\n",
      "    <published>2024-02-12T12:52:47Z</published>\n",
      "    <title>Developing a Multi-variate Prediction Model For COVID-19 From\n",
      "  Crowd-sourced Respiratory Voice Data</title>\n",
      "    <summary>  COVID-19 has affected more than 223 countries worldwide and in the Post-COVID\n",
      "Era, there is a pressing need for non-invasive, low-cost, and highly scalable\n",
      "solutions to detect COVID-19. We develop a deep learning model to identify\n",
      "COVID-19 from voice recording data. The novelty of this work is in the\n",
      "development of deep learning models for COVID-19 identification from only voice\n",
      "recordings. We use the Cambridge COVID-19 Sound database which contains 893\n",
      "speech samples, crowd-sourced from 4352 participants via a COVID-19 Sounds app.\n",
      "Voice features including Mel-spectrograms and Mel-frequency cepstral\n",
      "coefficients (MFCC) and CNN Encoder features are extracted. Based on the voice\n",
      "data, we develop deep learning classification models to detect COVID-19 cases.\n",
      "These models include Long Short-Term Memory (LSTM) and Convolutional Neural\n",
      "Network (CNN) and Hidden-Unit BERT (HuBERT). We compare their predictive power\n",
      "to baseline machine learning models. HuBERT achieves the highest accuracy of\n",
      "86\\% and the highest AUC of 0.93. The results achieved with the proposed models\n",
      "suggest promising results in COVID-19 diagnosis from voice recordings when\n",
      "compared to the results obtained from the state-of-the-art.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Yuyang Yan</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Wafaa Aljbawi</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sami O. Simons</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Visara Urovi</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">arXiv admin note: text overlap with arXiv:2209.03727</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2402.07619v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.07619v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.07599v1</id>\n",
      "    <updated>2024-02-12T11:58:41Z</updated>\n",
      "    <published>2024-02-12T11:58:41Z</published>\n",
      "    <title>Interactive singing melody extraction based on active adaptation</title>\n",
      "    <summary>  Extraction of predominant pitch from polyphonic audio is one of the\n",
      "fundamental tasks in the field of music information retrieval and computational\n",
      "musicology. To accomplish this task using machine learning, a large amount of\n",
      "labeled audio data is required to train the model. However, a classical model\n",
      "pre-trained on data from one domain (source), e.g., songs of a particular\n",
      "singer or genre, may not perform comparatively well in extracting melody from\n",
      "other domains (target). The performance of such models can be boosted by\n",
      "adapting the model using very little annotated data from the target domain. In\n",
      "this work, we propose an efficient interactive melody adaptation method. Our\n",
      "method selects the regions in the target audio that require human annotation\n",
      "using a confidence criterion based on normalized true class probability. The\n",
      "annotations are used by the model to adapt itself to the target domain using\n",
      "meta-learning. Our method also provides a novel meta-learning approach that\n",
      "handles class imbalance, i.e., a few representative samples from a few classes\n",
      "are available for adaptation in the target domain. Experimental results show\n",
      "that the proposed method outperforms other adaptive melody extraction\n",
      "baselines. The proposed method is model-agnostic and hence can be applied to\n",
      "other non-adaptive melody extraction models to boost their performance. Also,\n",
      "we released a Hindustani Alankaar and Raga (HAR) dataset containing 523 audio\n",
      "files of about 6.86 hours of duration intended for singing melody extraction\n",
      "tasks.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Kavya Ranjan Saxena</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Vipul Arora</name>\n",
      "    </author>\n",
      "    <link href=\"http://arxiv.org/abs/2402.07599v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.07599v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2402.07596v1</id>\n",
      "    <updated>2024-02-12T11:52:21Z</updated>\n",
      "    <published>2024-02-12T11:52:21Z</published>\n",
      "    <title>Sheet Music Transformer: End-To-End Optical Music Recognition Beyond\n",
      "  Monophonic Transcription</title>\n",
      "    <summary>  State-of-the-art end-to-end Optical Music Recognition (OMR) has, to date,\n",
      "primarily been carried out using monophonic transcription techniques to handle\n",
      "complex score layouts, such as polyphony, often by resorting to simplifications\n",
      "or specific adaptations. Despite their efficacy, these approaches imply\n",
      "challenges related to scalability and limitations. This paper presents the\n",
      "Sheet Music Transformer, the first end-to-end OMR model designed to transcribe\n",
      "complex musical scores without relying solely on monophonic strategies. Our\n",
      "model employs a Transformer-based image-to-sequence framework that predicts\n",
      "score transcriptions in a standard digital music encoding format from input\n",
      "images. Our model has been tested on two polyphonic music datasets and has\n",
      "proven capable of handling these intricate music structures effectively. The\n",
      "experimental outcomes not only indicate the competence of the model, but also\n",
      "show that it is better than the state-of-the-art methods, thus contributing to\n",
      "advancements in end-to-end OMR transcription.\n",
      "</summary>\n",
      "    <author>\n",
      "      <name>Antonio Ros-Vila</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jorge Calvo-Zaragoza</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Thierry Paquet</name>\n",
      "    </author>\n",
      "    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">Submitted to the International Conference on Document Analysis and\n",
      "  Recognition 2024</arxiv:comment>\n",
      "    <link href=\"http://arxiv.org/abs/2402.07596v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2402.07596v1\" rel=\"related\" type=\"application/pdf\"/>\n",
      "    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "  </entry>\n",
      "</feed>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get data from arxiv api \n",
    "# https://gist.github.com/jozefg/c2542f51a0b9b3f6efe528fcec90e334\n",
    "\n",
    "import requests\n",
    "url = 'http://export.arxiv.org/api/query?search_query=cat:cs.SD&sortBy=submittedDate&sortOrder=descending&max_results=50'\n",
    "response = requests.get(url)\n",
    "xml_data = response.text  # data.read().decode('utf-8')\n",
    "print(xml_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"feed\": {\"@xmlns\": \"http://www.w3.org/2005/Atom\", \"link\": {\"@href\": \"http://arxiv.org/api/query?search_query%3Dcat%3Acs.SD%26id_list%3D%26start%3D0%26max_results%3D50\", \"@rel\": \"self\", \"@type\": \"application/atom+xml\"}, \"title\": {\"@type\": \"html\", \"#text\": \"ArXiv Query: search_query=cat:cs.SD&id_list=&start=0&max_results=50\"}, \"id\": \"http://arxiv.org/api/Uk4hOarnIHkcx3INbQB+qNBCFEo\", \"updated\": \"2024-02-21T00:00:00-05:00\", \"opensearch:totalResults\": {\"@xmlns:opensearch\": \"http://a9.com/-/spec/opensearch/1.1/\", \"#text\": \"13172\"}, \"opensearch:startIndex\": {\"@xmlns:opensearch\": \"http://a9.com/-/spec/opensearch/1.1/\", \"#text\": \"0\"}, \"opensearch:itemsPerPage\": {\"@xmlns:opensearch\": \"http://a9.com/-/spec/opensearch/1.1/\", \"#text\": \"50\"}, \"entry\": [{\"id\": \"http://arxiv.org/abs/2402.13236v1\", \"updated\": \"2024-02-20T18:50:25Z\", \"published\": \"2024-02-20T18:50:25Z\", \"title\": \"Towards audio language modeling - an overview\", \"summary\": \"Neural audio codecs are initially introduced to compress audio data into\\ncompact codes to reduce transmission latency. Researchers recently discovered\\nthe potential of codecs as suitable tokenizers for converting continuous audio\\ninto discrete codes, which can be employed to develop audio language models\\n(LMs). Numerous high-performance neural audio codecs and codec-based LMs have\\nbeen developed. The paper aims to provide a thorough and systematic overview of\\nthe neural audio codec models and codec-based LMs.\", \"author\": [{\"name\": \"Haibin Wu\"}, {\"name\": \"Xuanjun Chen\"}, {\"name\": \"Yi-Cheng Lin\"}, {\"name\": \"Kai-wei Chang\"}, {\"name\": \"Ho-Lam Chung\"}, {\"name\": \"Alexander H. Liu\"}, {\"name\": \"Hung-yi Lee\"}], \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.13236v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.13236v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.13110v1\", \"updated\": \"2024-02-20T16:03:02Z\", \"published\": \"2024-02-20T16:03:02Z\", \"title\": \"HiRIS: an Airborne Sonar Sensor with a 1024 Channel Microphone Array for\\n  In-Air Acoustic Imaging\", \"summary\": \"Airborne 3D imaging using ultrasound is a promising sensing modality for\\nrobotic applications in harsh environments. Over the last decade, several\\nhigh-performance systems have been proposed in the literature. Most of these\\nsensors use a reduced aperture microphone array, leading to artifacts in the\\nresulting acoustic images. This paper presents a novel in-air ultrasound sensor\\nthat incorporates 1024 microphones, in a 32-by- 32 uniform rectangular array,\\nin combination with a distributed embedded hardware design to perform the data\\nacquisition. Using a broadband Minimum Variance Distortionless Response (MVDR)\\nbeamformer with Forward-Backward Spatial Smoothing (FB-SS), the sensor is able\\nto create both 2D and 3D ultrasound images of the full-frontal hemisphere with\\nhigh angular accuracy with up to 70dB main lobe to side lobe ratio. This paper\\ndescribes both the hardware infrastructure needed to obtain such highly\\ndetailed acoustical images, as well as the signal processing chain needed to\\nconvert the raw acoustic data into said images. Utilizing this novel\\nhigh-resolution ultrasound imaging sensor, we wish to investigate the limits of\\nboth passive and active airborne ultrasound sensing by utilizing this virtually\\nartifact-free imaging modality.\", \"author\": [{\"name\": \"Dennis Laurijssen\"}, {\"name\": \"Walter Daems\"}, {\"name\": \"Jan Steckel\"}], \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.13110v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.13110v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"eess.SP\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"eess.SP\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.13076v1\", \"updated\": \"2024-02-20T15:22:25Z\", \"published\": \"2024-02-20T15:22:25Z\", \"title\": \"Not All Weights Are Created Equal: Enhancing Energy Efficiency in\\n  On-Device Streaming Speech Recognition\", \"summary\": \"Power consumption plays an important role in on-device streaming speech\\nrecognition, as it has a direct impact on the user experience. This study\\ndelves into how weight parameters in speech recognition models influence the\\noverall power consumption of these models. We discovered that the impact of\\nweight parameters on power consumption varies, influenced by factors including\\nhow often they are invoked and their placement in memory. Armed with this\\ninsight, we developed design guidelines aimed at optimizing on-device speech\\nrecognition models. These guidelines focus on minimizing power use without\\nsubstantially affecting accuracy. Our method, which employs targeted\\ncompression based on the varying sensitivities of weight parameters,\\ndemonstrates superior performance compared to state-of-the-art compression\\nmethods. It achieves a reduction in energy usage of up to 47% while maintaining\\nsimilar model accuracy and improving the real-time factor.\", \"author\": [{\"name\": \"Yang Li\"}, {\"name\": \"Yuan Shangguan\"}, {\"name\": \"Yuhao Wang\"}, {\"name\": \"Liangzhen Lai\"}, {\"name\": \"Ernie Chang\"}, {\"name\": \"Changsheng Zhao\"}, {\"name\": \"Yangyang Shi\"}, {\"name\": \"Vikas Chandra\"}], \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.13076v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.13076v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.LG\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.13071v1\", \"updated\": \"2024-02-20T15:13:38Z\", \"published\": \"2024-02-20T15:13:38Z\", \"title\": \"Codec-SUPERB: An In-Depth Analysis of Sound Codec Models\", \"summary\": \"The sound codec's dual roles in minimizing data transmission latency and\\nserving as tokenizers underscore its critical importance. Recent years have\\nwitnessed significant developments in codec models. The ideal sound codec\\nshould preserve content, paralinguistics, speakers, and audio information.\\nHowever, the question of which codec achieves optimal sound information\\npreservation remains unanswered, as in different papers, models are evaluated\\non their selected experimental settings. This study introduces Codec-SUPERB, an\\nacronym for Codec sound processing Universal PERformance Benchmark. It is an\\necosystem designed to assess codec models across representative sound\\napplications and signal-level metrics rooted in sound domain\\nknowledge.Codec-SUPERB simplifies result sharing through an online leaderboard,\\npromoting collaboration within a community-driven benchmark database, thereby\\nstimulating new development cycles for codecs. Furthermore, we undertake an\\nin-depth analysis to offer insights into codec models from both application and\\nsignal perspectives, diverging from previous codec papers mainly concentrating\\non signal-level comparisons. Finally, we will release codes, the leaderboard,\\nand data to accelerate progress within the community.\", \"author\": [{\"name\": \"Haibin Wu\"}, {\"name\": \"Ho-Lam Chung\"}, {\"name\": \"Yi-Cheng Lin\"}, {\"name\": \"Yuan-Kuei Wu\"}, {\"name\": \"Xuanjun Chen\"}, {\"name\": \"Yu-Chi Pai\"}, {\"name\": \"Hsiu-Hsuan Wang\"}, {\"name\": \"Kai-Wei Chang\"}, {\"name\": \"Alexander H. Liu\"}, {\"name\": \"Hung-yi Lee\"}], \"arxiv:comment\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"#text\": \"Github: https://github.com/voidful/Codec-SUPERB\"}, \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.13071v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.13071v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.13018v1\", \"updated\": \"2024-02-20T14:00:53Z\", \"published\": \"2024-02-20T14:00:53Z\", \"title\": \"EMO-SUPERB: An In-depth Look at Speech Emotion Recognition\", \"summary\": \"Speech emotion recognition (SER) is a pivotal technology for human-computer\\ninteraction systems. However, 80.77% of SER papers yield results that cannot be\\nreproduced. We develop EMO-SUPERB, short for EMOtion Speech Universal\\nPERformance Benchmark, which aims to enhance open-source initiatives for SER.\\nEMO-SUPERB includes a user-friendly codebase to leverage 15 state-of-the-art\\nspeech self-supervised learning models (SSLMs) for exhaustive evaluation across\\nsix open-source SER datasets. EMO-SUPERB streamlines result sharing via an\\nonline leaderboard, fostering collaboration within a community-driven benchmark\\nand thereby enhancing the development of SER. On average, 2.58% of annotations\\nare annotated using natural language. SER relies on classification models and\\nis unable to process natural languages, leading to the discarding of these\\nvaluable annotations. We prompt ChatGPT to mimic annotators, comprehend natural\\nlanguage annotations, and subsequently re-label the data. By utilizing labels\\ngenerated by ChatGPT, we consistently achieve an average relative gain of 3.08%\\nacross all settings.\", \"author\": [{\"name\": \"Haibin Wu\"}, {\"name\": \"Huang-Cheng Chou\"}, {\"name\": \"Kai-Wei Chang\"}, {\"name\": \"Lucas Goncalves\"}, {\"name\": \"Jiawei Du\"}, {\"name\": \"Jyh-Shing Roger Jang\"}, {\"name\": \"Chi-Chun Lee\"}, {\"name\": \"Hung-Yi Lee\"}], \"arxiv:comment\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"#text\": \"webpage: https://emosuperb.github.io/\"}, \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.13018v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.13018v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.12746v1\", \"updated\": \"2024-02-20T06:24:38Z\", \"published\": \"2024-02-20T06:24:38Z\", \"title\": \"Plugin Speech Enhancement: A Universal Speech Enhancement Framework\\n  Inspired by Dynamic Neural Network\", \"summary\": \"The expectation to deploy a universal neural network for speech enhancement,\\nwith the aim of improving noise robustness across diverse speech processing\\ntasks, faces challenges due to the existing lack of awareness within static\\nspeech enhancement frameworks regarding the expected speech in downstream\\nmodules. These limitations impede the effectiveness of static speech\\nenhancement approaches in achieving optimal performance for a range of speech\\nprocessing tasks, thereby challenging the notion of universal applicability.\\nThe fundamental issue in achieving universal speech enhancement lies in\\neffectively informing the speech enhancement module about the features of\\ndownstream modules. In this study, we present a novel weighting prediction\\napproach, which explicitly learns the task relationships from downstream\\ntraining information to address the core challenge of universal speech\\nenhancement. We found the role of deciding whether to employ data augmentation\\ntechniques as crucial downstream training information. This decision\\nsignificantly impacts the expected speech and the performance of the speech\\nenhancement module. Moreover, we introduce a novel speech enhancement network,\\nthe Plugin Speech Enhancement (Plugin-SE). The Plugin-SE is a dynamic neural\\nnetwork that includes the speech enhancement module, gate module, and weight\\nprediction module. Experimental results demonstrate that the proposed Plugin-SE\\napproach is competitive or superior to other joint training methods across\\nvarious downstream tasks.\", \"author\": [{\"name\": \"Yanan Chen\"}, {\"name\": \"Zihao Cui\"}, {\"name\": \"Yingying Gao\"}, {\"name\": \"Junlan Feng\"}, {\"name\": \"Chao Deng\"}, {\"name\": \"Shilei Zhang\"}], \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.12746v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.12746v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.12660v1\", \"updated\": \"2024-02-20T02:16:24Z\", \"published\": \"2024-02-20T02:16:24Z\", \"title\": \"SingVisio: Visual Analytics of Diffusion Model for Singing Voice\\n  Conversion\", \"summary\": \"In this study, we present SingVisio, an interactive visual analysis system\\nthat aims to explain the diffusion model used in singing voice conversion.\\nSingVisio provides a visual display of the generation process in diffusion\\nmodels, showcasing the step-by-step denoising of the noisy spectrum and its\\ntransformation into a clean spectrum that captures the desired singer's timbre.\\nThe system also facilitates side-by-side comparisons of different conditions,\\nsuch as source content, melody, and target timbre, highlighting the impact of\\nthese conditions on the diffusion generation process and resulting conversions.\\nThrough comprehensive evaluations, SingVisio demonstrates its effectiveness in\\nterms of system design, functionality, explainability, and user-friendliness.\\nIt offers users of various backgrounds valuable learning experiences and\\ninsights into the diffusion model for singing voice conversion.\", \"author\": [{\"name\": \"Liumeng Xue\"}, {\"name\": \"Chaoren Wang\"}, {\"name\": \"Mingxuan Wang\"}, {\"name\": \"Xueyao Zhang\"}, {\"name\": \"Jun Han\"}, {\"name\": \"Zhizheng Wu\"}], \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.12660v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.12660v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.HC\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.12658v1\", \"updated\": \"2024-02-20T02:14:45Z\", \"published\": \"2024-02-20T02:14:45Z\", \"title\": \"Guiding the underwater acoustic target recognition with interpretable\\n  contrastive learning\", \"summary\": \"Recognizing underwater targets from acoustic signals is a challenging task\\nowing to the intricate ocean environments and variable underwater channels.\\nWhile deep learning-based systems have become the mainstream approach for\\nunderwater acoustic target recognition, they have faced criticism for their\\nlack of interpretability and weak generalization performance in practical\\napplications. In this work, we apply the class activation mapping (CAM) to\\ngenerate visual explanations for the predictions of a spectrogram-based\\nrecognition system. CAM can help to understand the behavior of recognition\\nmodels by highlighting the regions of the input features that contribute the\\nmost to the prediction. Our explorations reveal that recognition models tend to\\nfocus on the low-frequency line spectrum and high-frequency periodic modulation\\ninformation of underwater signals. Based on the observation, we propose an\\ninterpretable contrastive learning (ICL) strategy that employs two encoders to\\nlearn from acoustic features with different emphases (line spectrum and\\nmodulation information). By imposing constraints between encoders, the proposed\\nstrategy can enhance the generalization performance of the recognition system.\\nOur experiments demonstrate that the proposed contrastive learning approach can\\nimprove the recognition accuracy and bring significant improvements across\\nvarious underwater databases.\", \"author\": [{\"name\": \"Yuan Xie\"}, {\"name\": \"Jiawei Ren\"}, {\"name\": \"Ji Xu\"}], \"arxiv:doi\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"#text\": \"10.1109/OCEANSLimerick52467.2023.10244447\"}, \"link\": [{\"@title\": \"doi\", \"@href\": \"http://dx.doi.org/10.1109/OCEANSLimerick52467.2023.10244447\", \"@rel\": \"related\"}, {\"@href\": \"http://arxiv.org/abs/2402.12658v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.12658v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:journal_ref\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"#text\": \"OCEANS 2023-Limerick. IEEE, 2023: 1-6\"}, \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.12654v1\", \"updated\": \"2024-02-20T02:04:38Z\", \"published\": \"2024-02-20T02:04:38Z\", \"title\": \"OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech\\n  Recognition, Translation, and Language Identification\", \"summary\": \"There has been an increasing interest in large speech models that can perform\\nmultiple speech processing tasks in a single model. Such models usually adopt\\nthe encoder-decoder or decoder-only architecture due to their popularity and\\ngood performance in many domains. However, autoregressive models can be slower\\nduring inference compared to non-autoregressive models and also have potential\\nrisks of hallucination. Though prior studies observed promising results of\\nnon-autoregressive models for certain tasks at small scales, it remains unclear\\nif they can be scaled to speech-to-text generation in diverse languages and\\ntasks. Inspired by the Open Whisper-style Speech Model (OWSM) project, we\\npropose OWSM-CTC, a novel encoder-only speech foundation model based on\\nConnectionist Temporal Classification (CTC). It is trained on 180k hours of\\npublic audio data for multilingual automatic speech recognition (ASR), speech\\ntranslation (ST), and language identification (LID). Compared to\\nencoder-decoder OWSM, our OWSM-CTC achieves competitive results on ASR and up\\nto 25% relative improvement on ST, while it is more robust and 3 to 4 times\\nfaster for inference. OWSM-CTC also improves the long-form ASR result with 20x\\nspeed-up. We will publicly release our codebase, pre-trained model, and\\ntraining logs to promote open science in speech foundation models.\", \"author\": [{\"name\": \"Yifan Peng\"}, {\"name\": \"Yui Sudo\"}, {\"name\": \"Muhammad Shakeel\"}, {\"name\": \"Shinji Watanabe\"}], \"arxiv:comment\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"#text\": \"18 pages, 2 figures\"}, \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.12654v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.12654v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"cs.CL\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"cs.CL\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.12482v1\", \"updated\": \"2024-02-19T19:38:37Z\", \"published\": \"2024-02-19T19:38:37Z\", \"title\": \"SECP: A Speech Enhancement-Based Curation Pipeline For Scalable\\n  Acquisition Of Clean Speech\", \"summary\": \"As more speech technologies rely on a supervised deep learning approach with\\nclean speech as the ground truth, a methodology to onboard said speech at scale\\nis needed. However, this approach needs to minimize the dependency on human\\nlistening and annotation, only requiring a human-in-the-loop when needed. In\\nthis paper, we address this issue by outlining Speech Enhancement-based\\nCuration Pipeline (SECP) which serves as a framework to onboard clean speech.\\nThis clean speech can then train a speech enhancement model, which can further\\nrefine the original dataset and thus close the iterative loop. By running two\\niterative rounds, we observe that enhanced output used as ground truth does not\\ndegrade model performance according to $\\\\Delta_{PESQ}$, a metric used in this\\npaper. We also show through comparative mean opinion score (CMOS) based\\nsubjective tests that the highest and lowest bound of refined data is\\nperceptually better than the original data.\", \"author\": [{\"name\": \"Adam Sabra\"}, {\"name\": \"Cyprian Wronka\"}, {\"name\": \"Michelle Mao\"}, {\"name\": \"Samer Hijazi\"}], \"arxiv:comment\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"#text\": \"Accepted to the International Conference on Acoustics, Speech and\\n  Signal Processing (ICASSP) 2024\"}, \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.12482v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.12482v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.IR\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.LG\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.12423v1\", \"updated\": \"2024-02-19T16:22:21Z\", \"published\": \"2024-02-19T16:22:21Z\", \"title\": \"On the Semantic Latent Space of Diffusion-Based Text-to-Speech Models\", \"summary\": \"The incorporation of Denoising Diffusion Models (DDMs) in the Text-to-Speech\\n(TTS) domain is rising, providing great value in synthesizing high quality\\nspeech. Although they exhibit impressive audio quality, the extent of their\\nsemantic capabilities is unknown, and controlling their synthesized speech's\\nvocal properties remains a challenge. Inspired by recent advances in image\\nsynthesis, we explore the latent space of frozen TTS models, which is composed\\nof the latent bottleneck activations of the DDM's denoiser. We identify that\\nthis space contains rich semantic information, and outline several novel\\nmethods for finding semantic directions within it, both supervised and\\nunsupervised. We then demonstrate how these enable off-the-shelf audio editing,\\nwithout any further training, architectural changes or data requirements. We\\npresent evidence of the semantic and acoustic qualities of the edited audio,\\nand provide supplemental samples:\\nhttps://latent-analysis-grad-tts.github.io/speech-samples/.\", \"author\": [{\"name\": \"Miri Varshavsky Hassid\"}, {\"name\": \"Roy Hirsch\"}, {\"name\": \"Regev Cohen\"}, {\"name\": \"Tomer Golany\"}, {\"name\": \"Daniel Freedman\"}, {\"name\": \"Ehud Rivlin\"}], \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.12423v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.12423v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.CL\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.LG\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.12239v1\", \"updated\": \"2024-02-19T15:50:32Z\", \"published\": \"2024-02-19T15:50:32Z\", \"title\": \"Significance of Chirp MFCC as a Feature in Speech and Audio Applications\", \"summary\": \"A novel feature, based on the chirp z-transform, that offers an improved\\nrepresentation of the underlying true spectrum is proposed. This feature, the\\nchirp MFCC, is derived by computing the Mel frequency cepstral coefficients\\nfrom the chirp magnitude spectrum, instead of the Fourier transform magnitude\\nspectrum. The theoretical foundations for the proposal, and the experimental\\nvalidation using product of likelihood Gaussians, to show the improved class\\nseparation offered by the proposed chirp MFCC, when compared with vanilla MFCC\\nare discussed. Further, real world evaluation of the feature is performed using\\nthree diverse tasks, namely, speech-music classification, speaker\\nidentification, and speech commands recognition. It is shown in all three tasks\\nthat the proposed chirp MFCC offers considerable improvements.\", \"author\": [{\"name\": \"S. Johanan Joysingh\"}, {\"name\": \"P. Vijayalakshmi\"}, {\"name\": \"T. Nagarajan\"}], \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.12239v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.12239v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"eess.SP\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"eess.SP\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.12208v2\", \"updated\": \"2024-02-20T15:24:30Z\", \"published\": \"2024-02-19T15:12:12Z\", \"title\": \"Language-Codec: Reducing the Gaps Between Discrete Codec Representation\\n  and Speech Language Models\", \"summary\": \"In recent years, large language models have achieved significant success in\\ngenerative tasks (e.g., speech cloning and audio generation) related to speech,\\naudio, music, and other signal domains. A crucial element of these models is\\nthe discrete acoustic codecs, which serves as an intermediate representation\\nreplacing the mel-spectrogram. However, there exist several gaps between\\ndiscrete codecs and downstream speech language models. Specifically, 1) most\\ncodec models are trained on only 1,000 hours of data, whereas most speech\\nlanguage models are trained on 60,000 hours; 2) Achieving good reconstruction\\nperformance requires the utilization of numerous codebooks, which increases the\\nburden on downstream speech language models; 3) The initial channel of the\\ncodebooks contains excessive information, making it challenging to directly\\ngenerate acoustic tokens from weakly supervised signals such as text in\\ndownstream tasks. Consequently, leveraging the characteristics of speech\\nlanguage models, we propose Language-Codec. In the Language-Codec, we introduce\\na Mask Channel Residual Vector Quantization (MCRVQ) mechanism along with\\nimproved Fourier transform structures and larger training datasets to address\\nthe aforementioned gaps. We compare our method with competing audio compression\\nalgorithms and observe significant outperformance across extensive evaluations.\\nFurthermore, we also validate the efficiency of the Language-Codec on\\ndownstream speech language models. The source code and pre-trained models can\\nbe accessed at https://github.com/jishengpeng/languagecodec .\", \"author\": [{\"name\": \"Shengpeng Ji\"}, {\"name\": \"Minghui Fang\"}, {\"name\": \"Ziyue Jiang\"}, {\"name\": \"Rongjie Huang\"}, {\"name\": \"Jialung Zuo\"}, {\"name\": \"Shulei Wang\"}, {\"name\": \"Zhou Zhao\"}], \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.12208v2\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.12208v2\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.12094v1\", \"updated\": \"2024-02-19T12:20:56Z\", \"published\": \"2024-02-19T12:20:56Z\", \"title\": \"On the relationship between speech and hearing\", \"summary\": \"We present a framework for experimentally linking speech production and\\nhearing. Using this approach, we describe experimental results, that lead to\\nthe concept that sounds made by different individuals and perceived to be the\\nsame can be transformed into each other by a \\\"speech scale\\\". The speech scale\\nis empirically determined using only speech data. We show the similarity of the\\nspeech scale to the MEL scale of Stevens and Volkmann, which was derived only\\nfrom hearing experiments. We thus experimentally link speech production and\\nhearing.\", \"author\": [{\"name\": \"Srinivasan Umesh\"}, {\"name\": \"Leon Cohen\"}, {\"name\": \"Douglas Nelson\"}], \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.12094v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.12094v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.11954v1\", \"updated\": \"2024-02-19T08:49:09Z\", \"published\": \"2024-02-19T08:49:09Z\", \"title\": \"Multimodal Emotion Recognition from Raw Audio with Sinc-convolution\", \"summary\": \"Speech Emotion Recognition (SER) is still a complex task for computers with\\naverage recall rates usually about 70% on the most realistic datasets. Most SER\\nsystems use hand-crafted features extracted from audio signal such as energy,\\nzero crossing rate, spectral information, prosodic, mel frequency cepstral\\ncoefficient (MFCC), and so on. More recently, using raw waveform for training\\nneural network is becoming an emerging trend. This approach is advantageous as\\nit eliminates the feature extraction pipeline. Learning from time-domain signal\\nhas shown good results for tasks such as speech recognition, speaker\\nverification etc. In this paper, we utilize Sinc-convolution layer, which is an\\nefficient architecture for preprocessing raw speech waveform for emotion\\nrecognition, to extract acoustic features from raw audio signals followed by a\\nlong short-term memory (LSTM). We also incorporate linguistic features and\\nappend a dialogical emotion decoding (DED) strategy. Our approach achieves a\\nweighted accuracy of 85.1\\\\% in four class emotion on the Interactive Emotional\\nDyadic Motion Capture (IEMOCAP) dataset.\", \"author\": [{\"name\": \"Xiaohui Zhang\"}, {\"name\": \"Wenjie Fu\"}, {\"name\": \"Mangui Liang\"}], \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.11954v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.11954v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.MM\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.11931v1\", \"updated\": \"2024-02-19T08:18:52Z\", \"published\": \"2024-02-19T08:18:52Z\", \"title\": \"Soft-Weighted CrossEntropy Loss for Continous Alzheimer's Disease\\n  Detection\", \"summary\": \"Alzheimer's disease is a common cognitive disorder in the elderly. Early and\\naccurate diagnosis of Alzheimer's disease (AD) has a major impact on the\\nprogress of research on dementia. At present, researchers have used machine\\nlearning methods to detect Alzheimer's disease from the speech of participants.\\nHowever, the recognition accuracy of current methods is unsatisfactory, and\\nmost of them focus on using low-dimensional handcrafted features to extract\\nrelevant information from audios. This paper proposes an Alzheimer's disease\\ndetection system based on the pre-trained framework Wav2vec 2.0 (Wav2vec2). In\\naddition, by replacing the loss function with the Soft-Weighted CrossEntropy\\nloss function, we achieved 85.45\\\\% recognition accuracy on the same test\\ndataset.\", \"author\": [{\"name\": \"Xiaohui Zhang\"}, {\"name\": \"Wenjie Fu\"}, {\"name\": \"Mangui Liang\"}], \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.11931v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.11931v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"q-bio.NC\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.11919v1\", \"updated\": \"2024-02-19T08:07:01Z\", \"published\": \"2024-02-19T08:07:01Z\", \"title\": \"Unraveling Complex Data Diversity in Underwater Acoustic Target\\n  Recognition through Convolution-based Mixture of Experts\", \"summary\": \"Underwater acoustic target recognition is a difficult task owing to the\\nintricate nature of underwater acoustic signals. The complex underwater\\nenvironments, unpredictable transmission channels, and dynamic motion states\\ngreatly impact the real-world underwater acoustic signals, and may even obscure\\nthe intrinsic characteristics related to targets. Consequently, the data\\ndistribution of underwater acoustic signals exhibits high intra-class\\ndiversity, thereby compromising the accuracy and robustness of recognition\\nsystems.To address these issues, this work proposes a convolution-based mixture\\nof experts (CMoE) that recognizes underwater targets in a fine-grained manner.\\nThe proposed technique introduces multiple expert layers as independent\\nlearners, along with a routing layer that determines the assignment of experts\\naccording to the characteristics of inputs. This design allows the model to\\nutilize independent parameter spaces, facilitating the learning of complex\\nunderwater signals with high intra-class diversity. Furthermore, this work\\noptimizes the CMoE structure by balancing regularization and an optional\\nresidual module. To validate the efficacy of our proposed techniques, we\\nconducted detailed experiments and visualization analyses on three underwater\\nacoustic databases across several acoustic features. The experimental results\\ndemonstrate that our CMoE consistently achieves significant performance\\nimprovements, delivering superior recognition accuracy when compared to\\nexisting advanced methods.\", \"author\": [{\"name\": \"Yuan Xie\"}, {\"name\": \"Jiawei Ren\"}, {\"name\": \"Ji Xu\"}], \"arxiv:doi\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"#text\": \"10.1016/j.eswa.2024.123431\"}, \"link\": [{\"@title\": \"doi\", \"@href\": \"http://dx.doi.org/10.1016/j.eswa.2024.123431\", \"@rel\": \"related\"}, {\"@href\": \"http://arxiv.org/abs/2402.11919v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.11919v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:journal_ref\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"#text\": \"Expert Systems with Applications (2024): 123431\"}, \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.11748v1\", \"updated\": \"2024-02-19T00:21:13Z\", \"published\": \"2024-02-19T00:21:13Z\", \"title\": \"Low-power SNN-based audio source localisation using a Hilbert Transform\\n  spike encoding scheme\", \"summary\": \"Sound source localisation is used in many consumer electronics devices, to\\nhelp isolate audio from individual speakers and to reject noise. Localization\\nis frequently accomplished by \\\"beamforming\\\" algorithms, which combine\\nmicrophone audio streams to improve received signal power from particular\\nincident source directions. Beamforming algorithms generally use knowledge of\\nthe frequency components of the audio source, along with the known microphone\\narray geometry, to analytically phase-shift microphone streams before combining\\nthem. A dense set of band-pass filters is often used to obtain known-frequency\\n\\\"narrowband\\\" components from wide-band audio streams. These approaches achieve\\nhigh accuracy, but state of the art narrowband beamforming algorithms are\\ncomputationally demanding, and are therefore difficult to integrate into\\nlow-power IoT devices. We demonstrate a novel method for sound source\\nlocalisation in arbitrary microphone arrays, designed for efficient\\nimplementation in ultra-low-power spiking neural networks (SNNs). We use a\\nnovel short-time Hilbert transform (STHT) to remove the need for demanding\\nband-pass filtering of audio, and introduce a new accompanying method for audio\\nencoding with spiking events. Our beamforming and localisation approach\\nachieves state-of-the-art accuracy for SNN methods, and comparable with\\ntraditional non-SNN super-resolution approaches. We deploy our method to\\nlow-power SNN audio inference hardware, and achieve much lower power\\nconsumption compared with super-resolution methods. We demonstrate that signal\\nprocessing approaches can be co-designed with spiking neural network\\nimplementations to achieve high levels of power efficiency. Our new\\nHilbert-transform-based method for beamforming promises to also improve the\\nefficiency of traditional DSP-based signal processing.\", \"author\": [{\"name\": \"Saeid Haghighatshoar\"}, {\"name\": \"Dylan R Muir\"}], \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.11748v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.11748v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.NE\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.11747v1\", \"updated\": \"2024-02-19T00:21:07Z\", \"published\": \"2024-02-19T00:21:07Z\", \"title\": \"Parameter Efficient Finetuning for Speech Emotion Recognition and Domain\\n  Adaptation\", \"summary\": \"Foundation models have shown superior performance for speech emotion\\nrecognition (SER). However, given the limited data in emotion corpora,\\nfinetuning all parameters of large pre-trained models for SER can be both\\nresource-intensive and susceptible to overfitting. This paper investigates\\nparameter-efficient finetuning (PEFT) for SER. Various PEFT adaptors are\\nsystematically studied for both classification of discrete emotion categories\\nand prediction of dimensional emotional attributes. The results demonstrate\\nthat the combination of PEFT methods surpasses full finetuning with a\\nsignificant reduction in the number of trainable parameters. Furthermore, a\\ntwo-stage adaptation strategy is proposed to adapt models trained on acted\\nemotion data, which is more readily available, to make the model more adept at\\ncapturing natural emotional expressions. Both intra- and cross-corpus\\nexperiments validate the efficacy of the proposed approach in enhancing the\\nperformance on both the source and target domains.\", \"author\": [{\"name\": \"Nineli Lashkarashvili\"}, {\"name\": \"Wen Wu\"}, {\"name\": \"Guangzhi Sun\"}, {\"name\": \"Philip C. Woodland\"}], \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.11747v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.11747v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.LG\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.11330v1\", \"updated\": \"2024-02-17T16:59:08Z\", \"published\": \"2024-02-17T16:59:08Z\", \"title\": \"Diffuse Sound Field Synthesis\", \"summary\": \"Can uncorrelated surrounding sound sources be used to generate extended\\ndiffuse sound fields? By definition, targets are a constant sound pressure\\nlevel, a vanishing average sound intensity, uncorrelated sound waves arriving\\nisotropically from all directions. Does this require specific sources and\\ngeometries for surrounding 2D and 3D source layouts?\\n  As methods, we employ numeric simulations and undertake a series of\\ncalculations with uncorrelated circular/spherical source layouts, or such with\\ninfinite excess dimensions, and we point out relations to potential theory.\\nUsing a radial decay 1/r^b modified by the exponent b, the representation of\\nthe resulting fields with hypergeometric functions, Gegenbauer polynomials, and\\ncircular as well as spherical harmonics yields fruitful insights.\\n  In circular layouts, waves decaying by the exponent b=1/2 synthesize ideally\\nextended, diffuse sound fields; spherical layouts do so with b=1. None of the\\nlayouts synthesizes a perfectly constant expected sound pressure level but its\\nflatness is acceptable.\\n  Spherical t-designs describe optimal source layouts with well-described area\\nof high diffuseness, and non-spherical, convex layouts can be improved by\\nrestoring isotropy or by mode matching for a maximally diffuse synthesis.\\n  Theory and simulation offer a basis for loudspeaker-based synthesis of\\ndiffuse sound fields and contribute physical reasons to recent psychoacoustic\\nfindings in spatial audio.\", \"author\": [{\"name\": \"Franz Zotter\"}, {\"name\": \"Stefan Riedel\"}, {\"name\": \"Lukas G\\u00f6lles\"}, {\"name\": \"Matthias Frank\"}], \"arxiv:comment\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"#text\": \"27 pages, 17 figures, submitted to acta acustica nov 20th 2023,\\n  including jan/feb 2024 upgrades while awaiting the reviews\"}, \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.11330v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.11330v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.13199v1\", \"updated\": \"2024-02-17T13:45:00Z\", \"published\": \"2024-02-17T13:45:00Z\", \"title\": \"Target Speech Extraction with Pre-trained Self-supervised Learning\\n  Models\", \"summary\": \"Pre-trained self-supervised learning (SSL) models have achieved remarkable\\nsuccess in various speech tasks. However, their potential in target speech\\nextraction (TSE) has not been fully exploited. TSE aims to extract the speech\\nof a target speaker in a mixture guided by enrollment utterances. We exploit\\npre-trained SSL models for two purposes within a TSE framework, i.e., to\\nprocess the input mixture and to derive speaker embeddings from the enrollment.\\nIn this paper, we focus on how to effectively use SSL models for TSE. We first\\nintroduce a novel TSE downstream task following the SUPERB principles. This\\nsimple experiment shows the potential of SSL models for TSE, but extraction\\nperformance remains far behind the state-of-the-art. We then extend a powerful\\nTSE architecture by incorporating two SSL-based modules: an Adaptive Input\\nEnhancer (AIE) and a speaker encoder. Specifically, the proposed AIE utilizes\\nintermediate representations from the CNN encoder by adjusting the time\\nresolution of CNN encoder and transformer blocks through progressive\\nupsampling, capturing both fine-grained and hierarchical features. Our method\\noutperforms current TSE systems achieving a SI-SDR improvement of 14.0 dB on\\nLibriMix. Moreover, we can further improve performance by 0.7 dB by fine-tuning\\nthe whole model including the SSL model parameters.\", \"author\": [{\"name\": \"Junyi Peng\"}, {\"name\": \"Marc Delcroix\"}, {\"name\": \"Tsubasa Ochiai\"}, {\"name\": \"Oldrich Plchot\"}, {\"name\": \"Shoko Araki\"}, {\"name\": \"Jan Cernocky\"}], \"arxiv:comment\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"#text\": \"Accepted to ICASSP 2024\"}, \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.13199v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.13199v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.13200v1\", \"updated\": \"2024-02-17T13:37:22Z\", \"published\": \"2024-02-17T13:37:22Z\", \"title\": \"Probing Self-supervised Learning Models with Target Speech Extraction\", \"summary\": \"Large-scale pre-trained self-supervised learning (SSL) models have shown\\nremarkable advancements in speech-related tasks. However, the utilization of\\nthese models in complex multi-talker scenarios, such as extracting a target\\nspeaker in a mixture, is yet to be fully evaluated. In this paper, we introduce\\ntarget speech extraction (TSE) as a novel downstream task to evaluate the\\nfeature extraction capabilities of pre-trained SSL models. TSE uniquely\\nrequires both speaker identification and speech separation, distinguishing it\\nfrom other tasks in the Speech processing Universal PERformance Benchmark\\n(SUPERB) evaluation. Specifically, we propose a TSE downstream model composed\\nof two lightweight task-oriented modules based on the same frozen SSL model.\\nOne module functions as a speaker encoder to obtain target speaker information\\nfrom an enrollment speech, while the other estimates the target speaker's mask\\nto extract its speech from the mixture. Experimental results on the Libri2mix\\ndatasets reveal the relevance of the TSE downstream task to probe SSL models,\\nas its performance cannot be simply deduced from other related tasks such as\\nspeaker verification and separation.\", \"author\": [{\"name\": \"Junyi Peng\"}, {\"name\": \"Marc Delcroix\"}, {\"name\": \"Tsubasa Ochiai\"}, {\"name\": \"Oldrich Plchot\"}, {\"name\": \"Takanori Ashihara\"}, {\"name\": \"Shoko Araki\"}, {\"name\": \"Jan Cernocky\"}], \"arxiv:comment\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"#text\": \"Accepted to ICASSP 2024, Self-supervision in Audio, Speech, and\\n  Beyond (SASB) workshop\"}, \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.13200v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.13200v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.11216v1\", \"updated\": \"2024-02-17T07:52:21Z\", \"published\": \"2024-02-17T07:52:21Z\", \"title\": \"Feedback Delay Network Optimization\", \"summary\": \"A common bane of artificial reverberation algorithms is spectral coloration,\\ntypically manifesting as metallic ringing, leading to a degradation in the\\nperceived sound quality. This paper presents an optimization framework where a\\ndifferentiable feedback delay network is used to learn a set of parameters to\\nreduce coloration iteratively. The parameters under optimization include the\\nfeedback matrix, as well as the input and output gains. The optimization\\nobjective is twofold: to maximize spectral flatness through a spectral loss\\nwhile maintaining temporal density by penalizing sparseness in the parameter\\nvalues. A favorable narrower distribution of modal excitation is achieved while\\nmaintaining the desired impulse response density. In a subjective assessment,\\nthe new method proves effective in reducing perceptual coloration of late\\nreverberation. The proposed method achieves computational savings compared to\\nthe baseline while preserving its performance. The effectiveness of this work\\nis demonstrated through two application scenarios where natural-sounding\\nsynthetic impulse responses are obtained via the introduction of attenuation\\nfilters and an optimizable scattering feedback matrix.\", \"author\": [{\"name\": \"Gloria Dal Santo\"}, {\"name\": \"Karolina Prawda\"}, {\"name\": \"Sebastian J. Schlecht\"}, {\"name\": \"Vesa V\\u00e4lim\\u00e4ki\"}], \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.11216v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.11216v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.10547v1\", \"updated\": \"2024-02-16T10:20:42Z\", \"published\": \"2024-02-16T10:20:42Z\", \"title\": \"Learning Disentangled Audio Representations through Controlled Synthesis\", \"summary\": \"This paper tackles the scarcity of benchmarking data in disentangled auditory\\nrepresentation learning. We introduce SynTone, a synthetic dataset with\\nexplicit ground truth explanatory factors for evaluating disentanglement\\ntechniques. Benchmarking state-of-the-art methods on SynTone highlights its\\nutility for method evaluation. Our results underscore strengths and limitations\\nin audio disentanglement, motivating future research.\", \"author\": [{\"name\": \"Yusuf Brima\"}, {\"name\": \"Ulf Krumnack\"}, {\"name\": \"Simone Pika\"}, {\"name\": \"Gunther Heidemann\"}], \"arxiv:comment\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"#text\": \"12 pages, 12 figures, accepted as a Tiny paper at ICLR 2024\"}, \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.10547v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.10547v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.LG\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.10533v1\", \"updated\": \"2024-02-16T09:38:16Z\", \"published\": \"2024-02-16T09:38:16Z\", \"title\": \"APCodec: A Neural Audio Codec with Parallel Amplitude and Phase Spectrum\\n  Encoding and Decoding\", \"summary\": \"This paper introduces a novel neural audio codec targeting high waveform\\nsampling rates and low bitrates named APCodec, which seamlessly integrates the\\nstrengths of parametric codecs and waveform codecs. The APCodec revolutionizes\\nthe process of audio encoding and decoding by concurrently handling the\\namplitude and phase spectra as audio parametric characteristics like parametric\\ncodecs. It is composed of an encoder and a decoder with the modified ConvNeXt\\nv2 network as the backbone, connected by a quantizer based on the residual\\nvector quantization (RVQ) mechanism. The encoder compresses the audio amplitude\\nand phase spectra in parallel, amalgamating them into a continuous latent code\\nat a reduced temporal resolution. This code is subsequently quantized by the\\nquantizer. Ultimately, the decoder reconstructs the audio amplitude and phase\\nspectra in parallel, and the decoded waveform is obtained by inverse short-time\\nFourier transform. To ensure the fidelity of decoded audio like waveform\\ncodecs, spectral-level loss, quantization loss, and generative adversarial\\nnetwork (GAN) based loss are collectively employed for training the APCodec. To\\nsupport low-latency streamable inference, we employ feed-forward layers and\\ncausal convolutional layers in APCodec, incorporating a knowledge distillation\\ntraining strategy to enhance the quality of decoded audio. Experimental results\\nconfirm that our proposed APCodec can encode 48 kHz audio at bitrate of just 6\\nkbps, with no significant degradation in the quality of the decoded audio. At\\nthe same bitrate, our proposed APCodec also demonstrates superior decoded audio\\nquality and faster generation speed compared to well-known codecs, such as\\nSoundStream, Encodec, HiFi-Codec and AudioDec.\", \"author\": [{\"name\": \"Yang Ai\"}, {\"name\": \"Xiao-Hang Jiang\"}, {\"name\": \"Ye-Xin Lu\"}, {\"name\": \"Hui-Peng Du\"}, {\"name\": \"Zhen-Hua Ling\"}], \"arxiv:comment\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"#text\": \"Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\\n  Processing\"}, \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.10533v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.10533v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.10427v1\", \"updated\": \"2024-02-16T03:30:27Z\", \"published\": \"2024-02-16T03:30:27Z\", \"title\": \"Evaluating and Improving Continual Learning in Spoken Language\\n  Understanding\", \"summary\": \"Continual learning has emerged as an increasingly important challenge across\\nvarious tasks, including Spoken Language Understanding (SLU). In SLU, its\\nobjective is to effectively handle the emergence of new concepts and evolving\\nenvironments. The evaluation of continual learning algorithms typically\\ninvolves assessing the model's stability, plasticity, and generalizability as\\nfundamental aspects of standards. However, existing continual learning metrics\\nprimarily focus on only one or two of the properties. They neglect the overall\\nperformance across all tasks, and do not adequately disentangle the plasticity\\nversus stability/generalizability trade-offs within the model. In this work, we\\npropose an evaluation methodology that provides a unified evaluation on\\nstability, plasticity, and generalizability in continual learning. By employing\\nthe proposed metric, we demonstrate how introducing various knowledge\\ndistillations can improve different aspects of these three properties of the\\nSLU model. We further show that our proposed metric is more sensitive in\\ncapturing the impact of task ordering in continual learning, making it better\\nsuited for practical use-case scenarios.\", \"author\": [{\"name\": \"Muqiao Yang\"}, {\"name\": \"Xiang Li\"}, {\"name\": \"Umberto Cappellazzo\"}, {\"name\": \"Shinji Watanabe\"}, {\"name\": \"Bhiksha Raj\"}], \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.10427v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.10427v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"cs.CL\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"cs.CL\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.AI\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.10168v1\", \"updated\": \"2024-02-15T18:11:02Z\", \"published\": \"2024-02-15T18:11:02Z\", \"title\": \"DeepSRGM -- Sequence Classification and Ranking in Indian Classical\\n  Music with Deep Learning\", \"summary\": \"A vital aspect of Indian Classical Music (ICM) is Raga, which serves as a\\nmelodic framework for compositions and improvisations alike. Raga Recognition\\nis an important music information retrieval task in ICM as it can aid numerous\\ndownstream applications ranging from music recommendations to organizing huge\\nmusic collections. In this work, we propose a deep learning based approach to\\nRaga recognition. Our approach employs efficient pre possessing and learns\\ntemporal sequences in music data using Long Short Term Memory based Recurrent\\nNeural Networks (LSTM-RNN). We train and test the network on smaller sequences\\nsampled from the original audio while the final inference is performed on the\\naudio as a whole. Our method achieves an accuracy of 88.1% and 97 % during\\ninference on the Comp Music Carnatic dataset and its 10 Raga subset\\nrespectively making it the state-of-the-art for the Raga recognition task. Our\\napproach also enables sequence ranking which aids us in retrieving melodic\\npatterns from a given music data base that are closely related to the presented\\nquery sequence.\", \"author\": [{\"name\": \"Sathwik Tejaswi Madhusudhan\"}, {\"name\": \"Girish Chowdhary\"}], \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.10168v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.10168v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.AI\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.IR\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.LG\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.10009v2\", \"updated\": \"2024-02-16T09:49:10Z\", \"published\": \"2024-02-15T15:17:26Z\", \"title\": \"Zero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion\", \"summary\": \"Editing signals using large pre-trained models, in a zero-shot manner, has\\nrecently seen rapid advancements in the image domain. However, this wave has\\nyet to reach the audio domain. In this paper, we explore two zero-shot editing\\ntechniques for audio signals, which use DDPM inversion on pre-trained diffusion\\nmodels. The first, adopted from the image domain, allows text-based editing.\\nThe second, is a novel approach for discovering semantically meaningful editing\\ndirections without supervision. When applied to music signals, this method\\nexposes a range of musically interesting modifications, from controlling the\\nparticipation of specific instruments to improvisations on the melody. Samples\\nand code can be found on our examples page in\\nhttps://hilamanor.github.io/AudioEditing/ .\", \"author\": [{\"name\": \"Hila Manor\"}, {\"name\": \"Tomer Michaeli\"}], \"arxiv:comment\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"#text\": \"Examples and code available in\\n  https://hilamanor.github.io/AudioEditing/\"}, \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.10009v2\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.10009v2\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.LG\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.09871v1\", \"updated\": \"2024-02-15T10:55:01Z\", \"published\": \"2024-02-15T10:55:01Z\", \"title\": \"MuChin: A Chinese Colloquial Description Benchmark for Evaluating\\n  Language Models in the Field of Music\", \"summary\": \"The rapidly evolving multimodal Large Language Models (LLMs) urgently require\\nnew benchmarks to uniformly evaluate their performance on understanding and\\ntextually describing music. However, due to semantic gaps between Music\\nInformation Retrieval (MIR) algorithms and human understanding, discrepancies\\nbetween professionals and the public, and low precision of annotations,\\nexisting music description datasets cannot serve as benchmarks. To this end, we\\npresent MuChin, the first open-source music description benchmark in Chinese\\ncolloquial language, designed to evaluate the performance of multimodal LLMs in\\nunderstanding and describing music. We established the Caichong Music\\nAnnotation Platform (CaiMAP) that employs an innovative multi-person,\\nmulti-stage assurance method, and recruited both amateurs and professionals to\\nensure the precision of annotations and alignment with popular semantics.\\nUtilizing this method, we built a dataset with multi-dimensional,\\nhigh-precision music annotations, the Caichong Music Dataset (CaiMD), and\\ncarefully selected 1,000 high-quality entries to serve as the test set for\\nMuChin. Based on MuChin, we analyzed the discrepancies between professionals\\nand amateurs in terms of music description, and empirically demonstrated the\\neffectiveness of annotated data for fine-tuning LLMs. Ultimately, we employed\\nMuChin to evaluate existing music understanding models on their ability to\\nprovide colloquial descriptions of music. All data related to the benchmark and\\nthe code for scoring have been open-sourced.\", \"author\": [{\"name\": \"Zihao Wang\"}, {\"name\": \"Shuyu Li\"}, {\"name\": \"Tao Zhang\"}, {\"name\": \"Qi Wang\"}, {\"name\": \"Pengfei Yu\"}, {\"name\": \"Jinyang Luo\"}, {\"name\": \"Yan Liu\"}, {\"name\": \"Ming Xi\"}, {\"name\": \"Kejun Zhang\"}], \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.09871v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.09871v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.AI\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.MM\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"68Txx(Primary)14F05, 91Fxx(Secondary)\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"I.2.7; J.5\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.10247v1\", \"updated\": \"2024-02-15T10:28:59Z\", \"published\": \"2024-02-15T10:28:59Z\", \"title\": \"Engraving Oriented Joint Estimation of Pitch Spelling and Local and\\n  Global Keys\", \"summary\": \"We revisit the problems of pitch spelling and tonality guessing with a new\\nalgorithm for their joint estimation from a MIDI file including information\\nabout the measure boundaries. Our algorithm does not only identify a global key\\nbut also local ones all along the analyzed piece. It uses Dynamic Programming\\ntechniques to search for an optimal spelling in term, roughly, of the number of\\naccidental symbols that would be displayed in the engraved score. The\\nevaluation of this number is coupled with an estimation of the global key and\\nsome local keys, one for each measure. Each of the three informations is used\\nfor the estimation of the other, in a multi-steps procedure. An evaluation\\nconducted on a monophonic and a piano dataset, comprising 216 464 notes in\\ntotal, shows a high degree of accuracy, both for pitch spelling (99.5% on\\naverage on the Bach corpus and 98.2% on the whole dataset) and global key\\nsignature estimation (93.0% on average, 95.58% on the piano dataset). Designed\\noriginally as a backend tool in a music transcription framework, this method\\nshould also be useful in other tasks related to music notation processing.\", \"author\": [{\"name\": \"Augustin Bouquillard\", \"arxiv:affiliation\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"#text\": \"CEDRIC - VERTIGO\"}}, {\"name\": \"Florent Jacquemard\", \"arxiv:affiliation\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"#text\": \"CEDRIC - VERTIGO\"}}], \"arxiv:comment\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"#text\": \"International Conference on Technologies for Music Notation and\\n  Representation (TENOR), Apr 2024, Zurich (CH), Switzerland\"}, \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.10247v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.10247v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.IR\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.09821v1\", \"updated\": \"2024-02-15T09:36:36Z\", \"published\": \"2024-02-15T09:36:36Z\", \"title\": \"Diffusion Models for Audio Restoration\", \"summary\": \"With the development of audio playback devices and fast data transmission,\\nthe demand for high sound quality is rising, for both entertainment and\\ncommunications. In this quest for better sound quality, challenges emerge from\\ndistortions and interferences originating at the recording side or caused by an\\nimperfect transmission pipeline. To address this problem, audio restoration\\nmethods aim to recover clean sound signals from the corrupted input data. We\\npresent here audio restoration algorithms based on diffusion models, with a\\nfocus on speech enhancement and music restoration tasks. Traditional\\napproaches, often grounded in handcrafted rules and statistical heuristics,\\nhave shaped our understanding of audio signals. In the past decades, there has\\nbeen a notable shift towards data-driven methods that exploit the modeling\\ncapabilities of deep neural networks (DNNs). Deep generative models, and among\\nthem diffusion models, have emerged as powerful techniques for learning complex\\ndata distributions. However, relying solely on DNN-based learning approaches\\ncarries the risk of reducing interpretability, particularly when employing\\nend-to-end models. Nonetheless, data-driven approaches allow more flexibility\\nin comparison to statistical model-based frameworks whose performance depends\\non distributional and statistical assumptions that can be difficult to\\nguarantee. Here, we aim to show that diffusion models can combine the best of\\nboth worlds and offer the opportunity to design audio restoration algorithms\\nwith a good degree of interpretability and a remarkable performance in terms of\\nsound quality.\", \"author\": [{\"name\": \"Jean-Marie Lemercier\"}, {\"name\": \"Julius Richter\"}, {\"name\": \"Simon Welker\"}, {\"name\": \"Eloi Moliner\"}, {\"name\": \"Vesa V\\u00e4lim\\u00e4ki\"}, {\"name\": \"Timo Gerkmann\"}], \"arxiv:comment\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"#text\": \"Full paper invited to the IEEE Signal Processing Magazine Special\\n  Issue \\\"Model-based and Data-Driven Audio Signal Processing\\\"\"}, \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.09821v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.09821v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.LG\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.09797v1\", \"updated\": \"2024-02-15T08:52:31Z\", \"published\": \"2024-02-15T08:52:31Z\", \"title\": \"A cross-talk robust multichannel VAD model for multiparty agent\\n  interactions trained using synthetic re-recordings\", \"summary\": \"In this work, we propose a novel cross-talk rejection framework for a\\nmulti-channel multi-talker setup for a live multiparty interactive show. Our\\nfar-field audio setup is required to be hands-free during live interaction and\\ncomprises four adjacent talkers with directional microphones in the same space.\\nSuch setups often introduce heavy cross-talk between channels, resulting in\\nreduced automatic speech recognition (ASR) and natural language understanding\\n(NLU) performance. To address this problem, we propose voice activity detection\\n(VAD) model for all talkers using multichannel information, which is then used\\nto filter audio for downstream tasks. We adopt a synthetic training data\\ngeneration approach through playback and re-recording for such scenarios,\\nsimulating challenging speech overlap conditions. We train our models on this\\nsynthetic data and demonstrate that our approach outperforms single-channel VAD\\nmodels and energy-based multi-channel VAD algorithm in various acoustic\\nenvironments. In addition to VAD results, we also present multiparty ASR\\nevaluation results to highlight the impact of using our VAD model for filtering\\naudio in downstream tasks by significantly reducing the insertion error.\", \"author\": [{\"name\": \"Hyewon Han\"}, {\"name\": \"Naveen Kumar\"}], \"arxiv:comment\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"#text\": \"Accepted for presentation at the Hands-free Speech Communication and\\n  Microphone Arrays (HSCMA 2024)\"}, \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.09797v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.09797v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.HC\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.09585v1\", \"updated\": \"2024-02-14T21:25:06Z\", \"published\": \"2024-02-14T21:25:06Z\", \"title\": \"Domain Adaptation for Contrastive Audio-Language Models\", \"summary\": \"Audio-Language Models (ALM) aim to be general-purpose audio models by\\nproviding zero-shot capabilities at test time. The zero-shot performance of ALM\\nimproves by using suitable text prompts for each domain. The text prompts are\\nusually hand-crafted through an ad-hoc process and lead to a drop in ALM\\ngeneralization and out-of-distribution performance. Existing approaches to\\nimprove domain performance, like few-shot learning or fine-tuning, require\\naccess to annotated data and iterations of training. Therefore, we propose a\\ntest-time domain adaptation method for ALMs that does not require access to\\nannotations. Our method learns a domain vector by enforcing consistency across\\naugmented views of the testing audio. We extensively evaluate our approach on\\n12 downstream tasks across domains. With just one example, our domain\\nadaptation method leads to 3.2% (max 8.4%) average zero-shot performance\\nimprovement. After adaptation, the model still retains the generalization\\nproperty of ALMs.\", \"author\": [{\"name\": \"Soham Deshmukh\"}, {\"name\": \"Rita Singh\"}, {\"name\": \"Bhiksha Raj\"}], \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.09585v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.09585v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.09508v1\", \"updated\": \"2024-02-14T19:00:01Z\", \"published\": \"2024-02-14T19:00:01Z\", \"title\": \"Arrange, Inpaint, and Refine: Steerable Long-term Music Audio Generation\\n  and Editing via Content-based Controls\", \"summary\": \"Controllable music generation plays a vital role in human-AI music\\nco-creation. While Large Language Models (LLMs) have shown promise in\\ngenerating high-quality music, their focus on autoregressive generation limits\\ntheir utility in music editing tasks. To bridge this gap, we introduce a novel\\nParameter-Efficient Fine-Tuning (PEFT) method. This approach enables\\nautoregressive language models to seamlessly address music inpainting tasks.\\nAdditionally, our PEFT method integrates frame-level content-based controls,\\nfacilitating track-conditioned music refinement and score-conditioned music\\narrangement. We apply this method to fine-tune MusicGen, a leading\\nautoregressive music generation model. Our experiments demonstrate promising\\nresults across multiple music editing tasks, offering more flexible controls\\nfor future AI-driven music editing tools. A demo\\npage\\\\footnote{\\\\url{https://kikyo-16.github.io/AIR/}.} showcasing our work and\\nsource codes\\\\footnote{\\\\url{https://github.com/Kikyo-16/airgen}.} are available\\nonline.\", \"author\": [{\"name\": \"Liwei Lin\"}, {\"name\": \"Gus Xia\"}, {\"name\": \"Yixiao Zhang\"}, {\"name\": \"Junyan Jiang\"}], \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.09508v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.09508v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.AI\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.09378v1\", \"updated\": \"2024-02-14T18:24:41Z\", \"published\": \"2024-02-14T18:24:41Z\", \"title\": \"MobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot\\n  Text-to-Speech\", \"summary\": \"Zero-shot text-to-speech (TTS) has gained significant attention due to its\\npowerful voice cloning capabilities, requiring only a few seconds of unseen\\nspeaker voice prompts. However, all previous work has been developed for\\ncloud-based systems. Taking autoregressive models as an example, although these\\napproaches achieve high-fidelity voice cloning, they fall short in terms of\\ninference speed, model size, and robustness. Therefore, we propose\\nMobileSpeech, which is a fast, lightweight, and robust zero-shot text-to-speech\\nsystem based on mobile devices for the first time. Specifically: 1) leveraging\\ndiscrete codec, we design a parallel speech mask decoder module called SMD,\\nwhich incorporates hierarchical information from the speech codec and weight\\nmechanisms across different codec layers during the generation process.\\nMoreover, to bridge the gap between text and speech, we introduce a high-level\\nprobabilistic mask that simulates the progression of information flow from less\\nto more during speech generation. 2) For speaker prompts, we extract\\nfine-grained prompt duration from the prompt speech and incorporate text,\\nprompt speech by cross attention in SMD. We demonstrate the effectiveness of\\nMobileSpeech on multilingual datasets at different levels, achieving\\nstate-of-the-art results in terms of generating speed and speech quality.\\nMobileSpeech achieves RTF of 0.09 on a single A100 GPU and we have successfully\\ndeployed MobileSpeech on mobile devices. Audio samples are available at\\n\\\\url{https://mobilespeech.github.io/} .\", \"author\": [{\"name\": \"Shengpeng Ji\"}, {\"name\": \"Ziyue Jiang\"}, {\"name\": \"Hanting Wang\"}, {\"name\": \"Jialong Zuo\"}, {\"name\": \"Zhou Zhao\"}], \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.09378v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.09378v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.09318v1\", \"updated\": \"2024-02-14T17:13:36Z\", \"published\": \"2024-02-14T17:13:36Z\", \"title\": \"Leveraging Pre-Trained Autoencoders for Interpretable Prototype Learning\\n  of Music Audio\", \"summary\": \"We present PECMAE, an interpretable model for music audio classification\\nbased on prototype learning. Our model is based on a previous method, APNet,\\nwhich jointly learns an autoencoder and a prototypical network. Instead, we\\npropose to decouple both training processes. This enables us to leverage\\nexisting self-supervised autoencoders pre-trained on much larger data\\n(EnCodecMAE), providing representations with better generalization. APNet\\nallows prototypes' reconstruction to waveforms for interpretability relying on\\nthe nearest training data samples. In contrast, we explore using a diffusion\\ndecoder that allows reconstruction without such dependency. We evaluate our\\nmethod on datasets for music instrument classification (Medley-Solos-DB) and\\ngenre recognition (GTZAN and a larger in-house dataset), the latter being a\\nmore challenging task not addressed with prototypical networks before. We find\\nthat the prototype-based models preserve most of the performance achieved with\\nthe autoencoder embeddings, while the sonification of prototypes benefits\\nunderstanding the behavior of the classifier.\", \"author\": [{\"name\": \"Pablo Alonso-Jim\\u00e9nez\"}, {\"name\": \"Leonardo Pepino\"}, {\"name\": \"Roser Batlle-Roca\"}, {\"name\": \"Pablo Zinemanas\"}, {\"name\": \"Dmitry Bogdanov\"}, {\"name\": \"Xavier Serra\"}, {\"name\": \"Mart\\u00edn Rocamora\"}], \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.09318v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.09318v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.AI\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.MM\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.08932v1\", \"updated\": \"2024-02-14T04:21:11Z\", \"published\": \"2024-02-14T04:21:11Z\", \"title\": \"Listening to Multi-talker Conversations: Modular and End-to-end\\n  Perspectives\", \"summary\": \"Since the first speech recognition systems were built more than 30 years ago,\\nimprovement in voice technology has enabled applications such as smart\\nassistants and automated customer support. However, conversation intelligence\\nof the future requires recognizing free-flowing multi-party conversations,\\nwhich is a crucial and challenging component that still remains unsolved. In\\nthis dissertation, we focus on this problem of speaker-attributed multi-talker\\nspeech recognition, and propose two perspectives which result from its\\nprobabilistic formulation.\\n  In the modular perspective, we build a pipeline of sub-tasks involving\\nspeaker diarization, target speaker extraction, and speech recognition. Our\\nfirst contribution is a method to perform overlap-aware diarization by\\nreformulating spectral clustering as a constrained optimization problem. We\\nalso describe an algorithm to ensemble diarization outputs, either to combine\\noverlap-aware systems or to perform multi-channel diarization by late fusion.\\nOnce speaker segments are identified, we robustly extract single-speaker\\nutterances from the mixture using a GPU-accelerated implementation of guided\\nsource separation, which allows us to use an off-the-shelf ASR system to obtain\\nspeaker-attributed transcripts.\\n  Since the modular approach suffers from error propagation, we propose an\\nalternate \\\"end-to-end\\\" perspective on the problem. For this, we describe the\\nStreaming Unmixing and Recognition Transducer (SURT). We show how to train SURT\\nmodels efficiently by carefully designing the network architecture, objective\\nfunctions, and mixture simulation techniques. Finally, we add an auxiliary\\nspeaker branch to enable joint prediction of speaker labels synchronized with\\nthe speech tokens. We demonstrate that training on synthetic mixtures and\\nadapting with real data helps these models transfer well for streaming\\ntranscription of real meeting sessions.\", \"author\": {\"name\": \"Desh Raj\"}, \"arxiv:comment\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"#text\": \"Ph.D. dissertation\"}, \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.08932v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.08932v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.08904v1\", \"updated\": \"2024-02-14T02:32:18Z\", \"published\": \"2024-02-14T02:32:18Z\", \"title\": \"Sound Field Reconstruction Using a Compact Acoustics-informed Neural\\n  Network\", \"summary\": \"Sound field reconstruction (SFR) augments the information of a sound field\\ncaptured by a microphone array. Conventional SFR methods using basis function\\ndecomposition are straightforward and computationally efficient, but may\\nrequire more microphones than needed to measure the sound field. Recent studies\\nshow that pure data-driven and learning-based methods are promising in some SFR\\ntasks, but they are usually computationally heavy and may fail to reconstruct a\\nphysically valid sound field. This paper proposes a compact acoustics-informed\\nneural network (AINN) method for SFR, whereby the Helmholtz equation is\\nexploited to regularize the neural network. As opposed to pure data-driven\\napproaches that solely rely on measured sound pressures, the integration of the\\nHelmholtz equation improves robustness of the neural network against variations\\nduring the measurement processes and prompts the generation of physically valid\\nreconstructions. The AINN is designed to be compact, and is able to predict not\\nonly the sound pressures but also sound pressure gradients within a spatial\\nregion of interest based on measured sound pressures along the boundary.\\nNumerical experiments with acoustic transfer functions measured in different\\nenvironments demonstrate the superiority of the AINN method over the\\ntraditional cylinder harmonic decomposition and the singular value\\ndecomposition methods.\", \"author\": [{\"name\": \"Fei Ma\"}, {\"name\": \"Sipei Zhao\"}, {\"name\": \"Ian S. Burnett\"}], \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.08904v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.08904v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.08898v1\", \"updated\": \"2024-02-14T02:11:04Z\", \"published\": \"2024-02-14T02:11:04Z\", \"title\": \"UniEnc-CASSNAT: An Encoder-only Non-autoregressive ASR for Speech SSL\\n  Models\", \"summary\": \"Non-autoregressive automatic speech recognition (NASR) models have gained\\nattention due to their parallelism and fast inference. The encoder-based NASR,\\ne.g. connectionist temporal classification (CTC), can be initialized from the\\nspeech foundation models (SFM) but does not account for any dependencies among\\nintermediate tokens. The encoder-decoder-based NASR, like CTC alignment-based\\nsingle-step non-autoregressive transformer (CASS-NAT), can mitigate the\\ndependency problem but is not able to efficiently integrate SFM. Inspired by\\nthe success of recent work of speech-text joint pre-training with a shared\\ntransformer encoder, we propose a new encoder-based NASR, UniEnc-CASSNAT, to\\ncombine the advantages of CTC and CASS-NAT. UniEnc-CASSNAT consists of only an\\nencoder as the major module, which can be the SFM. The encoder plays the role\\nof both the CASS-NAT encoder and decoder by two forward passes. The first pass\\nof the encoder accepts the speech signal as input, while the concatenation of\\nthe speech signal and the token-level acoustic embedding is used as the input\\nfor the second pass. Examined on the Librispeech 100h, MyST, and Aishell1\\ndatasets, the proposed UniEnc-CASSNAT achieves state-of-the-art NASR results\\nand is better or comparable to CASS-NAT with only an encoder and hence, fewer\\nmodel parameters. Our codes are publicly available.\", \"author\": [{\"name\": \"Ruchao Fan\"}, {\"name\": \"Natarajan Balaji Shanka\"}, {\"name\": \"Abeer Alwan\"}], \"arxiv:doi\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"#text\": \"10.1109/LSP.2024.3365036\"}, \"link\": [{\"@title\": \"doi\", \"@href\": \"http://dx.doi.org/10.1109/LSP.2024.3365036\", \"@rel\": \"related\"}, {\"@href\": \"http://arxiv.org/abs/2402.08898v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.08898v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:comment\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"#text\": \"Published in IEEE Signal Processing Letters\"}, \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.CL\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.08846v1\", \"updated\": \"2024-02-13T23:25:04Z\", \"published\": \"2024-02-13T23:25:04Z\", \"title\": \"An Embarrassingly Simple Approach for LLM with Strong ASR Capacity\", \"summary\": \"In this paper, we focus on solving one of the most important tasks in the\\nfield of speech processing, i.e., automatic speech recognition (ASR), with\\nspeech foundation encoders and large language models (LLM). Recent works have\\ncomplex designs such as compressing the output temporally for the speech\\nencoder, tackling modal alignment for the projector, and utilizing\\nparameter-efficient fine-tuning for the LLM. We found that delicate designs are\\nnot necessary, while an embarrassingly simple composition of off-the-shelf\\nspeech encoder, LLM, and the only trainable linear projector is competent for\\nthe ASR task. To be more specific, we benchmark and explore various\\ncombinations of LLMs and speech encoders, leading to the optimal LLM-based ASR\\nsystem, which we call SLAM-ASR. The proposed SLAM-ASR provides a clean setup\\nand little task-specific design, where only the linear projector is trained. To\\nthe best of our knowledge, SLAM-ASR achieves the best performance on the\\nLibrispeech benchmark among LLM-based ASR models and even outperforms the\\nlatest LLM-based audio-universal model trained on massive pair data. Finally,\\nwe explore the capability emergence of LLM-based ASR in the process of modal\\nalignment. We hope that our study can facilitate the research on extending LLM\\nwith cross-modality capacity and shed light on the LLM-based ASR community.\", \"author\": [{\"name\": \"Ziyang Ma\"}, {\"name\": \"Guanrou Yang\"}, {\"name\": \"Yifan Yang\"}, {\"name\": \"Zhifu Gao\"}, {\"name\": \"Jiaming Wang\"}, {\"name\": \"Zhihao Du\"}, {\"name\": \"Fan Yu\"}, {\"name\": \"Qian Chen\"}, {\"name\": \"Siqi Zheng\"}, {\"name\": \"Shiliang Zhang\"}, {\"name\": \"Xie Chen\"}], \"arxiv:comment\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"#text\": \"Working in progress and will open-source soon\"}, \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.08846v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.08846v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"cs.CL\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"cs.CL\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.AI\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.MM\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.08788v1\", \"updated\": \"2024-02-13T20:54:24Z\", \"published\": \"2024-02-13T20:54:24Z\", \"title\": \"Syllable based DNN-HMM Cantonese Speech to Text System\", \"summary\": \"This paper reports our work on building up a Cantonese Speech-to-Text (STT)\\nsystem with a syllable based acoustic model. This is a part of an effort in\\nbuilding a STT system to aid dyslexic students who have cognitive deficiency in\\nwriting skills but have no problem expressing their ideas through speech. For\\nCantonese speech recognition, the basic unit of acoustic models can either be\\nthe conventional Initial-Final (IF) syllables, or the Onset-Nucleus-Coda (ONC)\\nsyllables where finals are further split into nucleus and coda to reflect the\\nintra-syllable variations in Cantonese. By using the Kaldi toolkit, our system\\nis trained using the stochastic gradient descent optimization model with the\\naid of GPUs for the hybrid Deep Neural Network and Hidden Markov Model\\n(DNN-HMM) with and without I-vector based speaker adaptive training technique.\\nThe input features of the same Gaussian Mixture Model with speaker adaptive\\ntraining (GMM-SAT) to DNN are used in all cases. Experiments show that the\\nONC-based syllable acoustic modeling with I-vector based DNN-HMM achieves the\\nbest performance with the word error rate (WER) of 9.66% and the real time\\nfactor (RTF) of 1.38812.\", \"author\": [{\"name\": \"Timothy Wong\"}, {\"name\": \"Claire Li\"}, {\"name\": \"Sam Lam\"}, {\"name\": \"Billy Chiu\"}, {\"name\": \"Qin Lu\"}, {\"name\": \"Minglei Li\"}, {\"name\": \"Dan Xiong\"}, {\"name\": \"Roy Shing Yu\"}, {\"name\": \"Vincent T. Y. Ng\"}], \"arxiv:comment\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"#text\": \"7 pages, 3 figures, LREC 2016\"}, \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.08788v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.08788v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"cs.CL\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"cs.CL\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"94-06\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"I.2.7\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.08521v1\", \"updated\": \"2024-02-13T15:24:19Z\", \"published\": \"2024-02-13T15:24:19Z\", \"title\": \"Benchmarking multi-component signal processing methods in the\\n  time-frequency plane\", \"summary\": \"Signal processing in the time-frequency plane has a long history and remains\\na field of methodological innovation. For instance, detection and denoising\\nbased on the zeros of the spectrogram have been proposed since 2015,\\ncontrasting with a long history of focusing on larger values of the\\nspectrogram. Yet, unlike neighboring fields like optimization and machine\\nlearning, time-frequency signal processing lacks widely-adopted benchmarking\\ntools. In this work, we contribute an open-source, Python-based toolbox termed\\nMCSM-Benchs for benchmarking multi-component signal analysis methods, and we\\ndemonstrate our toolbox on three time-frequency benchmarks. First, we compare\\ndifferent methods for signal detection based on the zeros of the spectrogram,\\nincluding unexplored variations of previously proposed detection tests. Second,\\nwe compare zero-based denoising methods to both classical and novel methods\\nbased on large values and ridges of the spectrogram. Finally, we compare the\\ndenoising performance of these methods against typical spectrogram thresholding\\nstrategies, in terms of post-processing artifacts commonly referred to as\\nmusical noise. At a low level, the obtained results provide new insight on the\\nassessed approaches, and in particular research directions to further develop\\nzero-based methods. At a higher level, our benchmarks exemplify the benefits of\\nusing a public, collaborative, common framework for benchmarking.\", \"author\": [{\"name\": \"Juan M. Miramont\"}, {\"name\": \"R\\u00e9mi Bardenet\"}, {\"name\": \"Pierre Chainais\"}, {\"name\": \"Francois Auger\"}], \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.08521v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.08521v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"eess.SP\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"eess.SP\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.08312v1\", \"updated\": \"2024-02-13T09:16:40Z\", \"published\": \"2024-02-13T09:16:40Z\", \"title\": \"Channel-Combination Algorithms for Robust Distant Voice Activity and\\n  Overlapped Speech Detection\", \"summary\": \"Voice Activity Detection (VAD) and Overlapped Speech Detection (OSD) are key\\npre-processing tasks for speaker diarization. In the meeting context, it is\\noften easier to capture speech with a distant device. This consideration\\nhowever leads to severe performance degradation. We study a unified supervised\\nlearning framework to solve distant multi-microphone joint VAD and OSD\\n(VAD+OSD). This paper investigates various multi-channel VAD+OSD front-ends\\nthat weight and combine incoming channels. We propose three algorithms based on\\nthe Self-Attention Channel Combinator (SACC), previously proposed in the\\nliterature. Experiments conducted on the AMI meeting corpus exhibit that\\nchannel combination approaches bring significant VAD+OSD improvements in the\\ndistant speech scenario. Specifically, we explore the use of learned complex\\ncombination weights and demonstrate the benefits of such an approach in terms\\nof explainability. Channel combination-based VAD+OSD systems are evaluated on\\nthe final back-end task, i.e. speaker diarization, and show significant\\nimprovements. Finally, since multi-channel systems are trained given a fixed\\narray configuration, they may fail in generalizing to other array set-ups, e.g.\\nmismatched number of microphones. A channel-number invariant loss is proposed\\nto learn a unique feature representation regardless of the number of available\\nmicrophones. The evaluation conducted on mismatched array configurations\\nhighlights the robustness of this training strategy.\", \"author\": [{\"name\": \"Th\\u00e9o Mariotte\"}, {\"name\": \"Anthony Larcher\"}, {\"name\": \"Silvio Montr\\u00e9sor\"}, {\"name\": \"Jean-Hugh Thomas\"}], \"arxiv:comment\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"#text\": \"14 pages, 5 figures, accepted at IEEE/ACM Transactions on Audio,\\n  Speech and Language Processing (TASLP)\"}, \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.08312v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.08312v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.08252v1\", \"updated\": \"2024-02-13T06:47:26Z\", \"published\": \"2024-02-13T06:47:26Z\", \"title\": \"Unrestricted Global Phase Bias-Aware Single-channel Speech Enhancement\\n  with Conformer-based Metric GAN\", \"summary\": \"With the rapid development of neural networks in recent years, the ability of\\nvarious networks to enhance the magnitude spectrum of noisy speech in the\\nsingle-channel speech enhancement domain has become exceptionally outstanding.\\nHowever, enhancing the phase spectrum using neural networks is often\\nineffective, which remains a challenging problem. In this paper, we found that\\nthe human ear cannot sensitively perceive the difference between a precise\\nphase spectrum and a biased phase (BP) spectrum. Therefore, we propose an\\noptimization method of phase reconstruction, allowing freedom on the\\nglobal-phase bias instead of reconstructing the precise phase spectrum. We\\napplied it to a Conformer-based Metric Generative Adversarial Networks (CMGAN)\\nbaseline model, which relaxes the existing constraints of precise phase and\\ngives the neural network a broader learning space. Results show that this\\nmethod achieves a new state-of-the-art performance without incurring additional\\ncomputational overhead.\", \"author\": [{\"name\": \"Shiqi Zhang\"}, {\"name\": \"Zheng Qiu\"}, {\"name\": \"Daiki Takeuchi\"}, {\"name\": \"Noboru Harada\"}, {\"name\": \"Shoji Makino\"}], \"arxiv:comment\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"#text\": \"Accepted by ICASSP 2024\"}, \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.08252v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.08252v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.08217v1\", \"updated\": \"2024-02-13T05:10:04Z\", \"published\": \"2024-02-13T05:10:04Z\", \"title\": \"Springboard, Roadblock or \\\"Crutch\\\"?: How Transgender Users Leverage\\n  Voice Changers for Gender Presentation in Social Virtual Reality\", \"summary\": \"Social virtual reality (VR) serves as a vital platform for transgender\\nindividuals to explore their identities through avatars and foster personal\\nconnections within online communities. However, it presents a challenge: the\\ndisconnect between avatar embodiment and voice representation, often leading to\\nmisgendering and harassment. Prior research acknowledges this issue but\\noverlooks the potential solution of voice changers. We interviewed 13\\ntransgender and gender-nonconforming users of social VR platforms, focusing on\\ntheir experiences with and without voice changers. We found that using a voice\\nchanger not only reduces voice-related harassment, but also allows them to\\nexperience gender euphoria through both hearing their modified voice and the\\nreactions of others to their modified voice, motivating them to pursue voice\\ntraining and medication to achieve desired voices. Furthermore, we identified\\nthe technical barriers to current voice changer technology and potential\\nimprovements to alleviate the problems that transgender and\\ngender-nonconforming users face.\", \"author\": [{\"name\": \"Kassie Povinelli\"}, {\"name\": \"Yuhang Zhao\"}], \"arxiv:journal_ref\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"#text\": \"IEEE VR 2024\"}, \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.08217v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.08217v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"cs.HC\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"cs.HC\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.07729v1\", \"updated\": \"2024-02-12T15:41:22Z\", \"published\": \"2024-02-12T15:41:22Z\", \"title\": \"AIR-Bench: Benchmarking Large Audio-Language Models via Generative\\n  Comprehension\", \"summary\": \"Recently, instruction-following audio-language models have received broad\\nattention for human-audio interaction. However, the absence of benchmarks\\ncapable of evaluating audio-centric interaction capabilities has impeded\\nadvancements in this field. Previous models primarily focus on assessing\\ndifferent fundamental tasks, such as Automatic Speech Recognition (ASR), and\\nlack an assessment of the open-ended generative capabilities centered around\\naudio. Thus, it is challenging to track the progression in the Large\\nAudio-Language Models (LALMs) domain and to provide guidance for future\\nimprovement. In this paper, we introduce AIR-Bench (\\\\textbf{A}udio\\n\\\\textbf{I}nst\\\\textbf{R}uction \\\\textbf{Bench}mark), the first benchmark designed\\nto evaluate the ability of LALMs to understand various types of audio signals\\n(including human speech, natural sounds, and music), and furthermore, to\\ninteract with humans in the textual format. AIR-Bench encompasses two\\ndimensions: \\\\textit{foundation} and \\\\textit{chat} benchmarks. The former\\nconsists of 19 tasks with approximately 19k single-choice questions, intending\\nto inspect the basic single-task ability of LALMs. The latter one contains 2k\\ninstances of open-ended question-and-answer data, directly assessing the\\ncomprehension of the model on complex audio and its capacity to follow\\ninstructions. Both benchmarks require the model to generate hypotheses\\ndirectly. We design a unified framework that leverages advanced language\\nmodels, such as GPT-4, to evaluate the scores of generated hypotheses given the\\nmeta-information of the audio. Experimental results demonstrate a high level of\\nconsistency between GPT-4-based evaluation and human evaluation. By revealing\\nthe limitations of existing LALMs through evaluation results, AIR-Bench can\\nprovide insights into the direction of future research.\", \"author\": [{\"name\": \"Qian Yang\"}, {\"name\": \"Jin Xu\"}, {\"name\": \"Wenrui Liu\"}, {\"name\": \"Yunfei Chu\"}, {\"name\": \"Ziyue Jiang\"}, {\"name\": \"Xiaohuan Zhou\"}, {\"name\": \"Yichong Leng\"}, {\"name\": \"Yuanjun Lv\"}, {\"name\": \"Zhou Zhao\"}, {\"name\": \"Chang Zhou\"}, {\"name\": \"Jingren Zhou\"}], \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.07729v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.07729v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.CL\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.LG\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.07658v1\", \"updated\": \"2024-02-12T14:01:12Z\", \"published\": \"2024-02-12T14:01:12Z\", \"title\": \"The Sound of Healthcare: Improving Medical Transcription ASR Accuracy\\n  with Large Language Models\", \"summary\": \"In the rapidly evolving landscape of medical documentation, transcribing\\nclinical dialogues accurately is increasingly paramount. This study explores\\nthe potential of Large Language Models (LLMs) to enhance the accuracy of\\nAutomatic Speech Recognition (ASR) systems in medical transcription. Utilizing\\nthe PriMock57 dataset, which encompasses a diverse range of primary care\\nconsultations, we apply advanced LLMs to refine ASR-generated transcripts. Our\\nresearch is multifaceted, focusing on improvements in general Word Error Rate\\n(WER), Medical Concept WER (MC-WER) for the accurate transcription of essential\\nmedical terms, and speaker diarization accuracy. Additionally, we assess the\\nrole of LLM post-processing in improving semantic textual similarity, thereby\\npreserving the contextual integrity of clinical dialogues. Through a series of\\nexperiments, we compare the efficacy of zero-shot and Chain-of-Thought (CoT)\\nprompting techniques in enhancing diarization and correction accuracy. Our\\nfindings demonstrate that LLMs, particularly through CoT prompting, not only\\nimprove the diarization accuracy of existing ASR systems but also achieve\\nstate-of-the-art performance in this domain. This improvement extends to more\\naccurately capturing medical concepts and enhancing the overall semantic\\ncoherence of the transcribed dialogues. These findings illustrate the dual role\\nof LLMs in augmenting ASR outputs and independently excelling in transcription\\ntasks, holding significant promise for transforming medical ASR systems and\\nleading to more accurate and reliable patient records in healthcare settings.\", \"author\": [{\"name\": \"Ayo Adedeji\"}, {\"name\": \"Sarita Joshi\"}, {\"name\": \"Brendan Doohan\"}], \"arxiv:comment\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"#text\": \"31 pages, 17 figures\"}, \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.07658v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.07658v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"cs.CL\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"cs.CL\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.07619v1\", \"updated\": \"2024-02-12T12:52:47Z\", \"published\": \"2024-02-12T12:52:47Z\", \"title\": \"Developing a Multi-variate Prediction Model For COVID-19 From\\n  Crowd-sourced Respiratory Voice Data\", \"summary\": \"COVID-19 has affected more than 223 countries worldwide and in the Post-COVID\\nEra, there is a pressing need for non-invasive, low-cost, and highly scalable\\nsolutions to detect COVID-19. We develop a deep learning model to identify\\nCOVID-19 from voice recording data. The novelty of this work is in the\\ndevelopment of deep learning models for COVID-19 identification from only voice\\nrecordings. We use the Cambridge COVID-19 Sound database which contains 893\\nspeech samples, crowd-sourced from 4352 participants via a COVID-19 Sounds app.\\nVoice features including Mel-spectrograms and Mel-frequency cepstral\\ncoefficients (MFCC) and CNN Encoder features are extracted. Based on the voice\\ndata, we develop deep learning classification models to detect COVID-19 cases.\\nThese models include Long Short-Term Memory (LSTM) and Convolutional Neural\\nNetwork (CNN) and Hidden-Unit BERT (HuBERT). We compare their predictive power\\nto baseline machine learning models. HuBERT achieves the highest accuracy of\\n86\\\\% and the highest AUC of 0.93. The results achieved with the proposed models\\nsuggest promising results in COVID-19 diagnosis from voice recordings when\\ncompared to the results obtained from the state-of-the-art.\", \"author\": [{\"name\": \"Yuyang Yan\"}, {\"name\": \"Wafaa Aljbawi\"}, {\"name\": \"Sami O. Simons\"}, {\"name\": \"Visara Urovi\"}], \"arxiv:comment\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"#text\": \"arXiv admin note: text overlap with arXiv:2209.03727\"}, \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.07619v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.07619v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.AI\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.07599v1\", \"updated\": \"2024-02-12T11:58:41Z\", \"published\": \"2024-02-12T11:58:41Z\", \"title\": \"Interactive singing melody extraction based on active adaptation\", \"summary\": \"Extraction of predominant pitch from polyphonic audio is one of the\\nfundamental tasks in the field of music information retrieval and computational\\nmusicology. To accomplish this task using machine learning, a large amount of\\nlabeled audio data is required to train the model. However, a classical model\\npre-trained on data from one domain (source), e.g., songs of a particular\\nsinger or genre, may not perform comparatively well in extracting melody from\\nother domains (target). The performance of such models can be boosted by\\nadapting the model using very little annotated data from the target domain. In\\nthis work, we propose an efficient interactive melody adaptation method. Our\\nmethod selects the regions in the target audio that require human annotation\\nusing a confidence criterion based on normalized true class probability. The\\nannotations are used by the model to adapt itself to the target domain using\\nmeta-learning. Our method also provides a novel meta-learning approach that\\nhandles class imbalance, i.e., a few representative samples from a few classes\\nare available for adaptation in the target domain. Experimental results show\\nthat the proposed method outperforms other adaptive melody extraction\\nbaselines. The proposed method is model-agnostic and hence can be applied to\\nother non-adaptive melody extraction models to boost their performance. Also,\\nwe released a Hindustani Alankaar and Raga (HAR) dataset containing 523 audio\\nfiles of about 6.86 hours of duration intended for singing melody extraction\\ntasks.\", \"author\": [{\"name\": \"Kavya Ranjan Saxena\"}, {\"name\": \"Vipul Arora\"}], \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.07599v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.07599v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}, {\"id\": \"http://arxiv.org/abs/2402.07596v1\", \"updated\": \"2024-02-12T11:52:21Z\", \"published\": \"2024-02-12T11:52:21Z\", \"title\": \"Sheet Music Transformer: End-To-End Optical Music Recognition Beyond\\n  Monophonic Transcription\", \"summary\": \"State-of-the-art end-to-end Optical Music Recognition (OMR) has, to date,\\nprimarily been carried out using monophonic transcription techniques to handle\\ncomplex score layouts, such as polyphony, often by resorting to simplifications\\nor specific adaptations. Despite their efficacy, these approaches imply\\nchallenges related to scalability and limitations. This paper presents the\\nSheet Music Transformer, the first end-to-end OMR model designed to transcribe\\ncomplex musical scores without relying solely on monophonic strategies. Our\\nmodel employs a Transformer-based image-to-sequence framework that predicts\\nscore transcriptions in a standard digital music encoding format from input\\nimages. Our model has been tested on two polyphonic music datasets and has\\nproven capable of handling these intricate music structures effectively. The\\nexperimental outcomes not only indicate the competence of the model, but also\\nshow that it is better than the state-of-the-art methods, thus contributing to\\nadvancements in end-to-end OMR transcription.\", \"author\": [{\"name\": \"Antonio R\\u00edos-Vila\"}, {\"name\": \"Jorge Calvo-Zaragoza\"}, {\"name\": \"Thierry Paquet\"}], \"arxiv:comment\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"#text\": \"Submitted to the International Conference on Document Analysis and\\n  Recognition 2024\"}, \"link\": [{\"@href\": \"http://arxiv.org/abs/2402.07596v1\", \"@rel\": \"alternate\", \"@type\": \"text/html\"}, {\"@title\": \"pdf\", \"@href\": \"http://arxiv.org/pdf/2402.07596v1\", \"@rel\": \"related\", \"@type\": \"application/pdf\"}], \"arxiv:primary_category\": {\"@xmlns:arxiv\": \"http://arxiv.org/schemas/atom\", \"@term\": \"cs.CV\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, \"category\": [{\"@term\": \"cs.CV\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"cs.SD\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}, {\"@term\": \"eess.AS\", \"@scheme\": \"http://arxiv.org/schemas/atom\"}]}]}}\n"
     ]
    }
   ],
   "source": [
    "import xmltodict\n",
    "import json\n",
    "\n",
    "dict_data = xmltodict.parse(xml_data)\n",
    "json_data = json.dumps(dict_data)\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 'http://arxiv.org/abs/2402.13236v1', 'updated': '2024-02-20T18:50:25Z', 'published': '2024-02-20T18:50:25Z', 'title': 'Towards audio language modeling - an overview', 'summary': 'Neural audio codecs are initially introduced to compress audio data into\\ncompact codes to reduce transmission latency. Researchers recently discovered\\nthe potential of codecs as suitable tokenizers for converting continuous audio\\ninto discrete codes, which can be employed to develop audio language models\\n(LMs). Numerous high-performance neural audio codecs and codec-based LMs have\\nbeen developed. The paper aims to provide a thorough and systematic overview of\\nthe neural audio codec models and codec-based LMs.', 'author': [{'name': 'Haibin Wu'}, {'name': 'Xuanjun Chen'}, {'name': 'Yi-Cheng Lin'}, {'name': 'Kai-wei Chang'}, {'name': 'Ho-Lam Chung'}, {'name': 'Alexander H. Liu'}, {'name': 'Hung-yi Lee'}], 'link': [{'@href': 'http://arxiv.org/abs/2402.13236v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2402.13236v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2402.13110v1', 'updated': '2024-02-20T16:03:02Z', 'published': '2024-02-20T16:03:02Z', 'title': 'HiRIS: an Airborne Sonar Sensor with a 1024 Channel Microphone Array for\\n  In-Air Acoustic Imaging', 'summary': 'Airborne 3D imaging using ultrasound is a promising sensing modality for\\nrobotic applications in harsh environments. Over the last decade, several\\nhigh-performance systems have been proposed in the literature. Most of these\\nsensors use a reduced aperture microphone array, leading to artifacts in the\\nresulting acoustic images. This paper presents a novel in-air ultrasound sensor\\nthat incorporates 1024 microphones, in a 32-by- 32 uniform rectangular array,\\nin combination with a distributed embedded hardware design to perform the data\\nacquisition. Using a broadband Minimum Variance Distortionless Response (MVDR)\\nbeamformer with Forward-Backward Spatial Smoothing (FB-SS), the sensor is able\\nto create both 2D and 3D ultrasound images of the full-frontal hemisphere with\\nhigh angular accuracy with up to 70dB main lobe to side lobe ratio. This paper\\ndescribes both the hardware infrastructure needed to obtain such highly\\ndetailed acoustical images, as well as the signal processing chain needed to\\nconvert the raw acoustic data into said images. Utilizing this novel\\nhigh-resolution ultrasound imaging sensor, we wish to investigate the limits of\\nboth passive and active airborne ultrasound sensing by utilizing this virtually\\nartifact-free imaging modality.', 'author': [{'name': 'Dennis Laurijssen'}, {'name': 'Walter Daems'}, {'name': 'Jan Steckel'}], 'link': [{'@href': 'http://arxiv.org/abs/2402.13110v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2402.13110v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'eess.SP', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'eess.SP', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2402.13076v1', 'updated': '2024-02-20T15:22:25Z', 'published': '2024-02-20T15:22:25Z', 'title': 'Not All Weights Are Created Equal: Enhancing Energy Efficiency in\\n  On-Device Streaming Speech Recognition', 'summary': 'Power consumption plays an important role in on-device streaming speech\\nrecognition, as it has a direct impact on the user experience. This study\\ndelves into how weight parameters in speech recognition models influence the\\noverall power consumption of these models. We discovered that the impact of\\nweight parameters on power consumption varies, influenced by factors including\\nhow often they are invoked and their placement in memory. Armed with this\\ninsight, we developed design guidelines aimed at optimizing on-device speech\\nrecognition models. These guidelines focus on minimizing power use without\\nsubstantially affecting accuracy. Our method, which employs targeted\\ncompression based on the varying sensitivities of weight parameters,\\ndemonstrates superior performance compared to state-of-the-art compression\\nmethods. It achieves a reduction in energy usage of up to 47% while maintaining\\nsimilar model accuracy and improving the real-time factor.', 'author': [{'name': 'Yang Li'}, {'name': 'Yuan Shangguan'}, {'name': 'Yuhao Wang'}, {'name': 'Liangzhen Lai'}, {'name': 'Ernie Chang'}, {'name': 'Changsheng Zhao'}, {'name': 'Yangyang Shi'}, {'name': 'Vikas Chandra'}], 'link': [{'@href': 'http://arxiv.org/abs/2402.13076v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2402.13076v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2402.13071v1', 'updated': '2024-02-20T15:13:38Z', 'published': '2024-02-20T15:13:38Z', 'title': 'Codec-SUPERB: An In-Depth Analysis of Sound Codec Models', 'summary': \"The sound codec's dual roles in minimizing data transmission latency and\\nserving as tokenizers underscore its critical importance. Recent years have\\nwitnessed significant developments in codec models. The ideal sound codec\\nshould preserve content, paralinguistics, speakers, and audio information.\\nHowever, the question of which codec achieves optimal sound information\\npreservation remains unanswered, as in different papers, models are evaluated\\non their selected experimental settings. This study introduces Codec-SUPERB, an\\nacronym for Codec sound processing Universal PERformance Benchmark. It is an\\necosystem designed to assess codec models across representative sound\\napplications and signal-level metrics rooted in sound domain\\nknowledge.Codec-SUPERB simplifies result sharing through an online leaderboard,\\npromoting collaboration within a community-driven benchmark database, thereby\\nstimulating new development cycles for codecs. Furthermore, we undertake an\\nin-depth analysis to offer insights into codec models from both application and\\nsignal perspectives, diverging from previous codec papers mainly concentrating\\non signal-level comparisons. Finally, we will release codes, the leaderboard,\\nand data to accelerate progress within the community.\", 'author': [{'name': 'Haibin Wu'}, {'name': 'Ho-Lam Chung'}, {'name': 'Yi-Cheng Lin'}, {'name': 'Yuan-Kuei Wu'}, {'name': 'Xuanjun Chen'}, {'name': 'Yu-Chi Pai'}, {'name': 'Hsiu-Hsuan Wang'}, {'name': 'Kai-Wei Chang'}, {'name': 'Alexander H. Liu'}, {'name': 'Hung-yi Lee'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'Github: https://github.com/voidful/Codec-SUPERB'}, 'link': [{'@href': 'http://arxiv.org/abs/2402.13071v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2402.13071v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2402.13018v1', 'updated': '2024-02-20T14:00:53Z', 'published': '2024-02-20T14:00:53Z', 'title': 'EMO-SUPERB: An In-depth Look at Speech Emotion Recognition', 'summary': 'Speech emotion recognition (SER) is a pivotal technology for human-computer\\ninteraction systems. However, 80.77% of SER papers yield results that cannot be\\nreproduced. We develop EMO-SUPERB, short for EMOtion Speech Universal\\nPERformance Benchmark, which aims to enhance open-source initiatives for SER.\\nEMO-SUPERB includes a user-friendly codebase to leverage 15 state-of-the-art\\nspeech self-supervised learning models (SSLMs) for exhaustive evaluation across\\nsix open-source SER datasets. EMO-SUPERB streamlines result sharing via an\\nonline leaderboard, fostering collaboration within a community-driven benchmark\\nand thereby enhancing the development of SER. On average, 2.58% of annotations\\nare annotated using natural language. SER relies on classification models and\\nis unable to process natural languages, leading to the discarding of these\\nvaluable annotations. We prompt ChatGPT to mimic annotators, comprehend natural\\nlanguage annotations, and subsequently re-label the data. By utilizing labels\\ngenerated by ChatGPT, we consistently achieve an average relative gain of 3.08%\\nacross all settings.', 'author': [{'name': 'Haibin Wu'}, {'name': 'Huang-Cheng Chou'}, {'name': 'Kai-Wei Chang'}, {'name': 'Lucas Goncalves'}, {'name': 'Jiawei Du'}, {'name': 'Jyh-Shing Roger Jang'}, {'name': 'Chi-Chun Lee'}, {'name': 'Hung-Yi Lee'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'webpage: https://emosuperb.github.io/'}, 'link': [{'@href': 'http://arxiv.org/abs/2402.13018v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2402.13018v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2402.12746v1', 'updated': '2024-02-20T06:24:38Z', 'published': '2024-02-20T06:24:38Z', 'title': 'Plugin Speech Enhancement: A Universal Speech Enhancement Framework\\n  Inspired by Dynamic Neural Network', 'summary': 'The expectation to deploy a universal neural network for speech enhancement,\\nwith the aim of improving noise robustness across diverse speech processing\\ntasks, faces challenges due to the existing lack of awareness within static\\nspeech enhancement frameworks regarding the expected speech in downstream\\nmodules. These limitations impede the effectiveness of static speech\\nenhancement approaches in achieving optimal performance for a range of speech\\nprocessing tasks, thereby challenging the notion of universal applicability.\\nThe fundamental issue in achieving universal speech enhancement lies in\\neffectively informing the speech enhancement module about the features of\\ndownstream modules. In this study, we present a novel weighting prediction\\napproach, which explicitly learns the task relationships from downstream\\ntraining information to address the core challenge of universal speech\\nenhancement. We found the role of deciding whether to employ data augmentation\\ntechniques as crucial downstream training information. This decision\\nsignificantly impacts the expected speech and the performance of the speech\\nenhancement module. Moreover, we introduce a novel speech enhancement network,\\nthe Plugin Speech Enhancement (Plugin-SE). The Plugin-SE is a dynamic neural\\nnetwork that includes the speech enhancement module, gate module, and weight\\nprediction module. Experimental results demonstrate that the proposed Plugin-SE\\napproach is competitive or superior to other joint training methods across\\nvarious downstream tasks.', 'author': [{'name': 'Yanan Chen'}, {'name': 'Zihao Cui'}, {'name': 'Yingying Gao'}, {'name': 'Junlan Feng'}, {'name': 'Chao Deng'}, {'name': 'Shilei Zhang'}], 'link': [{'@href': 'http://arxiv.org/abs/2402.12746v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2402.12746v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2402.12660v1', 'updated': '2024-02-20T02:16:24Z', 'published': '2024-02-20T02:16:24Z', 'title': 'SingVisio: Visual Analytics of Diffusion Model for Singing Voice\\n  Conversion', 'summary': \"In this study, we present SingVisio, an interactive visual analysis system\\nthat aims to explain the diffusion model used in singing voice conversion.\\nSingVisio provides a visual display of the generation process in diffusion\\nmodels, showcasing the step-by-step denoising of the noisy spectrum and its\\ntransformation into a clean spectrum that captures the desired singer's timbre.\\nThe system also facilitates side-by-side comparisons of different conditions,\\nsuch as source content, melody, and target timbre, highlighting the impact of\\nthese conditions on the diffusion generation process and resulting conversions.\\nThrough comprehensive evaluations, SingVisio demonstrates its effectiveness in\\nterms of system design, functionality, explainability, and user-friendliness.\\nIt offers users of various backgrounds valuable learning experiences and\\ninsights into the diffusion model for singing voice conversion.\", 'author': [{'name': 'Liumeng Xue'}, {'name': 'Chaoren Wang'}, {'name': 'Mingxuan Wang'}, {'name': 'Xueyao Zhang'}, {'name': 'Jun Han'}, {'name': 'Zhizheng Wu'}], 'link': [{'@href': 'http://arxiv.org/abs/2402.12660v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2402.12660v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.HC', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2402.12658v1', 'updated': '2024-02-20T02:14:45Z', 'published': '2024-02-20T02:14:45Z', 'title': 'Guiding the underwater acoustic target recognition with interpretable\\n  contrastive learning', 'summary': 'Recognizing underwater targets from acoustic signals is a challenging task\\nowing to the intricate ocean environments and variable underwater channels.\\nWhile deep learning-based systems have become the mainstream approach for\\nunderwater acoustic target recognition, they have faced criticism for their\\nlack of interpretability and weak generalization performance in practical\\napplications. In this work, we apply the class activation mapping (CAM) to\\ngenerate visual explanations for the predictions of a spectrogram-based\\nrecognition system. CAM can help to understand the behavior of recognition\\nmodels by highlighting the regions of the input features that contribute the\\nmost to the prediction. Our explorations reveal that recognition models tend to\\nfocus on the low-frequency line spectrum and high-frequency periodic modulation\\ninformation of underwater signals. Based on the observation, we propose an\\ninterpretable contrastive learning (ICL) strategy that employs two encoders to\\nlearn from acoustic features with different emphases (line spectrum and\\nmodulation information). By imposing constraints between encoders, the proposed\\nstrategy can enhance the generalization performance of the recognition system.\\nOur experiments demonstrate that the proposed contrastive learning approach can\\nimprove the recognition accuracy and bring significant improvements across\\nvarious underwater databases.', 'author': [{'name': 'Yuan Xie'}, {'name': 'Jiawei Ren'}, {'name': 'Ji Xu'}], 'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': '10.1109/OCEANSLimerick52467.2023.10244447'}, 'link': [{'@title': 'doi', '@href': 'http://dx.doi.org/10.1109/OCEANSLimerick52467.2023.10244447', '@rel': 'related'}, {'@href': 'http://arxiv.org/abs/2402.12658v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2402.12658v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'OCEANS 2023-Limerick. IEEE, 2023: 1-6'}, 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2402.12654v1', 'updated': '2024-02-20T02:04:38Z', 'published': '2024-02-20T02:04:38Z', 'title': 'OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech\\n  Recognition, Translation, and Language Identification', 'summary': 'There has been an increasing interest in large speech models that can perform\\nmultiple speech processing tasks in a single model. Such models usually adopt\\nthe encoder-decoder or decoder-only architecture due to their popularity and\\ngood performance in many domains. However, autoregressive models can be slower\\nduring inference compared to non-autoregressive models and also have potential\\nrisks of hallucination. Though prior studies observed promising results of\\nnon-autoregressive models for certain tasks at small scales, it remains unclear\\nif they can be scaled to speech-to-text generation in diverse languages and\\ntasks. Inspired by the Open Whisper-style Speech Model (OWSM) project, we\\npropose OWSM-CTC, a novel encoder-only speech foundation model based on\\nConnectionist Temporal Classification (CTC). It is trained on 180k hours of\\npublic audio data for multilingual automatic speech recognition (ASR), speech\\ntranslation (ST), and language identification (LID). Compared to\\nencoder-decoder OWSM, our OWSM-CTC achieves competitive results on ASR and up\\nto 25% relative improvement on ST, while it is more robust and 3 to 4 times\\nfaster for inference. OWSM-CTC also improves the long-form ASR result with 20x\\nspeed-up. We will publicly release our codebase, pre-trained model, and\\ntraining logs to promote open science in speech foundation models.', 'author': [{'name': 'Yifan Peng'}, {'name': 'Yui Sudo'}, {'name': 'Muhammad Shakeel'}, {'name': 'Shinji Watanabe'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': '18 pages, 2 figures'}, 'link': [{'@href': 'http://arxiv.org/abs/2402.12654v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2402.12654v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2402.12482v1', 'updated': '2024-02-19T19:38:37Z', 'published': '2024-02-19T19:38:37Z', 'title': 'SECP: A Speech Enhancement-Based Curation Pipeline For Scalable\\n  Acquisition Of Clean Speech', 'summary': 'As more speech technologies rely on a supervised deep learning approach with\\nclean speech as the ground truth, a methodology to onboard said speech at scale\\nis needed. However, this approach needs to minimize the dependency on human\\nlistening and annotation, only requiring a human-in-the-loop when needed. In\\nthis paper, we address this issue by outlining Speech Enhancement-based\\nCuration Pipeline (SECP) which serves as a framework to onboard clean speech.\\nThis clean speech can then train a speech enhancement model, which can further\\nrefine the original dataset and thus close the iterative loop. By running two\\niterative rounds, we observe that enhanced output used as ground truth does not\\ndegrade model performance according to $\\\\Delta_{PESQ}$, a metric used in this\\npaper. We also show through comparative mean opinion score (CMOS) based\\nsubjective tests that the highest and lowest bound of refined data is\\nperceptually better than the original data.', 'author': [{'name': 'Adam Sabra'}, {'name': 'Cyprian Wronka'}, {'name': 'Michelle Mao'}, {'name': 'Samer Hijazi'}], 'arxiv:comment': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'Accepted to the International Conference on Acoustics, Speech and\\n  Signal Processing (ICASSP) 2024'}, 'link': [{'@href': 'http://arxiv.org/abs/2402.12482v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2402.12482v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.IR', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2402.12423v1', 'updated': '2024-02-19T16:22:21Z', 'published': '2024-02-19T16:22:21Z', 'title': 'On the Semantic Latent Space of Diffusion-Based Text-to-Speech Models', 'summary': \"The incorporation of Denoising Diffusion Models (DDMs) in the Text-to-Speech\\n(TTS) domain is rising, providing great value in synthesizing high quality\\nspeech. Although they exhibit impressive audio quality, the extent of their\\nsemantic capabilities is unknown, and controlling their synthesized speech's\\nvocal properties remains a challenge. Inspired by recent advances in image\\nsynthesis, we explore the latent space of frozen TTS models, which is composed\\nof the latent bottleneck activations of the DDM's denoiser. We identify that\\nthis space contains rich semantic information, and outline several novel\\nmethods for finding semantic directions within it, both supervised and\\nunsupervised. We then demonstrate how these enable off-the-shelf audio editing,\\nwithout any further training, architectural changes or data requirements. We\\npresent evidence of the semantic and acoustic qualities of the edited audio,\\nand provide supplemental samples:\\nhttps://latent-analysis-grad-tts.github.io/speech-samples/.\", 'author': [{'name': 'Miri Varshavsky Hassid'}, {'name': 'Roy Hirsch'}, {'name': 'Regev Cohen'}, {'name': 'Tomer Golany'}, {'name': 'Daniel Freedman'}, {'name': 'Ehud Rivlin'}], 'link': [{'@href': 'http://arxiv.org/abs/2402.12423v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2402.12423v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.CL', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2402.12239v1', 'updated': '2024-02-19T15:50:32Z', 'published': '2024-02-19T15:50:32Z', 'title': 'Significance of Chirp MFCC as a Feature in Speech and Audio Applications', 'summary': 'A novel feature, based on the chirp z-transform, that offers an improved\\nrepresentation of the underlying true spectrum is proposed. This feature, the\\nchirp MFCC, is derived by computing the Mel frequency cepstral coefficients\\nfrom the chirp magnitude spectrum, instead of the Fourier transform magnitude\\nspectrum. The theoretical foundations for the proposal, and the experimental\\nvalidation using product of likelihood Gaussians, to show the improved class\\nseparation offered by the proposed chirp MFCC, when compared with vanilla MFCC\\nare discussed. Further, real world evaluation of the feature is performed using\\nthree diverse tasks, namely, speech-music classification, speaker\\nidentification, and speech commands recognition. It is shown in all three tasks\\nthat the proposed chirp MFCC offers considerable improvements.', 'author': [{'name': 'S. Johanan Joysingh'}, {'name': 'P. Vijayalakshmi'}, {'name': 'T. Nagarajan'}], 'link': [{'@href': 'http://arxiv.org/abs/2402.12239v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2402.12239v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'eess.SP', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'eess.SP', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2402.12208v2', 'updated': '2024-02-20T15:24:30Z', 'published': '2024-02-19T15:12:12Z', 'title': 'Language-Codec: Reducing the Gaps Between Discrete Codec Representation\\n  and Speech Language Models', 'summary': 'In recent years, large language models have achieved significant success in\\ngenerative tasks (e.g., speech cloning and audio generation) related to speech,\\naudio, music, and other signal domains. A crucial element of these models is\\nthe discrete acoustic codecs, which serves as an intermediate representation\\nreplacing the mel-spectrogram. However, there exist several gaps between\\ndiscrete codecs and downstream speech language models. Specifically, 1) most\\ncodec models are trained on only 1,000 hours of data, whereas most speech\\nlanguage models are trained on 60,000 hours; 2) Achieving good reconstruction\\nperformance requires the utilization of numerous codebooks, which increases the\\nburden on downstream speech language models; 3) The initial channel of the\\ncodebooks contains excessive information, making it challenging to directly\\ngenerate acoustic tokens from weakly supervised signals such as text in\\ndownstream tasks. Consequently, leveraging the characteristics of speech\\nlanguage models, we propose Language-Codec. In the Language-Codec, we introduce\\na Mask Channel Residual Vector Quantization (MCRVQ) mechanism along with\\nimproved Fourier transform structures and larger training datasets to address\\nthe aforementioned gaps. We compare our method with competing audio compression\\nalgorithms and observe significant outperformance across extensive evaluations.\\nFurthermore, we also validate the efficiency of the Language-Codec on\\ndownstream speech language models. The source code and pre-trained models can\\nbe accessed at https://github.com/jishengpeng/languagecodec .', 'author': [{'name': 'Shengpeng Ji'}, {'name': 'Minghui Fang'}, {'name': 'Ziyue Jiang'}, {'name': 'Rongjie Huang'}, {'name': 'Jialung Zuo'}, {'name': 'Shulei Wang'}, {'name': 'Zhou Zhao'}], 'link': [{'@href': 'http://arxiv.org/abs/2402.12208v2', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2402.12208v2', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2402.12094v1', 'updated': '2024-02-19T12:20:56Z', 'published': '2024-02-19T12:20:56Z', 'title': 'On the relationship between speech and hearing', 'summary': 'We present a framework for experimentally linking speech production and\\nhearing. Using this approach, we describe experimental results, that lead to\\nthe concept that sounds made by different individuals and perceived to be the\\nsame can be transformed into each other by a \"speech scale\". The speech scale\\nis empirically determined using only speech data. We show the similarity of the\\nspeech scale to the MEL scale of Stevens and Volkmann, which was derived only\\nfrom hearing experiments. We thus experimentally link speech production and\\nhearing.', 'author': [{'name': 'Srinivasan Umesh'}, {'name': 'Leon Cohen'}, {'name': 'Douglas Nelson'}], 'link': [{'@href': 'http://arxiv.org/abs/2402.12094v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2402.12094v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2402.11954v1', 'updated': '2024-02-19T08:49:09Z', 'published': '2024-02-19T08:49:09Z', 'title': 'Multimodal Emotion Recognition from Raw Audio with Sinc-convolution', 'summary': 'Speech Emotion Recognition (SER) is still a complex task for computers with\\naverage recall rates usually about 70% on the most realistic datasets. Most SER\\nsystems use hand-crafted features extracted from audio signal such as energy,\\nzero crossing rate, spectral information, prosodic, mel frequency cepstral\\ncoefficient (MFCC), and so on. More recently, using raw waveform for training\\nneural network is becoming an emerging trend. This approach is advantageous as\\nit eliminates the feature extraction pipeline. Learning from time-domain signal\\nhas shown good results for tasks such as speech recognition, speaker\\nverification etc. In this paper, we utilize Sinc-convolution layer, which is an\\nefficient architecture for preprocessing raw speech waveform for emotion\\nrecognition, to extract acoustic features from raw audio signals followed by a\\nlong short-term memory (LSTM). We also incorporate linguistic features and\\nappend a dialogical emotion decoding (DED) strategy. Our approach achieves a\\nweighted accuracy of 85.1\\\\% in four class emotion on the Interactive Emotional\\nDyadic Motion Capture (IEMOCAP) dataset.', 'author': [{'name': 'Xiaohui Zhang'}, {'name': 'Wenjie Fu'}, {'name': 'Mangui Liang'}], 'link': [{'@href': 'http://arxiv.org/abs/2402.11954v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2402.11954v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.MM', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2402.11931v1', 'updated': '2024-02-19T08:18:52Z', 'published': '2024-02-19T08:18:52Z', 'title': \"Soft-Weighted CrossEntropy Loss for Continous Alzheimer's Disease\\n  Detection\", 'summary': \"Alzheimer's disease is a common cognitive disorder in the elderly. Early and\\naccurate diagnosis of Alzheimer's disease (AD) has a major impact on the\\nprogress of research on dementia. At present, researchers have used machine\\nlearning methods to detect Alzheimer's disease from the speech of participants.\\nHowever, the recognition accuracy of current methods is unsatisfactory, and\\nmost of them focus on using low-dimensional handcrafted features to extract\\nrelevant information from audios. This paper proposes an Alzheimer's disease\\ndetection system based on the pre-trained framework Wav2vec 2.0 (Wav2vec2). In\\naddition, by replacing the loss function with the Soft-Weighted CrossEntropy\\nloss function, we achieved 85.45\\\\% recognition accuracy on the same test\\ndataset.\", 'author': [{'name': 'Xiaohui Zhang'}, {'name': 'Wenjie Fu'}, {'name': 'Mangui Liang'}], 'link': [{'@href': 'http://arxiv.org/abs/2402.11931v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2402.11931v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'q-bio.NC', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2402.11919v1', 'updated': '2024-02-19T08:07:01Z', 'published': '2024-02-19T08:07:01Z', 'title': 'Unraveling Complex Data Diversity in Underwater Acoustic Target\\n  Recognition through Convolution-based Mixture of Experts', 'summary': 'Underwater acoustic target recognition is a difficult task owing to the\\nintricate nature of underwater acoustic signals. The complex underwater\\nenvironments, unpredictable transmission channels, and dynamic motion states\\ngreatly impact the real-world underwater acoustic signals, and may even obscure\\nthe intrinsic characteristics related to targets. Consequently, the data\\ndistribution of underwater acoustic signals exhibits high intra-class\\ndiversity, thereby compromising the accuracy and robustness of recognition\\nsystems.To address these issues, this work proposes a convolution-based mixture\\nof experts (CMoE) that recognizes underwater targets in a fine-grained manner.\\nThe proposed technique introduces multiple expert layers as independent\\nlearners, along with a routing layer that determines the assignment of experts\\naccording to the characteristics of inputs. This design allows the model to\\nutilize independent parameter spaces, facilitating the learning of complex\\nunderwater signals with high intra-class diversity. Furthermore, this work\\noptimizes the CMoE structure by balancing regularization and an optional\\nresidual module. To validate the efficacy of our proposed techniques, we\\nconducted detailed experiments and visualization analyses on three underwater\\nacoustic databases across several acoustic features. The experimental results\\ndemonstrate that our CMoE consistently achieves significant performance\\nimprovements, delivering superior recognition accuracy when compared to\\nexisting advanced methods.', 'author': [{'name': 'Yuan Xie'}, {'name': 'Jiawei Ren'}, {'name': 'Ji Xu'}], 'arxiv:doi': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': '10.1016/j.eswa.2024.123431'}, 'link': [{'@title': 'doi', '@href': 'http://dx.doi.org/10.1016/j.eswa.2024.123431', '@rel': 'related'}, {'@href': 'http://arxiv.org/abs/2402.11919v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2402.11919v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:journal_ref': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '#text': 'Expert Systems with Applications (2024): 123431'}, 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2402.11748v1', 'updated': '2024-02-19T00:21:13Z', 'published': '2024-02-19T00:21:13Z', 'title': 'Low-power SNN-based audio source localisation using a Hilbert Transform\\n  spike encoding scheme', 'summary': 'Sound source localisation is used in many consumer electronics devices, to\\nhelp isolate audio from individual speakers and to reject noise. Localization\\nis frequently accomplished by \"beamforming\" algorithms, which combine\\nmicrophone audio streams to improve received signal power from particular\\nincident source directions. Beamforming algorithms generally use knowledge of\\nthe frequency components of the audio source, along with the known microphone\\narray geometry, to analytically phase-shift microphone streams before combining\\nthem. A dense set of band-pass filters is often used to obtain known-frequency\\n\"narrowband\" components from wide-band audio streams. These approaches achieve\\nhigh accuracy, but state of the art narrowband beamforming algorithms are\\ncomputationally demanding, and are therefore difficult to integrate into\\nlow-power IoT devices. We demonstrate a novel method for sound source\\nlocalisation in arbitrary microphone arrays, designed for efficient\\nimplementation in ultra-low-power spiking neural networks (SNNs). We use a\\nnovel short-time Hilbert transform (STHT) to remove the need for demanding\\nband-pass filtering of audio, and introduce a new accompanying method for audio\\nencoding with spiking events. Our beamforming and localisation approach\\nachieves state-of-the-art accuracy for SNN methods, and comparable with\\ntraditional non-SNN super-resolution approaches. We deploy our method to\\nlow-power SNN audio inference hardware, and achieve much lower power\\nconsumption compared with super-resolution methods. We demonstrate that signal\\nprocessing approaches can be co-designed with spiking neural network\\nimplementations to achieve high levels of power efficiency. Our new\\nHilbert-transform-based method for beamforming promises to also improve the\\nefficiency of traditional DSP-based signal processing.', 'author': [{'name': 'Saeid Haghighatshoar'}, {'name': 'Dylan R Muir'}], 'link': [{'@href': 'http://arxiv.org/abs/2402.11748v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2402.11748v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.NE', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}]}, {'id': 'http://arxiv.org/abs/2402.11747v1', 'updated': '2024-02-19T00:21:07Z', 'published': '2024-02-19T00:21:07Z', 'title': 'Parameter Efficient Finetuning for Speech Emotion Recognition and Domain\\n  Adaptation', 'summary': 'Foundation models have shown superior performance for speech emotion\\nrecognition (SER). However, given the limited data in emotion corpora,\\nfinetuning all parameters of large pre-trained models for SER can be both\\nresource-intensive and susceptible to overfitting. This paper investigates\\nparameter-efficient finetuning (PEFT) for SER. Various PEFT adaptors are\\nsystematically studied for both classification of discrete emotion categories\\nand prediction of dimensional emotional attributes. The results demonstrate\\nthat the combination of PEFT methods surpasses full finetuning with a\\nsignificant reduction in the number of trainable parameters. Furthermore, a\\ntwo-stage adaptation strategy is proposed to adapt models trained on acted\\nemotion data, which is more readily available, to make the model more adept at\\ncapturing natural emotional expressions. Both intra- and cross-corpus\\nexperiments validate the efficacy of the proposed approach in enhancing the\\nperformance on both the source and target domains.', 'author': [{'name': 'Nineli Lashkarashvili'}, {'name': 'Wen Wu'}, {'name': 'Guangzhi Sun'}, {'name': 'Philip C. Woodland'}], 'link': [{'@href': 'http://arxiv.org/abs/2402.11747v1', '@rel': 'alternate', '@type': 'text/html'}, {'@title': 'pdf', '@href': 'http://arxiv.org/pdf/2402.11747v1', '@rel': 'related', '@type': 'application/pdf'}], 'arxiv:primary_category': {'@xmlns:arxiv': 'http://arxiv.org/schemas/atom', '@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}, 'category': [{'@term': 'eess.AS', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.LG', '@scheme': 'http://arxiv.org/schemas/atom'}, {'@term': 'cs.SD', '@scheme': 'http://arxiv.org/schemas/atom'}]}]\n"
     ]
    }
   ],
   "source": [
    "# get entries from the past 4 days\n",
    "import datetime\n",
    "today = datetime.date.today()\n",
    "entries = []\n",
    "for entry in dict_data['feed']['entry']:\n",
    "    date = datetime.datetime.strptime(entry['published'], '%Y-%m-%dT%H:%M:%SZ').date()\n",
    "    if (today - date).days <= 4: # <= to be inclusive in case we miss something posted late on the 4th day\n",
    "        entries.append(entry)\n",
    "print(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Towards audio language modeling - an overview: 2024-02-20T18:50:25Z\n",
      "Neural audio codecs are initially introduced to compress audio data into\n",
      "compact codes to reduce transmission latency. Researchers recently discovered\n",
      "the potential of codecs as suitable tokenizers for converting continuous audio\n",
      "into discrete codes, which can be employed to develop audio language models\n",
      "(LMs). Numerous high-performance neural audio codecs and codec-based LMs have\n",
      "been developed. The paper aims to provide a thorough and systematic overview of\n",
      "the neural audio codec models and codec-based LMs.\n",
      "----\n",
      "HiRIS: an Airborne Sonar Sensor with a 1024 Channel Microphone Array for\n",
      "  In-Air Acoustic Imaging: 2024-02-20T16:03:02Z\n",
      "Airborne 3D imaging using ultrasound is a promising sensing modality for\n",
      "robotic applications in harsh environments. Over the last decade, several\n",
      "high-performance systems have been proposed in the literature. Most of these\n",
      "sensors use a reduced aperture microphone array, leading to artifacts in the\n",
      "resulting acoustic images. This paper presents a novel in-air ultrasound sensor\n",
      "that incorporates 1024 microphones, in a 32-by- 32 uniform rectangular array,\n",
      "in combination with a distributed embedded hardware design to perform the data\n",
      "acquisition. Using a broadband Minimum Variance Distortionless Response (MVDR)\n",
      "beamformer with Forward-Backward Spatial Smoothing (FB-SS), the sensor is able\n",
      "to create both 2D and 3D ultrasound images of the full-frontal hemisphere with\n",
      "high angular accuracy with up to 70dB main lobe to side lobe ratio. This paper\n",
      "describes both the hardware infrastructure needed to obtain such highly\n",
      "detailed acoustical images, as well as the signal processing chain needed to\n",
      "convert the raw acoustic data into said images. Utilizing this novel\n",
      "high-resolution ultrasound imaging sensor, we wish to investigate the limits of\n",
      "both passive and active airborne ultrasound sensing by utilizing this virtually\n",
      "artifact-free imaging modality.\n",
      "----\n",
      "Not All Weights Are Created Equal: Enhancing Energy Efficiency in\n",
      "  On-Device Streaming Speech Recognition: 2024-02-20T15:22:25Z\n",
      "Power consumption plays an important role in on-device streaming speech\n",
      "recognition, as it has a direct impact on the user experience. This study\n",
      "delves into how weight parameters in speech recognition models influence the\n",
      "overall power consumption of these models. We discovered that the impact of\n",
      "weight parameters on power consumption varies, influenced by factors including\n",
      "how often they are invoked and their placement in memory. Armed with this\n",
      "insight, we developed design guidelines aimed at optimizing on-device speech\n",
      "recognition models. These guidelines focus on minimizing power use without\n",
      "substantially affecting accuracy. Our method, which employs targeted\n",
      "compression based on the varying sensitivities of weight parameters,\n",
      "demonstrates superior performance compared to state-of-the-art compression\n",
      "methods. It achieves a reduction in energy usage of up to 47% while maintaining\n",
      "similar model accuracy and improving the real-time factor.\n",
      "----\n",
      "Codec-SUPERB: An In-Depth Analysis of Sound Codec Models: 2024-02-20T15:13:38Z\n",
      "The sound codec's dual roles in minimizing data transmission latency and\n",
      "serving as tokenizers underscore its critical importance. Recent years have\n",
      "witnessed significant developments in codec models. The ideal sound codec\n",
      "should preserve content, paralinguistics, speakers, and audio information.\n",
      "However, the question of which codec achieves optimal sound information\n",
      "preservation remains unanswered, as in different papers, models are evaluated\n",
      "on their selected experimental settings. This study introduces Codec-SUPERB, an\n",
      "acronym for Codec sound processing Universal PERformance Benchmark. It is an\n",
      "ecosystem designed to assess codec models across representative sound\n",
      "applications and signal-level metrics rooted in sound domain\n",
      "knowledge.Codec-SUPERB simplifies result sharing through an online leaderboard,\n",
      "promoting collaboration within a community-driven benchmark database, thereby\n",
      "stimulating new development cycles for codecs. Furthermore, we undertake an\n",
      "in-depth analysis to offer insights into codec models from both application and\n",
      "signal perspectives, diverging from previous codec papers mainly concentrating\n",
      "on signal-level comparisons. Finally, we will release codes, the leaderboard,\n",
      "and data to accelerate progress within the community.\n",
      "----\n",
      "EMO-SUPERB: An In-depth Look at Speech Emotion Recognition: 2024-02-20T14:00:53Z\n",
      "Speech emotion recognition (SER) is a pivotal technology for human-computer\n",
      "interaction systems. However, 80.77% of SER papers yield results that cannot be\n",
      "reproduced. We develop EMO-SUPERB, short for EMOtion Speech Universal\n",
      "PERformance Benchmark, which aims to enhance open-source initiatives for SER.\n",
      "EMO-SUPERB includes a user-friendly codebase to leverage 15 state-of-the-art\n",
      "speech self-supervised learning models (SSLMs) for exhaustive evaluation across\n",
      "six open-source SER datasets. EMO-SUPERB streamlines result sharing via an\n",
      "online leaderboard, fostering collaboration within a community-driven benchmark\n",
      "and thereby enhancing the development of SER. On average, 2.58% of annotations\n",
      "are annotated using natural language. SER relies on classification models and\n",
      "is unable to process natural languages, leading to the discarding of these\n",
      "valuable annotations. We prompt ChatGPT to mimic annotators, comprehend natural\n",
      "language annotations, and subsequently re-label the data. By utilizing labels\n",
      "generated by ChatGPT, we consistently achieve an average relative gain of 3.08%\n",
      "across all settings.\n",
      "----\n",
      "Plugin Speech Enhancement: A Universal Speech Enhancement Framework\n",
      "  Inspired by Dynamic Neural Network: 2024-02-20T06:24:38Z\n",
      "The expectation to deploy a universal neural network for speech enhancement,\n",
      "with the aim of improving noise robustness across diverse speech processing\n",
      "tasks, faces challenges due to the existing lack of awareness within static\n",
      "speech enhancement frameworks regarding the expected speech in downstream\n",
      "modules. These limitations impede the effectiveness of static speech\n",
      "enhancement approaches in achieving optimal performance for a range of speech\n",
      "processing tasks, thereby challenging the notion of universal applicability.\n",
      "The fundamental issue in achieving universal speech enhancement lies in\n",
      "effectively informing the speech enhancement module about the features of\n",
      "downstream modules. In this study, we present a novel weighting prediction\n",
      "approach, which explicitly learns the task relationships from downstream\n",
      "training information to address the core challenge of universal speech\n",
      "enhancement. We found the role of deciding whether to employ data augmentation\n",
      "techniques as crucial downstream training information. This decision\n",
      "significantly impacts the expected speech and the performance of the speech\n",
      "enhancement module. Moreover, we introduce a novel speech enhancement network,\n",
      "the Plugin Speech Enhancement (Plugin-SE). The Plugin-SE is a dynamic neural\n",
      "network that includes the speech enhancement module, gate module, and weight\n",
      "prediction module. Experimental results demonstrate that the proposed Plugin-SE\n",
      "approach is competitive or superior to other joint training methods across\n",
      "various downstream tasks.\n",
      "----\n",
      "SingVisio: Visual Analytics of Diffusion Model for Singing Voice\n",
      "  Conversion: 2024-02-20T02:16:24Z\n",
      "In this study, we present SingVisio, an interactive visual analysis system\n",
      "that aims to explain the diffusion model used in singing voice conversion.\n",
      "SingVisio provides a visual display of the generation process in diffusion\n",
      "models, showcasing the step-by-step denoising of the noisy spectrum and its\n",
      "transformation into a clean spectrum that captures the desired singer's timbre.\n",
      "The system also facilitates side-by-side comparisons of different conditions,\n",
      "such as source content, melody, and target timbre, highlighting the impact of\n",
      "these conditions on the diffusion generation process and resulting conversions.\n",
      "Through comprehensive evaluations, SingVisio demonstrates its effectiveness in\n",
      "terms of system design, functionality, explainability, and user-friendliness.\n",
      "It offers users of various backgrounds valuable learning experiences and\n",
      "insights into the diffusion model for singing voice conversion.\n",
      "----\n",
      "Guiding the underwater acoustic target recognition with interpretable\n",
      "  contrastive learning: 2024-02-20T02:14:45Z\n",
      "Recognizing underwater targets from acoustic signals is a challenging task\n",
      "owing to the intricate ocean environments and variable underwater channels.\n",
      "While deep learning-based systems have become the mainstream approach for\n",
      "underwater acoustic target recognition, they have faced criticism for their\n",
      "lack of interpretability and weak generalization performance in practical\n",
      "applications. In this work, we apply the class activation mapping (CAM) to\n",
      "generate visual explanations for the predictions of a spectrogram-based\n",
      "recognition system. CAM can help to understand the behavior of recognition\n",
      "models by highlighting the regions of the input features that contribute the\n",
      "most to the prediction. Our explorations reveal that recognition models tend to\n",
      "focus on the low-frequency line spectrum and high-frequency periodic modulation\n",
      "information of underwater signals. Based on the observation, we propose an\n",
      "interpretable contrastive learning (ICL) strategy that employs two encoders to\n",
      "learn from acoustic features with different emphases (line spectrum and\n",
      "modulation information). By imposing constraints between encoders, the proposed\n",
      "strategy can enhance the generalization performance of the recognition system.\n",
      "Our experiments demonstrate that the proposed contrastive learning approach can\n",
      "improve the recognition accuracy and bring significant improvements across\n",
      "various underwater databases.\n",
      "----\n",
      "OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech\n",
      "  Recognition, Translation, and Language Identification: 2024-02-20T02:04:38Z\n",
      "There has been an increasing interest in large speech models that can perform\n",
      "multiple speech processing tasks in a single model. Such models usually adopt\n",
      "the encoder-decoder or decoder-only architecture due to their popularity and\n",
      "good performance in many domains. However, autoregressive models can be slower\n",
      "during inference compared to non-autoregressive models and also have potential\n",
      "risks of hallucination. Though prior studies observed promising results of\n",
      "non-autoregressive models for certain tasks at small scales, it remains unclear\n",
      "if they can be scaled to speech-to-text generation in diverse languages and\n",
      "tasks. Inspired by the Open Whisper-style Speech Model (OWSM) project, we\n",
      "propose OWSM-CTC, a novel encoder-only speech foundation model based on\n",
      "Connectionist Temporal Classification (CTC). It is trained on 180k hours of\n",
      "public audio data for multilingual automatic speech recognition (ASR), speech\n",
      "translation (ST), and language identification (LID). Compared to\n",
      "encoder-decoder OWSM, our OWSM-CTC achieves competitive results on ASR and up\n",
      "to 25% relative improvement on ST, while it is more robust and 3 to 4 times\n",
      "faster for inference. OWSM-CTC also improves the long-form ASR result with 20x\n",
      "speed-up. We will publicly release our codebase, pre-trained model, and\n",
      "training logs to promote open science in speech foundation models.\n",
      "----\n",
      "SECP: A Speech Enhancement-Based Curation Pipeline For Scalable\n",
      "  Acquisition Of Clean Speech: 2024-02-19T19:38:37Z\n",
      "As more speech technologies rely on a supervised deep learning approach with\n",
      "clean speech as the ground truth, a methodology to onboard said speech at scale\n",
      "is needed. However, this approach needs to minimize the dependency on human\n",
      "listening and annotation, only requiring a human-in-the-loop when needed. In\n",
      "this paper, we address this issue by outlining Speech Enhancement-based\n",
      "Curation Pipeline (SECP) which serves as a framework to onboard clean speech.\n",
      "This clean speech can then train a speech enhancement model, which can further\n",
      "refine the original dataset and thus close the iterative loop. By running two\n",
      "iterative rounds, we observe that enhanced output used as ground truth does not\n",
      "degrade model performance according to $\\Delta_{PESQ}$, a metric used in this\n",
      "paper. We also show through comparative mean opinion score (CMOS) based\n",
      "subjective tests that the highest and lowest bound of refined data is\n",
      "perceptually better than the original data.\n",
      "----\n",
      "On the Semantic Latent Space of Diffusion-Based Text-to-Speech Models: 2024-02-19T16:22:21Z\n",
      "The incorporation of Denoising Diffusion Models (DDMs) in the Text-to-Speech\n",
      "(TTS) domain is rising, providing great value in synthesizing high quality\n",
      "speech. Although they exhibit impressive audio quality, the extent of their\n",
      "semantic capabilities is unknown, and controlling their synthesized speech's\n",
      "vocal properties remains a challenge. Inspired by recent advances in image\n",
      "synthesis, we explore the latent space of frozen TTS models, which is composed\n",
      "of the latent bottleneck activations of the DDM's denoiser. We identify that\n",
      "this space contains rich semantic information, and outline several novel\n",
      "methods for finding semantic directions within it, both supervised and\n",
      "unsupervised. We then demonstrate how these enable off-the-shelf audio editing,\n",
      "without any further training, architectural changes or data requirements. We\n",
      "present evidence of the semantic and acoustic qualities of the edited audio,\n",
      "and provide supplemental samples:\n",
      "https://latent-analysis-grad-tts.github.io/speech-samples/.\n",
      "----\n",
      "Significance of Chirp MFCC as a Feature in Speech and Audio Applications: 2024-02-19T15:50:32Z\n",
      "A novel feature, based on the chirp z-transform, that offers an improved\n",
      "representation of the underlying true spectrum is proposed. This feature, the\n",
      "chirp MFCC, is derived by computing the Mel frequency cepstral coefficients\n",
      "from the chirp magnitude spectrum, instead of the Fourier transform magnitude\n",
      "spectrum. The theoretical foundations for the proposal, and the experimental\n",
      "validation using product of likelihood Gaussians, to show the improved class\n",
      "separation offered by the proposed chirp MFCC, when compared with vanilla MFCC\n",
      "are discussed. Further, real world evaluation of the feature is performed using\n",
      "three diverse tasks, namely, speech-music classification, speaker\n",
      "identification, and speech commands recognition. It is shown in all three tasks\n",
      "that the proposed chirp MFCC offers considerable improvements.\n",
      "----\n",
      "Language-Codec: Reducing the Gaps Between Discrete Codec Representation\n",
      "  and Speech Language Models: 2024-02-19T15:12:12Z\n",
      "In recent years, large language models have achieved significant success in\n",
      "generative tasks (e.g., speech cloning and audio generation) related to speech,\n",
      "audio, music, and other signal domains. A crucial element of these models is\n",
      "the discrete acoustic codecs, which serves as an intermediate representation\n",
      "replacing the mel-spectrogram. However, there exist several gaps between\n",
      "discrete codecs and downstream speech language models. Specifically, 1) most\n",
      "codec models are trained on only 1,000 hours of data, whereas most speech\n",
      "language models are trained on 60,000 hours; 2) Achieving good reconstruction\n",
      "performance requires the utilization of numerous codebooks, which increases the\n",
      "burden on downstream speech language models; 3) The initial channel of the\n",
      "codebooks contains excessive information, making it challenging to directly\n",
      "generate acoustic tokens from weakly supervised signals such as text in\n",
      "downstream tasks. Consequently, leveraging the characteristics of speech\n",
      "language models, we propose Language-Codec. In the Language-Codec, we introduce\n",
      "a Mask Channel Residual Vector Quantization (MCRVQ) mechanism along with\n",
      "improved Fourier transform structures and larger training datasets to address\n",
      "the aforementioned gaps. We compare our method with competing audio compression\n",
      "algorithms and observe significant outperformance across extensive evaluations.\n",
      "Furthermore, we also validate the efficiency of the Language-Codec on\n",
      "downstream speech language models. The source code and pre-trained models can\n",
      "be accessed at https://github.com/jishengpeng/languagecodec .\n",
      "----\n",
      "On the relationship between speech and hearing: 2024-02-19T12:20:56Z\n",
      "We present a framework for experimentally linking speech production and\n",
      "hearing. Using this approach, we describe experimental results, that lead to\n",
      "the concept that sounds made by different individuals and perceived to be the\n",
      "same can be transformed into each other by a \"speech scale\". The speech scale\n",
      "is empirically determined using only speech data. We show the similarity of the\n",
      "speech scale to the MEL scale of Stevens and Volkmann, which was derived only\n",
      "from hearing experiments. We thus experimentally link speech production and\n",
      "hearing.\n",
      "----\n",
      "Multimodal Emotion Recognition from Raw Audio with Sinc-convolution: 2024-02-19T08:49:09Z\n",
      "Speech Emotion Recognition (SER) is still a complex task for computers with\n",
      "average recall rates usually about 70% on the most realistic datasets. Most SER\n",
      "systems use hand-crafted features extracted from audio signal such as energy,\n",
      "zero crossing rate, spectral information, prosodic, mel frequency cepstral\n",
      "coefficient (MFCC), and so on. More recently, using raw waveform for training\n",
      "neural network is becoming an emerging trend. This approach is advantageous as\n",
      "it eliminates the feature extraction pipeline. Learning from time-domain signal\n",
      "has shown good results for tasks such as speech recognition, speaker\n",
      "verification etc. In this paper, we utilize Sinc-convolution layer, which is an\n",
      "efficient architecture for preprocessing raw speech waveform for emotion\n",
      "recognition, to extract acoustic features from raw audio signals followed by a\n",
      "long short-term memory (LSTM). We also incorporate linguistic features and\n",
      "append a dialogical emotion decoding (DED) strategy. Our approach achieves a\n",
      "weighted accuracy of 85.1\\% in four class emotion on the Interactive Emotional\n",
      "Dyadic Motion Capture (IEMOCAP) dataset.\n",
      "----\n",
      "Soft-Weighted CrossEntropy Loss for Continous Alzheimer's Disease\n",
      "  Detection: 2024-02-19T08:18:52Z\n",
      "Alzheimer's disease is a common cognitive disorder in the elderly. Early and\n",
      "accurate diagnosis of Alzheimer's disease (AD) has a major impact on the\n",
      "progress of research on dementia. At present, researchers have used machine\n",
      "learning methods to detect Alzheimer's disease from the speech of participants.\n",
      "However, the recognition accuracy of current methods is unsatisfactory, and\n",
      "most of them focus on using low-dimensional handcrafted features to extract\n",
      "relevant information from audios. This paper proposes an Alzheimer's disease\n",
      "detection system based on the pre-trained framework Wav2vec 2.0 (Wav2vec2). In\n",
      "addition, by replacing the loss function with the Soft-Weighted CrossEntropy\n",
      "loss function, we achieved 85.45\\% recognition accuracy on the same test\n",
      "dataset.\n",
      "----\n",
      "Unraveling Complex Data Diversity in Underwater Acoustic Target\n",
      "  Recognition through Convolution-based Mixture of Experts: 2024-02-19T08:07:01Z\n",
      "Underwater acoustic target recognition is a difficult task owing to the\n",
      "intricate nature of underwater acoustic signals. The complex underwater\n",
      "environments, unpredictable transmission channels, and dynamic motion states\n",
      "greatly impact the real-world underwater acoustic signals, and may even obscure\n",
      "the intrinsic characteristics related to targets. Consequently, the data\n",
      "distribution of underwater acoustic signals exhibits high intra-class\n",
      "diversity, thereby compromising the accuracy and robustness of recognition\n",
      "systems.To address these issues, this work proposes a convolution-based mixture\n",
      "of experts (CMoE) that recognizes underwater targets in a fine-grained manner.\n",
      "The proposed technique introduces multiple expert layers as independent\n",
      "learners, along with a routing layer that determines the assignment of experts\n",
      "according to the characteristics of inputs. This design allows the model to\n",
      "utilize independent parameter spaces, facilitating the learning of complex\n",
      "underwater signals with high intra-class diversity. Furthermore, this work\n",
      "optimizes the CMoE structure by balancing regularization and an optional\n",
      "residual module. To validate the efficacy of our proposed techniques, we\n",
      "conducted detailed experiments and visualization analyses on three underwater\n",
      "acoustic databases across several acoustic features. The experimental results\n",
      "demonstrate that our CMoE consistently achieves significant performance\n",
      "improvements, delivering superior recognition accuracy when compared to\n",
      "existing advanced methods.\n",
      "----\n",
      "Low-power SNN-based audio source localisation using a Hilbert Transform\n",
      "  spike encoding scheme: 2024-02-19T00:21:13Z\n",
      "Sound source localisation is used in many consumer electronics devices, to\n",
      "help isolate audio from individual speakers and to reject noise. Localization\n",
      "is frequently accomplished by \"beamforming\" algorithms, which combine\n",
      "microphone audio streams to improve received signal power from particular\n",
      "incident source directions. Beamforming algorithms generally use knowledge of\n",
      "the frequency components of the audio source, along with the known microphone\n",
      "array geometry, to analytically phase-shift microphone streams before combining\n",
      "them. A dense set of band-pass filters is often used to obtain known-frequency\n",
      "\"narrowband\" components from wide-band audio streams. These approaches achieve\n",
      "high accuracy, but state of the art narrowband beamforming algorithms are\n",
      "computationally demanding, and are therefore difficult to integrate into\n",
      "low-power IoT devices. We demonstrate a novel method for sound source\n",
      "localisation in arbitrary microphone arrays, designed for efficient\n",
      "implementation in ultra-low-power spiking neural networks (SNNs). We use a\n",
      "novel short-time Hilbert transform (STHT) to remove the need for demanding\n",
      "band-pass filtering of audio, and introduce a new accompanying method for audio\n",
      "encoding with spiking events. Our beamforming and localisation approach\n",
      "achieves state-of-the-art accuracy for SNN methods, and comparable with\n",
      "traditional non-SNN super-resolution approaches. We deploy our method to\n",
      "low-power SNN audio inference hardware, and achieve much lower power\n",
      "consumption compared with super-resolution methods. We demonstrate that signal\n",
      "processing approaches can be co-designed with spiking neural network\n",
      "implementations to achieve high levels of power efficiency. Our new\n",
      "Hilbert-transform-based method for beamforming promises to also improve the\n",
      "efficiency of traditional DSP-based signal processing.\n",
      "----\n",
      "Parameter Efficient Finetuning for Speech Emotion Recognition and Domain\n",
      "  Adaptation: 2024-02-19T00:21:07Z\n",
      "Foundation models have shown superior performance for speech emotion\n",
      "recognition (SER). However, given the limited data in emotion corpora,\n",
      "finetuning all parameters of large pre-trained models for SER can be both\n",
      "resource-intensive and susceptible to overfitting. This paper investigates\n",
      "parameter-efficient finetuning (PEFT) for SER. Various PEFT adaptors are\n",
      "systematically studied for both classification of discrete emotion categories\n",
      "and prediction of dimensional emotional attributes. The results demonstrate\n",
      "that the combination of PEFT methods surpasses full finetuning with a\n",
      "significant reduction in the number of trainable parameters. Furthermore, a\n",
      "two-stage adaptation strategy is proposed to adapt models trained on acted\n",
      "emotion data, which is more readily available, to make the model more adept at\n",
      "capturing natural emotional expressions. Both intra- and cross-corpus\n",
      "experiments validate the efficacy of the proposed approach in enhancing the\n",
      "performance on both the source and target domains.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for e in entries:\n",
    "    print(e['title'] + ': ' + e['published'])\n",
    "    print(e['summary']) \n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural audio codecs are initially introduced to compress audio data into\n",
      "compact codes to reduce transmission latency. Researchers recently discovered\n",
      "the potential of codecs as suitable tokenizers for converting continuous audio\n",
      "into discrete codes, which can be employed to develop audio language models\n",
      "(LMs). Numerous high-performance neural audio codecs and codec-based LMs have\n",
      "been developed. The paper aims to provide a thorough and systematic overview of\n",
      "the neural audio codec models and codec-based LMs.\n",
      "----\n",
      " This article discusses how neural audio codecs, initially designed for compressing audio data, are now being explored as effective tokenizers for creating discrete codes for developing audio language models, with several high-performance neural audio codecs and codec-based models having been developed, and the paper aims to provide an exhaustive review of these models.\n"
     ]
    }
   ],
   "source": [
    "# use ollama / mistral to get a summary of the papers\n",
    "# https://ollama.com/\n",
    "# https://github.com/ollama/ollama\n",
    "# https://github.com/ollama/ollama-python\n",
    "# https://wandb.ai/byyoung3/ml-news/reports/How-to-Run-Mistral-7B-on-an-M1-Mac-With-Ollama--Vmlldzo2MTg4MjA0\n",
    "\n",
    "\n",
    "import ollama\n",
    "\n",
    "response = ollama.chat(model='mistral', messages=[\n",
    "    {\n",
    "        'role' : 'user',\n",
    "        'content' : 'Please write one sentence summary of the following article description' + entries[0]['summary'] ,\n",
    "    }\n",
    "])\n",
    "\n",
    "print(entries[0]['summary'])\n",
    "print('----')\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Towards audio language modeling - an overview\n",
      " Researchers have discovered that neural audio codecs, initially developed for compressing audio data, can also function as effective tokenizers for creating discrete codes for audio language models, leading to the development of numerous high-performance models. (The paper aims to provide a comprehensive overview of these neural audio codec models and codec-based language models.)\n",
      "http://arxiv.org/abs/2402.13236v1\n",
      "----\n",
      "HiRIS: an Airborne Sonar Sensor with a 1024 Channel Microphone Array for  In-Air Acoustic Imaging\n",
      " This paper introduces a high-performance in-air ultrasound sensor with 1024 microphones in a uniform rectangular array, coupled with distributed embedded hardware and advanced signal processing techniques to create artifact-free 2D and 3D ultrasound images in harsh environments.\n",
      "http://arxiv.org/abs/2402.13110v1\n",
      "----\n",
      "Not All Weights Are Created Equal: Enhancing Energy Efficiency in  On-Device Streaming Speech Recognition\n",
      " This study explores how weight parameters in speech recognition models impact power consumption, discovering that their influence varies based on frequency of use and memory placement. The findings result in design guidelines for optimizing on-device speech recognition models, achieving up to 47% energy reduction while maintaining similar accuracy and improving real-time performance.\n",
      "http://arxiv.org/abs/2402.13076v1\n",
      "----\n",
      "Codec-SUPERB: An In-Depth Analysis of Sound Codec Models\n",
      " This article introduces Codec-SUPERB, an ecosystem for assessing sound codec models across various applications and metrics, promoting collaboration and new development cycles in the field.\n",
      "http://arxiv.org/abs/2402.13071v1\n",
      "----\n",
      "EMO-SUPERB: An In-depth Look at Speech Emotion Recognition\n",
      " EMO-SUPERB, a new benchmark for speech emotion recognition, addresses the issue of unreproducible results and lack of natural language processing by incorporating user-friendly codebase for 15 SSLMs, an online leaderboard for result sharing, and utilizing ChatGPT to comprehend and re-label natural language annotations, resulting in an average gain of 3.08% across all settings.\n",
      "http://arxiv.org/abs/2402.13018v1\n",
      "----\n",
      "Plugin Speech Enhancement: A Universal Speech Enhancement Framework  Inspired by Dynamic Neural Network\n",
      " This study proposes a novel weighting prediction approach for universal speech enhancement by learning task relationships from downstream training information, addressing the challenge of effectively informing speech enhancement modules about expected features in downstream modules, and introduces the Plugin Speech Enhancement network, which outperforms other joint training methods.\n",
      "http://arxiv.org/abs/2402.12746v1\n",
      "----\n",
      "SingVisio: Visual Analytics of Diffusion Model for Singing Voice  Conversion\n",
      " This article introduces SingVisio, an interactive visual analysis system that explains the diffusion model used in singing voice conversion through a step-by-step denoising process and side-by-side comparisons, showcasing its effectiveness in design, functionality, explainability, and user-friendliness.\n",
      "http://arxiv.org/abs/2402.12660v1\n",
      "----\n",
      "Guiding the underwater acoustic target recognition with interpretable  contrastive learning\n",
      " This study applies class activation mapping to generate visual explanations for a spectrogram-based underwater acoustic target recognition system, revealing focus on low-frequency lines and high-frequency periodic modulation, and proposes an interpretable contrastive learning strategy to enhance generalization performance, leading to improved recognition accuracy across various underwater databases.\n",
      "http://dx.doi.org/10.1109/OCEANSLimerick52467.2023.10244447\n",
      "----\n",
      "OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech  Recognition, Translation, and Language Identification\n",
      " A new encoder-only speech foundation model called OWSM-CTC is proposed, which is based on Connectionist Temporal Classification (CTC) and trained on 180k hours of public audio data for multilingual automatic speech recognition (ASR), speech translation (ST), and language identification (LID). Compared to encoder-decoder OWSM, OWSM-CTC achieves competitive results on ASR, up to 25% relative improvement on ST, is more robust and faster for inference, and improves long-form ASR result with 20x speed-up. The codebase, pre-trained model, and training logs will be publicly released.\n",
      "http://arxiv.org/abs/2402.12654v1\n",
      "----\n",
      "SECP: A Speech Enhancement-Based Curation Pipeline For Scalable  Acquisition Of Clean Speech\n",
      " This paper introduces Speech Enhancement-based Curation Pipeline (SECP), a framework to onboard clean speech at scale for deep learning speech technologies, minimizing human dependence through iterative enhancement and validation using PEASQ and CMOS metrics.\n",
      "http://arxiv.org/abs/2402.12482v1\n",
      "----\n",
      "On the Semantic Latent Space of Diffusion-Based Text-to-Speech Models\n",
      " The exploration of Denoising Diffusion Models' latent space in Text-to-Speech reveals rich semantic information for off-the-shelf audio editing, enhancing semantic capabilities and vocal control without additional training or resources.\n",
      "http://arxiv.org/abs/2402.12423v1\n",
      "----\n",
      "Significance of Chirp MFCC as a Feature in Speech and Audio Applications\n",
      " A new method using the chirp z-transform for extracting Mel frequency cepstral coefficients (chirp MFCC) is presented, resulting in improved class separation and better performance in speech-music classification, speaker identification, and speech commands recognition compared to traditional MFCC features.\n",
      "http://arxiv.org/abs/2402.12239v1\n",
      "----\n",
      "Language-Codec: Reducing the Gaps Between Discrete Codec Representation  and Speech Language Models\n",
      " A new approach called Language-Codec with Mask Channel Residual Vector Quantization (MCRVQ) and improved Fourier transform structures is proposed to address gaps between discrete acoustic codecs and downstream speech language models, achieving significant outperformance in extensive evaluations.\n",
      "http://arxiv.org/abs/2402.12208v2\n",
      "----\n",
      "On the relationship between speech and hearing\n",
      " A new framework connects the transformation of speech sounds between individuals into a \"speech scale,\" derived solely from speech data, revealing similarities to the MEL scale from hearing experiments, thereby experimentally linking speech production and hearing.\n",
      "http://arxiv.org/abs/2402.12094v1\n",
      "----\n",
      "Multimodal Emotion Recognition from Raw Audio with Sinc-convolution\n",
      " This paper proposes using raw speech waveforms with a Sinc-convolution layer and LSTM for emotion recognition, achieving a high weighted accuracy of 85.1% on the IEMOCAP dataset, while eliminating the need for hand-crafted feature extraction.\n",
      "http://arxiv.org/abs/2402.11954v1\n",
      "----\n",
      "Soft-Weighted CrossEntropy Loss for Continous Alzheimer's Disease  Detection\n",
      " This paper proposes a new system using Wav2vec 2.0 and Soft-Weighted CrossEntropy loss function for improved accuracy in detecting Alzheimer's disease from speech recordings, outperforming current methods with an accuracy of 85.45%.\n",
      "http://arxiv.org/abs/2402.11931v1\n",
      "----\n",
      "Unraveling Complex Data Diversity in Underwater Acoustic Target  Recognition through Convolution-based Mixture of Experts\n",
      " This article proposes a convolution-based mixture of experts (CMoE) model to recognize underwater targets by introducing multiple expert layers and a routing layer for fine-grained learning, achieving significant performance improvements over existing advanced methods despite the high intra-class diversity and complexities in underwater acoustic signals.\n",
      "http://dx.doi.org/10.1016/j.eswa.2024.123431\n",
      "----\n",
      "Low-power SNN-based audio source localisation using a Hilbert Transform  spike encoding scheme\n",
      " This article describes a novel method for sound source localization in microphone arrays using short-time Hilbert transform and spiking neural networks, achieving state-of-the-art accuracy while being computationally efficient and much less power-consuming than traditional narrowband beamforming algorithms.\n",
      "http://arxiv.org/abs/2402.11748v1\n",
      "----\n",
      "Parameter Efficient Finetuning for Speech Emotion Recognition and Domain  Adaptation\n",
      " This paper explores parameter-efficient finetuning techniques for speech emotion recognition, demonstrating that their combination outperforms full finetuning while reducing trainable parameters, and proposes a two-stage adaptation strategy to improve models' ability to capture natural emotional expressions using acted data.\n",
      "http://arxiv.org/abs/2402.11747v1\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# use ollama / mistral to get a summary of all the papers in entries\n",
    "# trying with generate / prompt instead of chat\n",
    "class Summary:\n",
    "    def __init__(self, title, summary, generated_summary_sentence, link):\n",
    "        self.title = title\n",
    "        self.summary = summary\n",
    "        self.generated_summary_sentence = generated_summary_sentence\n",
    "        self.link = link\n",
    "\n",
    "summaries = []\n",
    "\n",
    "for e in entries:\n",
    "    response = ollama.generate(model='mistral', \n",
    "                           prompt='Please generate a one sentence summary of the following article description: ' + e['summary'] ,\n",
    "            )\n",
    "    \n",
    "    s = Summary(\n",
    "        title = e['title'].replace('\\n', ''), # remove newlines from titles\n",
    "        summary = e['summary'],\n",
    "        generated_summary_sentence = response['response'],\n",
    "        link = e['link'][0]['@href']\n",
    "    )    \n",
    "    summaries.append(s)\n",
    "\n",
    "for s in summaries:\n",
    "    print(s.title)\n",
    "    print(s.generated_summary_sentence)\n",
    "    print(s.link)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "DISCORD_WEBHOOK_URL = '' # fill this in with your discord webhook url\n",
    "\n",
    "for s in summaries:\n",
    "    data = {\n",
    "        \"content\" : '**' + s.title + '**' + '\\n\\n' + s.generated_summary_sentence + '\\n\\n' + s.link\n",
    "    }\n",
    "    requests.post(DISCORD_WEBHOOK_URL, data=data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aql",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
